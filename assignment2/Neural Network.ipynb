{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T10:41:22.910434Z",
     "start_time": "2019-05-04T10:41:22.250418Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T10:41:23.395406Z",
     "start_time": "2019-05-04T10:41:22.910434Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T10:41:29.343468Z",
     "start_time": "2019-05-04T10:41:23.395406Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T10:55:04.420919Z",
     "start_time": "2019-05-04T10:55:04.118755Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T10:55:06.196693Z",
     "start_time": "2019-05-04T10:55:05.936709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T10:55:45.335202Z",
     "start_time": "2019-05-04T10:55:36.096272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W0\n",
      "Gradient check passed!\n",
      "Checking gradient for B0\n",
      "Gradient check passed!\n",
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T10:55:58.388453Z",
     "start_time": "2019-05-04T10:55:48.398154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W0\n",
      "Gradient check passed!\n",
      "Checking gradient for B0\n",
      "Gradient check passed!\n",
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T10:56:01.572913Z",
     "start_time": "2019-05-04T10:56:01.267930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03333333333333333"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T11:11:33.408276Z",
     "start_time": "2019-05-04T11:08:22.915138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.371232, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.190023, Train accuracy: 0.251444, val accuracy: 0.248000\n",
      "Loss: 2.109512, Train accuracy: 0.307889, val accuracy: 0.300000\n",
      "Loss: 1.602587, Train accuracy: 0.306000, val accuracy: 0.298000\n",
      "Loss: 2.118404, Train accuracy: 0.314111, val accuracy: 0.300000\n",
      "Loss: 1.780123, Train accuracy: 0.323889, val accuracy: 0.310000\n",
      "Loss: 1.568589, Train accuracy: 0.339556, val accuracy: 0.312000\n",
      "Loss: 1.827189, Train accuracy: 0.346556, val accuracy: 0.324000\n",
      "Loss: 2.052840, Train accuracy: 0.341444, val accuracy: 0.316000\n",
      "Loss: 1.843110, Train accuracy: 0.348889, val accuracy: 0.319000\n",
      "Loss: 1.983501, Train accuracy: 0.355556, val accuracy: 0.331000\n",
      "Loss: 1.777461, Train accuracy: 0.354889, val accuracy: 0.325000\n",
      "Loss: 2.066839, Train accuracy: 0.359000, val accuracy: 0.336000\n",
      "Loss: 2.163057, Train accuracy: 0.350556, val accuracy: 0.314000\n",
      "Loss: 1.917888, Train accuracy: 0.348000, val accuracy: 0.312000\n",
      "Loss: 2.103744, Train accuracy: 0.363111, val accuracy: 0.326000\n",
      "Loss: 1.852713, Train accuracy: 0.364000, val accuracy: 0.333000\n",
      "Loss: 1.665899, Train accuracy: 0.349111, val accuracy: 0.310000\n",
      "Loss: 1.657733, Train accuracy: 0.363889, val accuracy: 0.323000\n",
      "Loss: 1.436025, Train accuracy: 0.393111, val accuracy: 0.357000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(),learning_rate=1e-1)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down and train and val accuracy go up for every epoch\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T11:11:34.823213Z",
     "start_time": "2019-05-04T11:11:34.393217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21e051ddcf8>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T11:20:29.808418Z",
     "start_time": "2019-05-04T11:17:13.875749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.329456, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.267185, Train accuracy: 0.154111, val accuracy: 0.139000\n",
      "Loss: 2.137389, Train accuracy: 0.164000, val accuracy: 0.142000\n",
      "Loss: 2.183547, Train accuracy: 0.217444, val accuracy: 0.199000\n",
      "Loss: 2.035910, Train accuracy: 0.235333, val accuracy: 0.205000\n",
      "Loss: 2.135559, Train accuracy: 0.245222, val accuracy: 0.221000\n",
      "Loss: 1.984761, Train accuracy: 0.249222, val accuracy: 0.223000\n",
      "Loss: 2.324807, Train accuracy: 0.247556, val accuracy: 0.230000\n",
      "Loss: 2.179817, Train accuracy: 0.268556, val accuracy: 0.236000\n",
      "Loss: 2.100097, Train accuracy: 0.278778, val accuracy: 0.236000\n",
      "Loss: 1.617454, Train accuracy: 0.286222, val accuracy: 0.243000\n",
      "Loss: 1.940812, Train accuracy: 0.282444, val accuracy: 0.243000\n",
      "Loss: 1.939723, Train accuracy: 0.290111, val accuracy: 0.242000\n",
      "Loss: 2.039355, Train accuracy: 0.285111, val accuracy: 0.245000\n",
      "Loss: 1.876405, Train accuracy: 0.288333, val accuracy: 0.241000\n",
      "Loss: 2.148526, Train accuracy: 0.292556, val accuracy: 0.254000\n",
      "Loss: 1.708712, Train accuracy: 0.294000, val accuracy: 0.254000\n",
      "Loss: 2.110806, Train accuracy: 0.293889, val accuracy: 0.252000\n",
      "Loss: 1.791954, Train accuracy: 0.299889, val accuracy: 0.254000\n",
      "Loss: 2.045201, Train accuracy: 0.303667, val accuracy: 0.253000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99,learning_rate=1e-1)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T11:35:12.229189Z",
     "start_time": "2019-05-04T11:31:13.410701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.216653, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 1.910852, Train accuracy: 0.302000, val accuracy: 0.303000\n",
      "Loss: 2.031970, Train accuracy: 0.357556, val accuracy: 0.368000\n",
      "Loss: 1.223458, Train accuracy: 0.397222, val accuracy: 0.403000\n",
      "Loss: 1.519811, Train accuracy: 0.405778, val accuracy: 0.401000\n",
      "Loss: 1.828500, Train accuracy: 0.421556, val accuracy: 0.420000\n",
      "Loss: 1.801781, Train accuracy: 0.425667, val accuracy: 0.431000\n",
      "Loss: 1.531535, Train accuracy: 0.426333, val accuracy: 0.422000\n",
      "Loss: 1.578265, Train accuracy: 0.437333, val accuracy: 0.430000\n",
      "Loss: 1.753024, Train accuracy: 0.438222, val accuracy: 0.434000\n",
      "Loss: 1.915738, Train accuracy: 0.443556, val accuracy: 0.438000\n",
      "Loss: 1.316627, Train accuracy: 0.436333, val accuracy: 0.426000\n",
      "Loss: 1.241993, Train accuracy: 0.443444, val accuracy: 0.426000\n",
      "Loss: 1.312618, Train accuracy: 0.446556, val accuracy: 0.433000\n",
      "Loss: 1.514479, Train accuracy: 0.450778, val accuracy: 0.427000\n",
      "Loss: 1.639355, Train accuracy: 0.456111, val accuracy: 0.430000\n",
      "Loss: 1.508842, Train accuracy: 0.448889, val accuracy: 0.427000\n",
      "Loss: 2.047624, Train accuracy: 0.459000, val accuracy: 0.440000\n",
      "Loss: 1.642698, Train accuracy: 0.458778, val accuracy: 0.434000\n",
      "Loss: 1.334799, Train accuracy: 0.465444, val accuracy: 0.439000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-2, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-05T18:48:50.573Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302672, Train accuracy: 0.100000, val accuracy: 0.000000\n",
      "Loss: 2.302951, Train accuracy: 0.100000, val accuracy: 0.000000\n",
      "Loss: 2.302331, Train accuracy: 0.100000, val accuracy: 0.000000\n",
      "Loss: 2.302521, Train accuracy: 0.100000, val accuracy: 0.000000\n",
      "Loss: 2.302167, Train accuracy: 0.100000, val accuracy: 0.000000\n",
      "Loss: 2.302334, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: 2.301971, Train accuracy: 0.200000, val accuracy: 0.100000\n",
      "Loss: 2.301632, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.302285, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.301438, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.301334, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.301695, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.301293, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.301534, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.300951, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.300961, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.300705, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.301426, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.299662, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.299976, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.299656, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.300598, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.300204, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.300039, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.301007, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.299665, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.299353, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.299698, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.299671, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.299335, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.300542, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.298017, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.300334, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.298969, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.298480, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.299777, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.298174, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.299459, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.298311, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.298081, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.298035, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.298062, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.299402, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.295971, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.297435, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.297392, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.296887, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.297049, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.299082, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.300485, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.292988, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.298494, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.296415, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.296347, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.295964, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.293738, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.296001, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.293898, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.295925, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.291084, Train accuracy: 0.300000, val accuracy: 0.000000\n",
      "Loss: 2.295496, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.295424, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.295150, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.292921, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.294984, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.294911, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.297180, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.294471, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.297329, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.294271, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.294269, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.294092, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.296820, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.296771, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.296743, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.296789, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.293781, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.293827, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.290168, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.296287, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.289745, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.292841, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.293124, Train accuracy: 0.300000, val accuracy: 0.100000\n",
      "Loss: 2.292672, Train accuracy: 0.300000, val accuracy: 0.100000\n"
     ]
    }
   ],
   "source": [
    "data_size = 10\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 150, reg = 1e-5)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-3, num_epochs=300, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T11:44:16.788982Z",
     "start_time": "2019-05-04T11:44:15.013934Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.329430, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.330671, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.293484, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.313949, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.281462, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.271883, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.321659, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.278206, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.267878, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.379453, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.250510, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.150565, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.478929, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.946470, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.503055, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.306408, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.645164, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.433683, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.023977, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.933362, Train accuracy: 0.266667, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **40%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
