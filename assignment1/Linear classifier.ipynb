{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T11:43:35.581808Z",
     "start_time": "2019-05-03T11:43:34.841827Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T11:43:38.866380Z",
     "start_time": "2019-05-03T11:43:37.271685Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T11:43:45.412129Z",
     "start_time": "2019-05-03T11:43:40.494665Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T11:43:47.157007Z",
     "start_time": "2019-05-03T11:43:46.932021Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T11:43:49.221891Z",
     "start_time": "2019-05-03T11:43:48.896906Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T11:43:53.926921Z",
     "start_time": "2019-05-03T11:43:53.656938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, np.array([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T12:10:15.831707Z",
     "start_time": "2019-05-03T12:10:15.491727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), np.array([1]))\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x,  np.array([1])), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T12:10:19.257219Z",
     "start_time": "2019-05-03T12:10:18.867241Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "# print(target_index)\n",
    "# print(predictions)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T12:18:17.312638Z",
     "start_time": "2019-05-03T12:18:16.786724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T12:22:35.666830Z",
     "start_time": "2019-05-03T12:22:35.236166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T12:41:47.770742Z",
     "start_time": "2019-05-03T12:41:42.332188Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.398349\n",
      "Epoch 1, loss: 2.330618\n",
      "Epoch 2, loss: 2.310400\n",
      "Epoch 3, loss: 2.304444\n",
      "Epoch 4, loss: 2.302615\n",
      "Epoch 5, loss: 2.302094\n",
      "Epoch 6, loss: 2.301873\n",
      "Epoch 7, loss: 2.302202\n",
      "Epoch 8, loss: 2.301848\n",
      "Epoch 9, loss: 2.302173\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T12:41:51.905817Z",
     "start_time": "2019-05-03T12:41:51.390831Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2670eeaf358>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T12:44:34.762568Z",
     "start_time": "2019-05-03T12:44:34.439092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 2, ..., 4, 2, 7], dtype=uint8)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T12:48:34.815835Z",
     "start_time": "2019-05-03T12:47:44.836771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.146\n",
      "Epoch 0, loss: 2.302423\n",
      "Epoch 1, loss: 2.302083\n",
      "Epoch 2, loss: 2.301670\n",
      "Epoch 3, loss: 2.302081\n",
      "Epoch 4, loss: 2.302290\n",
      "Epoch 5, loss: 2.302151\n",
      "Epoch 6, loss: 2.302414\n",
      "Epoch 7, loss: 2.301578\n",
      "Epoch 8, loss: 2.301524\n",
      "Epoch 9, loss: 2.303614\n",
      "Epoch 10, loss: 2.302318\n",
      "Epoch 11, loss: 2.300880\n",
      "Epoch 12, loss: 2.303031\n",
      "Epoch 13, loss: 2.302595\n",
      "Epoch 14, loss: 2.302180\n",
      "Epoch 15, loss: 2.301739\n",
      "Epoch 16, loss: 2.302451\n",
      "Epoch 17, loss: 2.301505\n",
      "Epoch 18, loss: 2.301952\n",
      "Epoch 19, loss: 2.302135\n",
      "Epoch 20, loss: 2.302436\n",
      "Epoch 21, loss: 2.301901\n",
      "Epoch 22, loss: 2.303091\n",
      "Epoch 23, loss: 2.302026\n",
      "Epoch 24, loss: 2.301581\n",
      "Epoch 25, loss: 2.301885\n",
      "Epoch 26, loss: 2.302856\n",
      "Epoch 27, loss: 2.302123\n",
      "Epoch 28, loss: 2.301570\n",
      "Epoch 29, loss: 2.301555\n",
      "Epoch 30, loss: 2.301822\n",
      "Epoch 31, loss: 2.302584\n",
      "Epoch 32, loss: 2.303070\n",
      "Epoch 33, loss: 2.301872\n",
      "Epoch 34, loss: 2.303081\n",
      "Epoch 35, loss: 2.302455\n",
      "Epoch 36, loss: 2.302097\n",
      "Epoch 37, loss: 2.301613\n",
      "Epoch 38, loss: 2.302004\n",
      "Epoch 39, loss: 2.303046\n",
      "Epoch 40, loss: 2.301645\n",
      "Epoch 41, loss: 2.302260\n",
      "Epoch 42, loss: 2.301288\n",
      "Epoch 43, loss: 2.302092\n",
      "Epoch 44, loss: 2.301598\n",
      "Epoch 45, loss: 2.301441\n",
      "Epoch 46, loss: 2.302620\n",
      "Epoch 47, loss: 2.303008\n",
      "Epoch 48, loss: 2.302136\n",
      "Epoch 49, loss: 2.302036\n",
      "Epoch 50, loss: 2.302529\n",
      "Epoch 51, loss: 2.302053\n",
      "Epoch 52, loss: 2.301882\n",
      "Epoch 53, loss: 2.302522\n",
      "Epoch 54, loss: 2.302034\n",
      "Epoch 55, loss: 2.302124\n",
      "Epoch 56, loss: 2.302188\n",
      "Epoch 57, loss: 2.301947\n",
      "Epoch 58, loss: 2.302634\n",
      "Epoch 59, loss: 2.301761\n",
      "Epoch 60, loss: 2.302990\n",
      "Epoch 61, loss: 2.303464\n",
      "Epoch 62, loss: 2.301774\n",
      "Epoch 63, loss: 2.302153\n",
      "Epoch 64, loss: 2.301965\n",
      "Epoch 65, loss: 2.301998\n",
      "Epoch 66, loss: 2.301875\n",
      "Epoch 67, loss: 2.302096\n",
      "Epoch 68, loss: 2.301671\n",
      "Epoch 69, loss: 2.301491\n",
      "Epoch 70, loss: 2.302959\n",
      "Epoch 71, loss: 2.301813\n",
      "Epoch 72, loss: 2.301200\n",
      "Epoch 73, loss: 2.301926\n",
      "Epoch 74, loss: 2.302052\n",
      "Epoch 75, loss: 2.302786\n",
      "Epoch 76, loss: 2.301682\n",
      "Epoch 77, loss: 2.302747\n",
      "Epoch 78, loss: 2.301772\n",
      "Epoch 79, loss: 2.302193\n",
      "Epoch 80, loss: 2.301687\n",
      "Epoch 81, loss: 2.301952\n",
      "Epoch 82, loss: 2.303305\n",
      "Epoch 83, loss: 2.302476\n",
      "Epoch 84, loss: 2.301677\n",
      "Epoch 85, loss: 2.301916\n",
      "Epoch 86, loss: 2.301978\n",
      "Epoch 87, loss: 2.302597\n",
      "Epoch 88, loss: 2.302056\n",
      "Epoch 89, loss: 2.301771\n",
      "Epoch 90, loss: 2.302867\n",
      "Epoch 91, loss: 2.301947\n",
      "Epoch 92, loss: 2.302122\n",
      "Epoch 93, loss: 2.301661\n",
      "Epoch 94, loss: 2.302635\n",
      "Epoch 95, loss: 2.302036\n",
      "Epoch 96, loss: 2.301897\n",
      "Epoch 97, loss: 2.302907\n",
      "Epoch 98, loss: 2.301933\n",
      "Epoch 99, loss: 2.301264\n",
      "Accuracy after training for 100 epochs:  0.124\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:11:27.074890Z",
     "start_time": "2019-05-03T13:28:01.187006Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.304403\n",
      "Epoch 1, loss: 2.302043\n",
      "Epoch 2, loss: 2.302137\n",
      "Epoch 3, loss: 2.303779\n",
      "Epoch 4, loss: 2.303087\n",
      "Epoch 5, loss: 2.304882\n",
      "Epoch 6, loss: 2.304572\n",
      "Epoch 7, loss: 2.301778\n",
      "Epoch 8, loss: 2.303566\n",
      "Epoch 9, loss: 2.302873\n",
      "Epoch 10, loss: 2.303241\n",
      "Epoch 11, loss: 2.303136\n",
      "Epoch 12, loss: 2.301686\n",
      "Epoch 13, loss: 2.302539\n",
      "Epoch 14, loss: 2.303942\n",
      "Epoch 15, loss: 2.302747\n",
      "Epoch 16, loss: 2.302587\n",
      "Epoch 17, loss: 2.304577\n",
      "Epoch 18, loss: 2.303483\n",
      "Epoch 19, loss: 2.302480\n",
      "Epoch 20, loss: 2.303805\n",
      "Epoch 21, loss: 2.304385\n",
      "Epoch 22, loss: 2.302677\n",
      "Epoch 23, loss: 2.301501\n",
      "Epoch 24, loss: 2.302337\n",
      "Epoch 25, loss: 2.304358\n",
      "Epoch 26, loss: 2.302345\n",
      "Epoch 27, loss: 2.302210\n",
      "Epoch 28, loss: 2.301907\n",
      "Epoch 29, loss: 2.301643\n",
      "Epoch 30, loss: 2.303244\n",
      "Epoch 31, loss: 2.301347\n",
      "Epoch 32, loss: 2.302037\n",
      "Epoch 33, loss: 2.303294\n",
      "Epoch 34, loss: 2.301910\n",
      "Epoch 35, loss: 2.303233\n",
      "Epoch 36, loss: 2.302848\n",
      "Epoch 37, loss: 2.304241\n",
      "Epoch 38, loss: 2.302321\n",
      "Epoch 39, loss: 2.301880\n",
      "Epoch 40, loss: 2.302508\n",
      "Epoch 41, loss: 2.303166\n",
      "Epoch 42, loss: 2.302603\n",
      "Epoch 43, loss: 2.303530\n",
      "Epoch 44, loss: 2.303058\n",
      "Epoch 45, loss: 2.301706\n",
      "Epoch 46, loss: 2.304454\n",
      "Epoch 47, loss: 2.302996\n",
      "Epoch 48, loss: 2.304430\n",
      "Epoch 49, loss: 2.306089\n",
      "Epoch 50, loss: 2.305587\n",
      "Epoch 51, loss: 2.303147\n",
      "Epoch 52, loss: 2.303989\n",
      "Epoch 53, loss: 2.300391\n",
      "Epoch 54, loss: 2.302848\n",
      "Epoch 55, loss: 2.303739\n",
      "Epoch 56, loss: 2.302360\n",
      "Epoch 57, loss: 2.301509\n",
      "Epoch 58, loss: 2.302187\n",
      "Epoch 59, loss: 2.302370\n",
      "Epoch 60, loss: 2.304222\n",
      "Epoch 61, loss: 2.302498\n",
      "Epoch 62, loss: 2.304571\n",
      "Epoch 63, loss: 2.304770\n",
      "Epoch 64, loss: 2.303286\n",
      "Epoch 65, loss: 2.305667\n",
      "Epoch 66, loss: 2.302191\n",
      "Epoch 67, loss: 2.305315\n",
      "Epoch 68, loss: 2.302444\n",
      "Epoch 69, loss: 2.302761\n",
      "Epoch 70, loss: 2.302556\n",
      "Epoch 71, loss: 2.300438\n",
      "Epoch 72, loss: 2.301920\n",
      "Epoch 73, loss: 2.302248\n",
      "Epoch 74, loss: 2.304753\n",
      "Epoch 75, loss: 2.303371\n",
      "Epoch 76, loss: 2.302750\n",
      "Epoch 77, loss: 2.302657\n",
      "Epoch 78, loss: 2.302722\n",
      "Epoch 79, loss: 2.304983\n",
      "Epoch 80, loss: 2.302690\n",
      "Epoch 81, loss: 2.301873\n",
      "Epoch 82, loss: 2.302270\n",
      "Epoch 83, loss: 2.301700\n",
      "Epoch 84, loss: 2.304188\n",
      "Epoch 85, loss: 2.302673\n",
      "Epoch 86, loss: 2.302735\n",
      "Epoch 87, loss: 2.302657\n",
      "Epoch 88, loss: 2.302635\n",
      "Epoch 89, loss: 2.301415\n",
      "Epoch 90, loss: 2.304282\n",
      "Epoch 91, loss: 2.301056\n",
      "Epoch 92, loss: 2.301207\n",
      "Epoch 93, loss: 2.303933\n",
      "Epoch 94, loss: 2.302019\n",
      "Epoch 95, loss: 2.301697\n",
      "Epoch 96, loss: 2.304555\n",
      "Epoch 97, loss: 2.303195\n",
      "Epoch 98, loss: 2.303132\n",
      "Epoch 99, loss: 2.302703\n",
      "Epoch 100, loss: 2.304073\n",
      "Epoch 101, loss: 2.308081\n",
      "Epoch 102, loss: 2.300290\n",
      "Epoch 103, loss: 2.301251\n",
      "Epoch 104, loss: 2.302659\n",
      "Epoch 105, loss: 2.299674\n",
      "Epoch 106, loss: 2.303606\n",
      "Epoch 107, loss: 2.301547\n",
      "Epoch 108, loss: 2.302470\n",
      "Epoch 109, loss: 2.304276\n",
      "Epoch 110, loss: 2.304200\n",
      "Epoch 111, loss: 2.306280\n",
      "Epoch 112, loss: 2.301362\n",
      "Epoch 113, loss: 2.303474\n",
      "Epoch 114, loss: 2.302889\n",
      "Epoch 115, loss: 2.303145\n",
      "Epoch 116, loss: 2.303065\n",
      "Epoch 117, loss: 2.302857\n",
      "Epoch 118, loss: 2.304642\n",
      "Epoch 119, loss: 2.304052\n",
      "Epoch 120, loss: 2.302270\n",
      "Epoch 121, loss: 2.303900\n",
      "Epoch 122, loss: 2.303495\n",
      "Epoch 123, loss: 2.303259\n",
      "Epoch 124, loss: 2.303801\n",
      "Epoch 125, loss: 2.303234\n",
      "Epoch 126, loss: 2.301946\n",
      "Epoch 127, loss: 2.302656\n",
      "Epoch 128, loss: 2.302752\n",
      "Epoch 129, loss: 2.301526\n",
      "Epoch 130, loss: 2.300942\n",
      "Epoch 131, loss: 2.301567\n",
      "Epoch 132, loss: 2.303020\n",
      "Epoch 133, loss: 2.303383\n",
      "Epoch 134, loss: 2.305304\n",
      "Epoch 135, loss: 2.303114\n",
      "Epoch 136, loss: 2.302069\n",
      "Epoch 137, loss: 2.303412\n",
      "Epoch 138, loss: 2.303609\n",
      "Epoch 139, loss: 2.303715\n",
      "Epoch 140, loss: 2.302394\n",
      "Epoch 141, loss: 2.302320\n",
      "Epoch 142, loss: 2.304986\n",
      "Epoch 143, loss: 2.301824\n",
      "Epoch 144, loss: 2.304178\n",
      "Epoch 145, loss: 2.304067\n",
      "Epoch 146, loss: 2.302183\n",
      "Epoch 147, loss: 2.303209\n",
      "Epoch 148, loss: 2.302771\n",
      "Epoch 149, loss: 2.302369\n",
      "Epoch 150, loss: 2.302699\n",
      "Epoch 151, loss: 2.302863\n",
      "Epoch 152, loss: 2.302935\n",
      "Epoch 153, loss: 2.303028\n",
      "Epoch 154, loss: 2.302273\n",
      "Epoch 155, loss: 2.302230\n",
      "Epoch 156, loss: 2.303443\n",
      "Epoch 157, loss: 2.301418\n",
      "Epoch 158, loss: 2.302970\n",
      "Epoch 159, loss: 2.302925\n",
      "Epoch 160, loss: 2.302357\n",
      "Epoch 161, loss: 2.302815\n",
      "Epoch 162, loss: 2.305292\n",
      "Epoch 163, loss: 2.302654\n",
      "Epoch 164, loss: 2.301791\n",
      "Epoch 165, loss: 2.303231\n",
      "Epoch 166, loss: 2.303179\n",
      "Epoch 167, loss: 2.302222\n",
      "Epoch 168, loss: 2.304430\n",
      "Epoch 169, loss: 2.302150\n",
      "Epoch 170, loss: 2.304987\n",
      "Epoch 171, loss: 2.301456\n",
      "Epoch 172, loss: 2.302840\n",
      "Epoch 173, loss: 2.301341\n",
      "Epoch 174, loss: 2.302693\n",
      "Epoch 175, loss: 2.303626\n",
      "Epoch 176, loss: 2.303394\n",
      "Epoch 177, loss: 2.301320\n",
      "Epoch 178, loss: 2.304602\n",
      "Epoch 179, loss: 2.300963\n",
      "Epoch 180, loss: 2.303904\n",
      "Epoch 181, loss: 2.302485\n",
      "Epoch 182, loss: 2.304541\n",
      "Epoch 183, loss: 2.300724\n",
      "Epoch 184, loss: 2.302696\n",
      "Epoch 185, loss: 2.304274\n",
      "Epoch 186, loss: 2.302797\n",
      "Epoch 187, loss: 2.302543\n",
      "Epoch 188, loss: 2.303298\n",
      "Epoch 189, loss: 2.301892\n",
      "Epoch 190, loss: 2.302869\n",
      "Epoch 191, loss: 2.305500\n",
      "Epoch 192, loss: 2.302026\n",
      "Epoch 193, loss: 2.303048\n",
      "Epoch 194, loss: 2.304490\n",
      "Epoch 195, loss: 2.303538\n",
      "Epoch 196, loss: 2.302895\n",
      "Epoch 197, loss: 2.302552\n",
      "Epoch 198, loss: 2.304468\n",
      "Epoch 199, loss: 2.302825\n",
      "learning rate = 0.01 reg = 10 accuracy = 0.151\n",
      "Epoch 0, loss: 2.309149\n",
      "Epoch 1, loss: 2.303114\n",
      "Epoch 2, loss: 2.295455\n",
      "Epoch 3, loss: 2.298019\n",
      "Epoch 4, loss: 2.302663\n",
      "Epoch 5, loss: 2.303775\n",
      "Epoch 6, loss: 2.296836\n",
      "Epoch 7, loss: 2.298212\n",
      "Epoch 8, loss: 2.293266\n",
      "Epoch 9, loss: 2.295343\n",
      "Epoch 10, loss: 2.296301\n",
      "Epoch 11, loss: 2.295301\n",
      "Epoch 12, loss: 2.302350\n",
      "Epoch 13, loss: 2.296345\n",
      "Epoch 14, loss: 2.297387\n",
      "Epoch 15, loss: 2.292740\n",
      "Epoch 16, loss: 2.299286\n",
      "Epoch 17, loss: 2.297967\n",
      "Epoch 18, loss: 2.296841\n",
      "Epoch 19, loss: 2.301110\n",
      "Epoch 20, loss: 2.294137\n",
      "Epoch 21, loss: 2.299676\n",
      "Epoch 22, loss: 2.299897\n",
      "Epoch 23, loss: 2.297062\n",
      "Epoch 24, loss: 2.294137\n",
      "Epoch 25, loss: 2.298536\n",
      "Epoch 26, loss: 2.292657\n",
      "Epoch 27, loss: 2.300701\n",
      "Epoch 28, loss: 2.292960\n",
      "Epoch 29, loss: 2.296446\n",
      "Epoch 30, loss: 2.296624\n",
      "Epoch 31, loss: 2.298708\n",
      "Epoch 32, loss: 2.300408\n",
      "Epoch 33, loss: 2.294645\n",
      "Epoch 34, loss: 2.296415\n",
      "Epoch 35, loss: 2.297988\n",
      "Epoch 36, loss: 2.297905\n",
      "Epoch 37, loss: 2.294531\n",
      "Epoch 38, loss: 2.294739\n",
      "Epoch 39, loss: 2.299391\n",
      "Epoch 40, loss: 2.297638\n",
      "Epoch 41, loss: 2.299516\n",
      "Epoch 42, loss: 2.295322\n",
      "Epoch 43, loss: 2.297250\n",
      "Epoch 44, loss: 2.300048\n",
      "Epoch 45, loss: 2.296408\n",
      "Epoch 46, loss: 2.298393\n",
      "Epoch 47, loss: 2.295129\n",
      "Epoch 48, loss: 2.294400\n",
      "Epoch 49, loss: 2.297157\n",
      "Epoch 50, loss: 2.297087\n",
      "Epoch 51, loss: 2.301244\n",
      "Epoch 52, loss: 2.298064\n",
      "Epoch 53, loss: 2.297063\n",
      "Epoch 54, loss: 2.296948\n",
      "Epoch 55, loss: 2.295005\n",
      "Epoch 56, loss: 2.296160\n",
      "Epoch 57, loss: 2.296065\n",
      "Epoch 58, loss: 2.299990\n",
      "Epoch 59, loss: 2.293943\n",
      "Epoch 60, loss: 2.296481\n",
      "Epoch 61, loss: 2.299080\n",
      "Epoch 62, loss: 2.301257\n",
      "Epoch 63, loss: 2.299995\n",
      "Epoch 64, loss: 2.296157\n",
      "Epoch 65, loss: 2.295592\n",
      "Epoch 66, loss: 2.295693\n",
      "Epoch 67, loss: 2.300181\n",
      "Epoch 68, loss: 2.293784\n",
      "Epoch 69, loss: 2.295323\n",
      "Epoch 70, loss: 2.293604\n",
      "Epoch 71, loss: 2.294529\n",
      "Epoch 72, loss: 2.297227\n",
      "Epoch 73, loss: 2.295778\n",
      "Epoch 74, loss: 2.294162\n",
      "Epoch 75, loss: 2.302155\n",
      "Epoch 76, loss: 2.298781\n",
      "Epoch 77, loss: 2.298410\n",
      "Epoch 78, loss: 2.294924\n",
      "Epoch 79, loss: 2.290868\n",
      "Epoch 80, loss: 2.297296\n",
      "Epoch 81, loss: 2.296367\n",
      "Epoch 82, loss: 2.294909\n",
      "Epoch 83, loss: 2.301341\n",
      "Epoch 84, loss: 2.297327\n",
      "Epoch 85, loss: 2.296796\n",
      "Epoch 86, loss: 2.295488\n",
      "Epoch 87, loss: 2.299115\n",
      "Epoch 88, loss: 2.298852\n",
      "Epoch 89, loss: 2.300266\n",
      "Epoch 90, loss: 2.298848\n",
      "Epoch 91, loss: 2.299972\n",
      "Epoch 92, loss: 2.301702\n",
      "Epoch 93, loss: 2.297616\n",
      "Epoch 94, loss: 2.296320\n",
      "Epoch 95, loss: 2.299040\n",
      "Epoch 96, loss: 2.298015\n",
      "Epoch 97, loss: 2.297163\n",
      "Epoch 98, loss: 2.292607\n",
      "Epoch 99, loss: 2.298511\n",
      "Epoch 100, loss: 2.300016\n",
      "Epoch 101, loss: 2.297361\n",
      "Epoch 102, loss: 2.301215\n",
      "Epoch 103, loss: 2.295983\n",
      "Epoch 104, loss: 2.301401\n",
      "Epoch 105, loss: 2.300129\n",
      "Epoch 106, loss: 2.297968\n",
      "Epoch 107, loss: 2.297367\n",
      "Epoch 108, loss: 2.296559\n",
      "Epoch 109, loss: 2.298374\n",
      "Epoch 110, loss: 2.292720\n",
      "Epoch 111, loss: 2.297087\n",
      "Epoch 112, loss: 2.293444\n",
      "Epoch 113, loss: 2.292758\n",
      "Epoch 114, loss: 2.300774\n",
      "Epoch 115, loss: 2.304838\n",
      "Epoch 116, loss: 2.297381\n",
      "Epoch 117, loss: 2.300490\n",
      "Epoch 118, loss: 2.298374\n",
      "Epoch 119, loss: 2.299295\n",
      "Epoch 120, loss: 2.296904\n",
      "Epoch 121, loss: 2.300672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122, loss: 2.295195\n",
      "Epoch 123, loss: 2.298737\n",
      "Epoch 124, loss: 2.295497\n",
      "Epoch 125, loss: 2.301088\n",
      "Epoch 126, loss: 2.298498\n",
      "Epoch 127, loss: 2.296150\n",
      "Epoch 128, loss: 2.297956\n",
      "Epoch 129, loss: 2.297717\n",
      "Epoch 130, loss: 2.298518\n",
      "Epoch 131, loss: 2.294029\n",
      "Epoch 132, loss: 2.297797\n",
      "Epoch 133, loss: 2.296920\n",
      "Epoch 134, loss: 2.300788\n",
      "Epoch 135, loss: 2.298213\n",
      "Epoch 136, loss: 2.302842\n",
      "Epoch 137, loss: 2.302381\n",
      "Epoch 138, loss: 2.300681\n",
      "Epoch 139, loss: 2.303038\n",
      "Epoch 140, loss: 2.296059\n",
      "Epoch 141, loss: 2.293295\n",
      "Epoch 142, loss: 2.293988\n",
      "Epoch 143, loss: 2.300555\n",
      "Epoch 144, loss: 2.298836\n",
      "Epoch 145, loss: 2.296955\n",
      "Epoch 146, loss: 2.301456\n",
      "Epoch 147, loss: 2.295116\n",
      "Epoch 148, loss: 2.293782\n",
      "Epoch 149, loss: 2.297648\n",
      "Epoch 150, loss: 2.296698\n",
      "Epoch 151, loss: 2.300342\n",
      "Epoch 152, loss: 2.297394\n",
      "Epoch 153, loss: 2.292158\n",
      "Epoch 154, loss: 2.297125\n",
      "Epoch 155, loss: 2.300941\n",
      "Epoch 156, loss: 2.295476\n",
      "Epoch 157, loss: 2.299765\n",
      "Epoch 158, loss: 2.294838\n",
      "Epoch 159, loss: 2.297026\n",
      "Epoch 160, loss: 2.297180\n",
      "Epoch 161, loss: 2.294661\n",
      "Epoch 162, loss: 2.303428\n",
      "Epoch 163, loss: 2.294464\n",
      "Epoch 164, loss: 2.300038\n",
      "Epoch 165, loss: 2.298702\n",
      "Epoch 166, loss: 2.297646\n",
      "Epoch 167, loss: 2.296056\n",
      "Epoch 168, loss: 2.294543\n",
      "Epoch 169, loss: 2.298057\n",
      "Epoch 170, loss: 2.296777\n",
      "Epoch 171, loss: 2.297819\n",
      "Epoch 172, loss: 2.294880\n",
      "Epoch 173, loss: 2.296686\n",
      "Epoch 174, loss: 2.296776\n",
      "Epoch 175, loss: 2.294012\n",
      "Epoch 176, loss: 2.294994\n",
      "Epoch 177, loss: 2.296016\n",
      "Epoch 178, loss: 2.297674\n",
      "Epoch 179, loss: 2.298547\n",
      "Epoch 180, loss: 2.296816\n",
      "Epoch 181, loss: 2.299726\n",
      "Epoch 182, loss: 2.296009\n",
      "Epoch 183, loss: 2.296818\n",
      "Epoch 184, loss: 2.289433\n",
      "Epoch 185, loss: 2.297249\n",
      "Epoch 186, loss: 2.294923\n",
      "Epoch 187, loss: 2.299781\n",
      "Epoch 188, loss: 2.297638\n",
      "Epoch 189, loss: 2.296571\n",
      "Epoch 190, loss: 2.296439\n",
      "Epoch 191, loss: 2.295262\n",
      "Epoch 192, loss: 2.297094\n",
      "Epoch 193, loss: 2.293887\n",
      "Epoch 194, loss: 2.297819\n",
      "Epoch 195, loss: 2.295878\n",
      "Epoch 196, loss: 2.295595\n",
      "Epoch 197, loss: 2.296389\n",
      "Epoch 198, loss: 2.296310\n",
      "Epoch 199, loss: 2.299475\n",
      "learning rate = 0.01 reg = 1 accuracy = 0.184\n",
      "Epoch 0, loss: 2.296211\n",
      "Epoch 1, loss: 2.287588\n",
      "Epoch 2, loss: 2.285954\n",
      "Epoch 3, loss: 2.278524\n",
      "Epoch 4, loss: 2.276250\n",
      "Epoch 5, loss: 2.261029\n",
      "Epoch 6, loss: 2.269731\n",
      "Epoch 7, loss: 2.270563\n",
      "Epoch 8, loss: 2.268352\n",
      "Epoch 9, loss: 2.265649\n",
      "Epoch 10, loss: 2.266047\n",
      "Epoch 11, loss: 2.249439\n",
      "Epoch 12, loss: 2.272280\n",
      "Epoch 13, loss: 2.256720\n",
      "Epoch 14, loss: 2.262354\n",
      "Epoch 15, loss: 2.261554\n",
      "Epoch 16, loss: 2.230133\n",
      "Epoch 17, loss: 2.230442\n",
      "Epoch 18, loss: 2.242393\n",
      "Epoch 19, loss: 2.261678\n",
      "Epoch 20, loss: 2.260702\n",
      "Epoch 21, loss: 2.267406\n",
      "Epoch 22, loss: 2.265322\n",
      "Epoch 23, loss: 2.233586\n",
      "Epoch 24, loss: 2.254456\n",
      "Epoch 25, loss: 2.255766\n",
      "Epoch 26, loss: 2.256888\n",
      "Epoch 27, loss: 2.246462\n",
      "Epoch 28, loss: 2.251862\n",
      "Epoch 29, loss: 2.266780\n",
      "Epoch 30, loss: 2.265106\n",
      "Epoch 31, loss: 2.254309\n",
      "Epoch 32, loss: 2.262357\n",
      "Epoch 33, loss: 2.277008\n",
      "Epoch 34, loss: 2.254525\n",
      "Epoch 35, loss: 2.252631\n",
      "Epoch 36, loss: 2.247725\n",
      "Epoch 37, loss: 2.267405\n",
      "Epoch 38, loss: 2.257984\n",
      "Epoch 39, loss: 2.260038\n",
      "Epoch 40, loss: 2.256226\n",
      "Epoch 41, loss: 2.271361\n",
      "Epoch 42, loss: 2.249385\n",
      "Epoch 43, loss: 2.240200\n",
      "Epoch 44, loss: 2.234205\n",
      "Epoch 45, loss: 2.284201\n",
      "Epoch 46, loss: 2.262589\n",
      "Epoch 47, loss: 2.255572\n",
      "Epoch 48, loss: 2.272101\n",
      "Epoch 49, loss: 2.254652\n",
      "Epoch 50, loss: 2.255899\n",
      "Epoch 51, loss: 2.260736\n",
      "Epoch 52, loss: 2.257410\n",
      "Epoch 53, loss: 2.245102\n",
      "Epoch 54, loss: 2.254277\n",
      "Epoch 55, loss: 2.259931\n",
      "Epoch 56, loss: 2.232681\n",
      "Epoch 57, loss: 2.270081\n",
      "Epoch 58, loss: 2.246627\n",
      "Epoch 59, loss: 2.271056\n",
      "Epoch 60, loss: 2.251496\n",
      "Epoch 61, loss: 2.256095\n",
      "Epoch 62, loss: 2.259433\n",
      "Epoch 63, loss: 2.250116\n",
      "Epoch 64, loss: 2.261049\n",
      "Epoch 65, loss: 2.255980\n",
      "Epoch 66, loss: 2.251758\n",
      "Epoch 67, loss: 2.249911\n",
      "Epoch 68, loss: 2.249069\n",
      "Epoch 69, loss: 2.255456\n",
      "Epoch 70, loss: 2.264384\n",
      "Epoch 71, loss: 2.252449\n",
      "Epoch 72, loss: 2.259816\n",
      "Epoch 73, loss: 2.262452\n",
      "Epoch 74, loss: 2.251916\n",
      "Epoch 75, loss: 2.261066\n",
      "Epoch 76, loss: 2.241535\n",
      "Epoch 77, loss: 2.259288\n",
      "Epoch 78, loss: 2.265050\n",
      "Epoch 79, loss: 2.252837\n",
      "Epoch 80, loss: 2.264709\n",
      "Epoch 81, loss: 2.259762\n",
      "Epoch 82, loss: 2.224064\n",
      "Epoch 83, loss: 2.235317\n",
      "Epoch 84, loss: 2.274490\n",
      "Epoch 85, loss: 2.252332\n",
      "Epoch 86, loss: 2.253681\n",
      "Epoch 87, loss: 2.272635\n",
      "Epoch 88, loss: 2.273855\n",
      "Epoch 89, loss: 2.257595\n",
      "Epoch 90, loss: 2.242098\n",
      "Epoch 91, loss: 2.252621\n",
      "Epoch 92, loss: 2.248058\n",
      "Epoch 93, loss: 2.261757\n",
      "Epoch 94, loss: 2.258441\n",
      "Epoch 95, loss: 2.278053\n",
      "Epoch 96, loss: 2.260624\n",
      "Epoch 97, loss: 2.246933\n",
      "Epoch 98, loss: 2.251985\n",
      "Epoch 99, loss: 2.264699\n",
      "Epoch 100, loss: 2.264069\n",
      "Epoch 101, loss: 2.252723\n",
      "Epoch 102, loss: 2.242570\n",
      "Epoch 103, loss: 2.259435\n",
      "Epoch 104, loss: 2.261099\n",
      "Epoch 105, loss: 2.250987\n",
      "Epoch 106, loss: 2.240500\n",
      "Epoch 107, loss: 2.267663\n",
      "Epoch 108, loss: 2.267650\n",
      "Epoch 109, loss: 2.256761\n",
      "Epoch 110, loss: 2.252691\n",
      "Epoch 111, loss: 2.270294\n",
      "Epoch 112, loss: 2.275510\n",
      "Epoch 113, loss: 2.262565\n",
      "Epoch 114, loss: 2.241805\n",
      "Epoch 115, loss: 2.262610\n",
      "Epoch 116, loss: 2.251492\n",
      "Epoch 117, loss: 2.258641\n",
      "Epoch 118, loss: 2.250430\n",
      "Epoch 119, loss: 2.255986\n",
      "Epoch 120, loss: 2.266428\n",
      "Epoch 121, loss: 2.267099\n",
      "Epoch 122, loss: 2.240041\n",
      "Epoch 123, loss: 2.242871\n",
      "Epoch 124, loss: 2.258991\n",
      "Epoch 125, loss: 2.236113\n",
      "Epoch 126, loss: 2.239091\n",
      "Epoch 127, loss: 2.264187\n",
      "Epoch 128, loss: 2.252898\n",
      "Epoch 129, loss: 2.242171\n",
      "Epoch 130, loss: 2.257347\n",
      "Epoch 131, loss: 2.252903\n",
      "Epoch 132, loss: 2.249414\n",
      "Epoch 133, loss: 2.240877\n",
      "Epoch 134, loss: 2.254383\n",
      "Epoch 135, loss: 2.268121\n",
      "Epoch 136, loss: 2.262555\n",
      "Epoch 137, loss: 2.252337\n",
      "Epoch 138, loss: 2.270437\n",
      "Epoch 139, loss: 2.261422\n",
      "Epoch 140, loss: 2.256223\n",
      "Epoch 141, loss: 2.250006\n",
      "Epoch 142, loss: 2.254117\n",
      "Epoch 143, loss: 2.236649\n",
      "Epoch 144, loss: 2.270867\n",
      "Epoch 145, loss: 2.260887\n",
      "Epoch 146, loss: 2.241521\n",
      "Epoch 147, loss: 2.262903\n",
      "Epoch 148, loss: 2.260069\n",
      "Epoch 149, loss: 2.244390\n",
      "Epoch 150, loss: 2.260634\n",
      "Epoch 151, loss: 2.276561\n",
      "Epoch 152, loss: 2.234144\n",
      "Epoch 153, loss: 2.257095\n",
      "Epoch 154, loss: 2.262259\n",
      "Epoch 155, loss: 2.282091\n",
      "Epoch 156, loss: 2.269817\n",
      "Epoch 157, loss: 2.259872\n",
      "Epoch 158, loss: 2.255723\n",
      "Epoch 159, loss: 2.262817\n",
      "Epoch 160, loss: 2.264390\n",
      "Epoch 161, loss: 2.253316\n",
      "Epoch 162, loss: 2.268236\n",
      "Epoch 163, loss: 2.252333\n",
      "Epoch 164, loss: 2.249388\n",
      "Epoch 165, loss: 2.267355\n",
      "Epoch 166, loss: 2.277794\n",
      "Epoch 167, loss: 2.261106\n",
      "Epoch 168, loss: 2.257628\n",
      "Epoch 169, loss: 2.250797\n",
      "Epoch 170, loss: 2.264931\n",
      "Epoch 171, loss: 2.256207\n",
      "Epoch 172, loss: 2.280634\n",
      "Epoch 173, loss: 2.249155\n",
      "Epoch 174, loss: 2.246310\n",
      "Epoch 175, loss: 2.259511\n",
      "Epoch 176, loss: 2.231861\n",
      "Epoch 177, loss: 2.260506\n",
      "Epoch 178, loss: 2.267873\n",
      "Epoch 179, loss: 2.236208\n",
      "Epoch 180, loss: 2.233149\n",
      "Epoch 181, loss: 2.280688\n",
      "Epoch 182, loss: 2.255163\n",
      "Epoch 183, loss: 2.269491\n",
      "Epoch 184, loss: 2.259546\n",
      "Epoch 185, loss: 2.232389\n",
      "Epoch 186, loss: 2.247607\n",
      "Epoch 187, loss: 2.274911\n",
      "Epoch 188, loss: 2.260201\n",
      "Epoch 189, loss: 2.257354\n",
      "Epoch 190, loss: 2.248646\n",
      "Epoch 191, loss: 2.259915\n",
      "Epoch 192, loss: 2.241571\n",
      "Epoch 193, loss: 2.250829\n",
      "Epoch 194, loss: 2.254170\n",
      "Epoch 195, loss: 2.257744\n",
      "Epoch 196, loss: 2.240854\n",
      "Epoch 197, loss: 2.242880\n",
      "Epoch 198, loss: 2.252588\n",
      "Epoch 199, loss: 2.240441\n",
      "learning rate = 0.01 reg = 0.1 accuracy = 0.232\n",
      "Epoch 0, loss: 2.299100\n",
      "Epoch 1, loss: 2.289043\n",
      "Epoch 2, loss: 2.280690\n",
      "Epoch 3, loss: 2.285068\n",
      "Epoch 4, loss: 2.272046\n",
      "Epoch 5, loss: 2.265032\n",
      "Epoch 6, loss: 2.254869\n",
      "Epoch 7, loss: 2.256870\n",
      "Epoch 8, loss: 2.243656\n",
      "Epoch 9, loss: 2.236907\n",
      "Epoch 10, loss: 2.240836\n",
      "Epoch 11, loss: 2.249439\n",
      "Epoch 12, loss: 2.235669\n",
      "Epoch 13, loss: 2.228782\n",
      "Epoch 14, loss: 2.228844\n",
      "Epoch 15, loss: 2.222319\n",
      "Epoch 16, loss: 2.220923\n",
      "Epoch 17, loss: 2.227891\n",
      "Epoch 18, loss: 2.238863\n",
      "Epoch 19, loss: 2.216876\n",
      "Epoch 20, loss: 2.208053\n",
      "Epoch 21, loss: 2.196722\n",
      "Epoch 22, loss: 2.214172\n",
      "Epoch 23, loss: 2.203029\n",
      "Epoch 24, loss: 2.229813\n",
      "Epoch 25, loss: 2.200668\n",
      "Epoch 26, loss: 2.184550\n",
      "Epoch 27, loss: 2.186318\n",
      "Epoch 28, loss: 2.181623\n",
      "Epoch 29, loss: 2.209424\n",
      "Epoch 30, loss: 2.183865\n",
      "Epoch 31, loss: 2.206068\n",
      "Epoch 32, loss: 2.166693\n",
      "Epoch 33, loss: 2.169579\n",
      "Epoch 34, loss: 2.202511\n",
      "Epoch 35, loss: 2.225592\n",
      "Epoch 36, loss: 2.223044\n",
      "Epoch 37, loss: 2.209295\n",
      "Epoch 38, loss: 2.174071\n",
      "Epoch 39, loss: 2.168279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, loss: 2.169717\n",
      "Epoch 41, loss: 2.177933\n",
      "Epoch 42, loss: 2.212255\n",
      "Epoch 43, loss: 2.175934\n",
      "Epoch 44, loss: 2.153248\n",
      "Epoch 45, loss: 2.187328\n",
      "Epoch 46, loss: 2.159721\n",
      "Epoch 47, loss: 2.216197\n",
      "Epoch 48, loss: 2.197102\n",
      "Epoch 49, loss: 2.162366\n",
      "Epoch 50, loss: 2.221283\n",
      "Epoch 51, loss: 2.126559\n",
      "Epoch 52, loss: 2.178190\n",
      "Epoch 53, loss: 2.162882\n",
      "Epoch 54, loss: 2.176247\n",
      "Epoch 55, loss: 2.174422\n",
      "Epoch 56, loss: 2.178158\n",
      "Epoch 57, loss: 2.198598\n",
      "Epoch 58, loss: 2.207023\n",
      "Epoch 59, loss: 2.208308\n",
      "Epoch 60, loss: 2.212615\n",
      "Epoch 61, loss: 2.149101\n",
      "Epoch 62, loss: 2.179589\n",
      "Epoch 63, loss: 2.183052\n",
      "Epoch 64, loss: 2.240584\n",
      "Epoch 65, loss: 2.178804\n",
      "Epoch 66, loss: 2.157035\n",
      "Epoch 67, loss: 2.156311\n",
      "Epoch 68, loss: 2.179380\n",
      "Epoch 69, loss: 2.129872\n",
      "Epoch 70, loss: 2.176482\n",
      "Epoch 71, loss: 2.217232\n",
      "Epoch 72, loss: 2.172905\n",
      "Epoch 73, loss: 2.164147\n",
      "Epoch 74, loss: 2.145703\n",
      "Epoch 75, loss: 2.154605\n",
      "Epoch 76, loss: 2.142663\n",
      "Epoch 77, loss: 2.128780\n",
      "Epoch 78, loss: 2.154880\n",
      "Epoch 79, loss: 2.170172\n",
      "Epoch 80, loss: 2.167697\n",
      "Epoch 81, loss: 2.177320\n",
      "Epoch 82, loss: 2.164243\n",
      "Epoch 83, loss: 2.193657\n",
      "Epoch 84, loss: 2.159583\n",
      "Epoch 85, loss: 2.124542\n",
      "Epoch 86, loss: 2.185913\n",
      "Epoch 87, loss: 2.148692\n",
      "Epoch 88, loss: 2.146392\n",
      "Epoch 89, loss: 2.221476\n",
      "Epoch 90, loss: 2.151606\n",
      "Epoch 91, loss: 2.166312\n",
      "Epoch 92, loss: 2.125441\n",
      "Epoch 93, loss: 2.175869\n",
      "Epoch 94, loss: 2.170026\n",
      "Epoch 95, loss: 2.214906\n",
      "Epoch 96, loss: 2.210694\n",
      "Epoch 97, loss: 2.170954\n",
      "Epoch 98, loss: 2.196996\n",
      "Epoch 99, loss: 2.180293\n",
      "Epoch 100, loss: 2.204816\n",
      "Epoch 101, loss: 2.179641\n",
      "Epoch 102, loss: 2.207328\n",
      "Epoch 103, loss: 2.159767\n",
      "Epoch 104, loss: 2.194329\n",
      "Epoch 105, loss: 2.155296\n",
      "Epoch 106, loss: 2.206028\n",
      "Epoch 107, loss: 2.128422\n",
      "Epoch 108, loss: 2.185968\n",
      "Epoch 109, loss: 2.157477\n",
      "Epoch 110, loss: 2.191270\n",
      "Epoch 111, loss: 2.134157\n",
      "Epoch 112, loss: 2.177575\n",
      "Epoch 113, loss: 2.158450\n",
      "Epoch 114, loss: 2.164066\n",
      "Epoch 115, loss: 2.209186\n",
      "Epoch 116, loss: 2.148235\n",
      "Epoch 117, loss: 2.167032\n",
      "Epoch 118, loss: 2.127763\n",
      "Epoch 119, loss: 2.187375\n",
      "Epoch 120, loss: 2.191870\n",
      "Epoch 121, loss: 2.142251\n",
      "Epoch 122, loss: 2.164405\n",
      "Epoch 123, loss: 2.155057\n",
      "Epoch 124, loss: 2.147127\n",
      "Epoch 125, loss: 2.153011\n",
      "Epoch 126, loss: 2.104394\n",
      "Epoch 127, loss: 2.138702\n",
      "Epoch 128, loss: 2.198200\n",
      "Epoch 129, loss: 2.195245\n",
      "Epoch 130, loss: 2.176688\n",
      "Epoch 131, loss: 2.193582\n",
      "Epoch 132, loss: 2.161500\n",
      "Epoch 133, loss: 2.164630\n",
      "Epoch 134, loss: 2.208839\n",
      "Epoch 135, loss: 2.110474\n",
      "Epoch 136, loss: 2.159242\n",
      "Epoch 137, loss: 2.177275\n",
      "Epoch 138, loss: 2.170070\n",
      "Epoch 139, loss: 2.180656\n",
      "Epoch 140, loss: 2.146404\n",
      "Epoch 141, loss: 2.147681\n",
      "Epoch 142, loss: 2.183763\n",
      "Epoch 143, loss: 2.202304\n",
      "Epoch 144, loss: 2.201580\n",
      "Epoch 145, loss: 2.174198\n",
      "Epoch 146, loss: 2.182327\n",
      "Epoch 147, loss: 2.152092\n",
      "Epoch 148, loss: 2.146863\n",
      "Epoch 149, loss: 2.131677\n",
      "Epoch 150, loss: 2.153866\n",
      "Epoch 151, loss: 2.167785\n",
      "Epoch 152, loss: 2.161361\n",
      "Epoch 153, loss: 2.187947\n",
      "Epoch 154, loss: 2.169683\n",
      "Epoch 155, loss: 2.128193\n",
      "Epoch 156, loss: 2.197098\n",
      "Epoch 157, loss: 2.191062\n",
      "Epoch 158, loss: 2.194357\n",
      "Epoch 159, loss: 2.174265\n",
      "Epoch 160, loss: 2.109943\n",
      "Epoch 161, loss: 2.170523\n",
      "Epoch 162, loss: 2.179636\n",
      "Epoch 163, loss: 2.133154\n",
      "Epoch 164, loss: 2.178765\n",
      "Epoch 165, loss: 2.238242\n",
      "Epoch 166, loss: 2.138399\n",
      "Epoch 167, loss: 2.158850\n",
      "Epoch 168, loss: 2.154643\n",
      "Epoch 169, loss: 2.188996\n",
      "Epoch 170, loss: 2.146530\n",
      "Epoch 171, loss: 2.114839\n",
      "Epoch 172, loss: 2.202471\n",
      "Epoch 173, loss: 2.179662\n",
      "Epoch 174, loss: 2.192399\n",
      "Epoch 175, loss: 2.143572\n",
      "Epoch 176, loss: 2.142914\n",
      "Epoch 177, loss: 2.154159\n",
      "Epoch 178, loss: 2.200463\n",
      "Epoch 179, loss: 2.180242\n",
      "Epoch 180, loss: 2.141133\n",
      "Epoch 181, loss: 2.157594\n",
      "Epoch 182, loss: 2.168041\n",
      "Epoch 183, loss: 2.151068\n",
      "Epoch 184, loss: 2.196890\n",
      "Epoch 185, loss: 2.198680\n",
      "Epoch 186, loss: 2.223463\n",
      "Epoch 187, loss: 2.153408\n",
      "Epoch 188, loss: 2.171287\n",
      "Epoch 189, loss: 2.166613\n",
      "Epoch 190, loss: 2.125915\n",
      "Epoch 191, loss: 2.162109\n",
      "Epoch 192, loss: 2.139573\n",
      "Epoch 193, loss: 2.176071\n",
      "Epoch 194, loss: 2.142357\n",
      "Epoch 195, loss: 2.152727\n",
      "Epoch 196, loss: 2.172352\n",
      "Epoch 197, loss: 2.177513\n",
      "Epoch 198, loss: 2.163591\n",
      "Epoch 199, loss: 2.166464\n",
      "learning rate = 0.01 reg = 0.01 accuracy = 0.24\n",
      "Epoch 0, loss: 2.298174\n",
      "Epoch 1, loss: 2.285379\n",
      "Epoch 2, loss: 2.287185\n",
      "Epoch 3, loss: 2.277966\n",
      "Epoch 4, loss: 2.271437\n",
      "Epoch 5, loss: 2.242571\n",
      "Epoch 6, loss: 2.250811\n",
      "Epoch 7, loss: 2.237644\n",
      "Epoch 8, loss: 2.240232\n",
      "Epoch 9, loss: 2.249112\n",
      "Epoch 10, loss: 2.244231\n",
      "Epoch 11, loss: 2.219724\n",
      "Epoch 12, loss: 2.237523\n",
      "Epoch 13, loss: 2.249143\n",
      "Epoch 14, loss: 2.216101\n",
      "Epoch 15, loss: 2.226075\n",
      "Epoch 16, loss: 2.219381\n",
      "Epoch 17, loss: 2.228709\n",
      "Epoch 18, loss: 2.200738\n",
      "Epoch 19, loss: 2.228771\n",
      "Epoch 20, loss: 2.183624\n",
      "Epoch 21, loss: 2.215342\n",
      "Epoch 22, loss: 2.189281\n",
      "Epoch 23, loss: 2.194127\n",
      "Epoch 24, loss: 2.188229\n",
      "Epoch 25, loss: 2.194113\n",
      "Epoch 26, loss: 2.168284\n",
      "Epoch 27, loss: 2.169038\n",
      "Epoch 28, loss: 2.189174\n",
      "Epoch 29, loss: 2.174346\n",
      "Epoch 30, loss: 2.185228\n",
      "Epoch 31, loss: 2.168259\n",
      "Epoch 32, loss: 2.176868\n",
      "Epoch 33, loss: 2.197203\n",
      "Epoch 34, loss: 2.209185\n",
      "Epoch 35, loss: 2.176602\n",
      "Epoch 36, loss: 2.159666\n",
      "Epoch 37, loss: 2.166773\n",
      "Epoch 38, loss: 2.132481\n",
      "Epoch 39, loss: 2.179752\n",
      "Epoch 40, loss: 2.197097\n",
      "Epoch 41, loss: 2.172938\n",
      "Epoch 42, loss: 2.163943\n",
      "Epoch 43, loss: 2.158377\n",
      "Epoch 44, loss: 2.154434\n",
      "Epoch 45, loss: 2.180566\n",
      "Epoch 46, loss: 2.205793\n",
      "Epoch 47, loss: 2.136363\n",
      "Epoch 48, loss: 2.192460\n",
      "Epoch 49, loss: 2.136087\n",
      "Epoch 50, loss: 2.164676\n",
      "Epoch 51, loss: 2.127714\n",
      "Epoch 52, loss: 2.161802\n",
      "Epoch 53, loss: 2.181920\n",
      "Epoch 54, loss: 2.133948\n",
      "Epoch 55, loss: 2.197903\n",
      "Epoch 56, loss: 2.166894\n",
      "Epoch 57, loss: 2.165834\n",
      "Epoch 58, loss: 2.139519\n",
      "Epoch 59, loss: 2.120423\n",
      "Epoch 60, loss: 2.161739\n",
      "Epoch 61, loss: 2.137799\n",
      "Epoch 62, loss: 2.120953\n",
      "Epoch 63, loss: 2.149216\n",
      "Epoch 64, loss: 2.179588\n",
      "Epoch 65, loss: 2.081051\n",
      "Epoch 66, loss: 2.121366\n",
      "Epoch 67, loss: 2.192332\n",
      "Epoch 68, loss: 2.146672\n",
      "Epoch 69, loss: 2.152005\n",
      "Epoch 70, loss: 2.172704\n",
      "Epoch 71, loss: 2.068477\n",
      "Epoch 72, loss: 2.174025\n",
      "Epoch 73, loss: 2.178505\n",
      "Epoch 74, loss: 2.196945\n",
      "Epoch 75, loss: 2.156450\n",
      "Epoch 76, loss: 2.133659\n",
      "Epoch 77, loss: 2.184844\n",
      "Epoch 78, loss: 2.081008\n",
      "Epoch 79, loss: 2.172884\n",
      "Epoch 80, loss: 2.166737\n",
      "Epoch 81, loss: 2.162826\n",
      "Epoch 82, loss: 2.181295\n",
      "Epoch 83, loss: 2.164483\n",
      "Epoch 84, loss: 2.153455\n",
      "Epoch 85, loss: 2.090368\n",
      "Epoch 86, loss: 2.144145\n",
      "Epoch 87, loss: 2.191858\n",
      "Epoch 88, loss: 2.143476\n",
      "Epoch 89, loss: 2.154564\n",
      "Epoch 90, loss: 2.155649\n",
      "Epoch 91, loss: 2.138381\n",
      "Epoch 92, loss: 2.090402\n",
      "Epoch 93, loss: 2.139596\n",
      "Epoch 94, loss: 2.150995\n",
      "Epoch 95, loss: 2.106690\n",
      "Epoch 96, loss: 2.112328\n",
      "Epoch 97, loss: 2.131997\n",
      "Epoch 98, loss: 2.149308\n",
      "Epoch 99, loss: 2.127564\n",
      "Epoch 100, loss: 2.151594\n",
      "Epoch 101, loss: 2.140528\n",
      "Epoch 102, loss: 2.104774\n",
      "Epoch 103, loss: 2.111067\n",
      "Epoch 104, loss: 2.132504\n",
      "Epoch 105, loss: 2.143201\n",
      "Epoch 106, loss: 2.177627\n",
      "Epoch 107, loss: 2.150769\n",
      "Epoch 108, loss: 2.148366\n",
      "Epoch 109, loss: 2.170638\n",
      "Epoch 110, loss: 2.145875\n",
      "Epoch 111, loss: 2.132089\n",
      "Epoch 112, loss: 2.090518\n",
      "Epoch 113, loss: 2.157592\n",
      "Epoch 114, loss: 2.126719\n",
      "Epoch 115, loss: 2.105378\n",
      "Epoch 116, loss: 2.163625\n",
      "Epoch 117, loss: 2.139884\n",
      "Epoch 118, loss: 2.141389\n",
      "Epoch 119, loss: 2.149085\n",
      "Epoch 120, loss: 2.114129\n",
      "Epoch 121, loss: 2.157894\n",
      "Epoch 122, loss: 2.147869\n",
      "Epoch 123, loss: 2.139206\n",
      "Epoch 124, loss: 2.086311\n",
      "Epoch 125, loss: 2.115364\n",
      "Epoch 126, loss: 2.076272\n",
      "Epoch 127, loss: 2.165320\n",
      "Epoch 128, loss: 2.105638\n",
      "Epoch 129, loss: 2.136256\n",
      "Epoch 130, loss: 2.086685\n",
      "Epoch 131, loss: 2.213414\n",
      "Epoch 132, loss: 2.214663\n",
      "Epoch 133, loss: 2.185185\n",
      "Epoch 134, loss: 2.125199\n",
      "Epoch 135, loss: 2.159519\n",
      "Epoch 136, loss: 2.129043\n",
      "Epoch 137, loss: 2.103245\n",
      "Epoch 138, loss: 2.080658\n",
      "Epoch 139, loss: 2.162217\n",
      "Epoch 140, loss: 2.128310\n",
      "Epoch 141, loss: 2.141466\n",
      "Epoch 142, loss: 2.116669\n",
      "Epoch 143, loss: 2.079733\n",
      "Epoch 144, loss: 2.102562\n",
      "Epoch 145, loss: 2.157108\n",
      "Epoch 146, loss: 2.149268\n",
      "Epoch 147, loss: 2.086589\n",
      "Epoch 148, loss: 2.130526\n",
      "Epoch 149, loss: 2.114078\n",
      "Epoch 150, loss: 2.167121\n",
      "Epoch 151, loss: 2.079445\n",
      "Epoch 152, loss: 2.129431\n",
      "Epoch 153, loss: 2.116591\n",
      "Epoch 154, loss: 2.074492\n",
      "Epoch 155, loss: 2.080121\n",
      "Epoch 156, loss: 2.132281\n",
      "Epoch 157, loss: 2.100465\n",
      "Epoch 158, loss: 2.129533\n",
      "Epoch 159, loss: 2.106220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160, loss: 2.112215\n",
      "Epoch 161, loss: 2.162848\n",
      "Epoch 162, loss: 2.097475\n",
      "Epoch 163, loss: 2.131359\n",
      "Epoch 164, loss: 2.091604\n",
      "Epoch 165, loss: 2.089751\n",
      "Epoch 166, loss: 2.135229\n",
      "Epoch 167, loss: 2.125959\n",
      "Epoch 168, loss: 2.104795\n",
      "Epoch 169, loss: 2.140940\n",
      "Epoch 170, loss: 2.143072\n",
      "Epoch 171, loss: 2.086619\n",
      "Epoch 172, loss: 2.134204\n",
      "Epoch 173, loss: 2.122184\n",
      "Epoch 174, loss: 2.182827\n",
      "Epoch 175, loss: 2.098222\n",
      "Epoch 176, loss: 2.097813\n",
      "Epoch 177, loss: 2.050174\n",
      "Epoch 178, loss: 2.131096\n",
      "Epoch 179, loss: 2.109318\n",
      "Epoch 180, loss: 2.157880\n",
      "Epoch 181, loss: 2.039032\n",
      "Epoch 182, loss: 2.203828\n",
      "Epoch 183, loss: 2.146859\n",
      "Epoch 184, loss: 2.096606\n",
      "Epoch 185, loss: 2.121216\n",
      "Epoch 186, loss: 2.085330\n",
      "Epoch 187, loss: 2.096949\n",
      "Epoch 188, loss: 2.120163\n",
      "Epoch 189, loss: 2.089899\n",
      "Epoch 190, loss: 2.122112\n",
      "Epoch 191, loss: 2.046683\n",
      "Epoch 192, loss: 2.102626\n",
      "Epoch 193, loss: 2.109744\n",
      "Epoch 194, loss: 2.109535\n",
      "Epoch 195, loss: 2.103648\n",
      "Epoch 196, loss: 2.097568\n",
      "Epoch 197, loss: 2.166815\n",
      "Epoch 198, loss: 2.098084\n",
      "Epoch 199, loss: 2.099309\n",
      "learning rate = 0.01 reg = 0.001 accuracy = 0.242\n",
      "Epoch 0, loss: 2.293853\n",
      "Epoch 1, loss: 2.286083\n",
      "Epoch 2, loss: 2.283572\n",
      "Epoch 3, loss: 2.274126\n",
      "Epoch 4, loss: 2.247419\n",
      "Epoch 5, loss: 2.256437\n",
      "Epoch 6, loss: 2.262667\n",
      "Epoch 7, loss: 2.259159\n",
      "Epoch 8, loss: 2.245682\n",
      "Epoch 9, loss: 2.252774\n",
      "Epoch 10, loss: 2.226706\n",
      "Epoch 11, loss: 2.222500\n",
      "Epoch 12, loss: 2.221920\n",
      "Epoch 13, loss: 2.228231\n",
      "Epoch 14, loss: 2.235539\n",
      "Epoch 15, loss: 2.204822\n",
      "Epoch 16, loss: 2.210716\n",
      "Epoch 17, loss: 2.202727\n",
      "Epoch 18, loss: 2.210276\n",
      "Epoch 19, loss: 2.202153\n",
      "Epoch 20, loss: 2.187795\n",
      "Epoch 21, loss: 2.204803\n",
      "Epoch 22, loss: 2.206405\n",
      "Epoch 23, loss: 2.215786\n",
      "Epoch 24, loss: 2.216400\n",
      "Epoch 25, loss: 2.198170\n",
      "Epoch 26, loss: 2.196234\n",
      "Epoch 27, loss: 2.169989\n",
      "Epoch 28, loss: 2.150567\n",
      "Epoch 29, loss: 2.172150\n",
      "Epoch 30, loss: 2.164781\n",
      "Epoch 31, loss: 2.180620\n",
      "Epoch 32, loss: 2.161806\n",
      "Epoch 33, loss: 2.132937\n",
      "Epoch 34, loss: 2.139425\n",
      "Epoch 35, loss: 2.204048\n",
      "Epoch 36, loss: 2.143661\n",
      "Epoch 37, loss: 2.184292\n",
      "Epoch 38, loss: 2.176175\n",
      "Epoch 39, loss: 2.142191\n",
      "Epoch 40, loss: 2.180798\n",
      "Epoch 41, loss: 2.128895\n",
      "Epoch 42, loss: 2.178505\n",
      "Epoch 43, loss: 2.198856\n",
      "Epoch 44, loss: 2.170019\n",
      "Epoch 45, loss: 2.155478\n",
      "Epoch 46, loss: 2.138672\n",
      "Epoch 47, loss: 2.177218\n",
      "Epoch 48, loss: 2.166303\n",
      "Epoch 49, loss: 2.199206\n",
      "Epoch 50, loss: 2.163143\n",
      "Epoch 51, loss: 2.118587\n",
      "Epoch 52, loss: 2.102636\n",
      "Epoch 53, loss: 2.224795\n",
      "Epoch 54, loss: 2.157084\n",
      "Epoch 55, loss: 2.139036\n",
      "Epoch 56, loss: 2.122206\n",
      "Epoch 57, loss: 2.160896\n",
      "Epoch 58, loss: 2.138831\n",
      "Epoch 59, loss: 2.164085\n",
      "Epoch 60, loss: 2.155655\n",
      "Epoch 61, loss: 2.166257\n",
      "Epoch 62, loss: 2.167529\n",
      "Epoch 63, loss: 2.160232\n",
      "Epoch 64, loss: 2.140177\n",
      "Epoch 65, loss: 2.141807\n",
      "Epoch 66, loss: 2.151476\n",
      "Epoch 67, loss: 2.165248\n",
      "Epoch 68, loss: 2.154574\n",
      "Epoch 69, loss: 2.190220\n",
      "Epoch 70, loss: 2.193448\n",
      "Epoch 71, loss: 2.162423\n",
      "Epoch 72, loss: 2.124196\n",
      "Epoch 73, loss: 2.178927\n",
      "Epoch 74, loss: 2.172513\n",
      "Epoch 75, loss: 2.148571\n",
      "Epoch 76, loss: 2.175594\n",
      "Epoch 77, loss: 2.120609\n",
      "Epoch 78, loss: 2.154422\n",
      "Epoch 79, loss: 2.147379\n",
      "Epoch 80, loss: 2.112145\n",
      "Epoch 81, loss: 2.105793\n",
      "Epoch 82, loss: 2.108347\n",
      "Epoch 83, loss: 2.142164\n",
      "Epoch 84, loss: 2.115996\n",
      "Epoch 85, loss: 2.132335\n",
      "Epoch 86, loss: 2.153209\n",
      "Epoch 87, loss: 2.146054\n",
      "Epoch 88, loss: 2.134720\n",
      "Epoch 89, loss: 2.138098\n",
      "Epoch 90, loss: 2.131392\n",
      "Epoch 91, loss: 2.154498\n",
      "Epoch 92, loss: 2.194729\n",
      "Epoch 93, loss: 2.145032\n",
      "Epoch 94, loss: 2.111236\n",
      "Epoch 95, loss: 2.141991\n",
      "Epoch 96, loss: 2.161953\n",
      "Epoch 97, loss: 2.085360\n",
      "Epoch 98, loss: 2.074098\n",
      "Epoch 99, loss: 2.114330\n",
      "Epoch 100, loss: 2.125951\n",
      "Epoch 101, loss: 2.113638\n",
      "Epoch 102, loss: 2.142607\n",
      "Epoch 103, loss: 2.074895\n",
      "Epoch 104, loss: 2.176796\n",
      "Epoch 105, loss: 2.097445\n",
      "Epoch 106, loss: 2.150171\n",
      "Epoch 107, loss: 2.133599\n",
      "Epoch 108, loss: 2.105376\n",
      "Epoch 109, loss: 2.122446\n",
      "Epoch 110, loss: 2.101891\n",
      "Epoch 111, loss: 2.159312\n",
      "Epoch 112, loss: 2.138053\n",
      "Epoch 113, loss: 2.177302\n",
      "Epoch 114, loss: 2.156376\n",
      "Epoch 115, loss: 2.111873\n",
      "Epoch 116, loss: 2.058097\n",
      "Epoch 117, loss: 2.163943\n",
      "Epoch 118, loss: 2.105446\n",
      "Epoch 119, loss: 2.172901\n",
      "Epoch 120, loss: 2.153015\n",
      "Epoch 121, loss: 2.149110\n",
      "Epoch 122, loss: 2.085945\n",
      "Epoch 123, loss: 2.133860\n",
      "Epoch 124, loss: 2.126072\n",
      "Epoch 125, loss: 2.147930\n",
      "Epoch 126, loss: 2.130054\n",
      "Epoch 127, loss: 2.100598\n",
      "Epoch 128, loss: 2.099706\n",
      "Epoch 129, loss: 2.141082\n",
      "Epoch 130, loss: 2.121118\n",
      "Epoch 131, loss: 2.093904\n",
      "Epoch 132, loss: 2.094195\n",
      "Epoch 133, loss: 2.117338\n",
      "Epoch 134, loss: 2.133311\n",
      "Epoch 135, loss: 2.115914\n",
      "Epoch 136, loss: 2.090179\n",
      "Epoch 137, loss: 2.115191\n",
      "Epoch 138, loss: 2.156714\n",
      "Epoch 139, loss: 2.138585\n",
      "Epoch 140, loss: 2.132083\n",
      "Epoch 141, loss: 2.074849\n",
      "Epoch 142, loss: 2.189002\n",
      "Epoch 143, loss: 2.135466\n",
      "Epoch 144, loss: 2.152038\n",
      "Epoch 145, loss: 2.060313\n",
      "Epoch 146, loss: 2.138341\n",
      "Epoch 147, loss: 2.091263\n",
      "Epoch 148, loss: 2.119974\n",
      "Epoch 149, loss: 2.091162\n",
      "Epoch 150, loss: 2.082432\n",
      "Epoch 151, loss: 2.087215\n",
      "Epoch 152, loss: 2.080425\n",
      "Epoch 153, loss: 2.154070\n",
      "Epoch 154, loss: 2.101112\n",
      "Epoch 155, loss: 2.090873\n",
      "Epoch 156, loss: 2.154606\n",
      "Epoch 157, loss: 2.090735\n",
      "Epoch 158, loss: 2.128636\n",
      "Epoch 159, loss: 2.082042\n",
      "Epoch 160, loss: 2.072922\n",
      "Epoch 161, loss: 2.111171\n",
      "Epoch 162, loss: 2.080833\n",
      "Epoch 163, loss: 2.053424\n",
      "Epoch 164, loss: 2.140553\n",
      "Epoch 165, loss: 2.084347\n",
      "Epoch 166, loss: 2.101356\n",
      "Epoch 167, loss: 2.111879\n",
      "Epoch 168, loss: 2.117736\n",
      "Epoch 169, loss: 2.131917\n",
      "Epoch 170, loss: 2.097428\n",
      "Epoch 171, loss: 2.138127\n",
      "Epoch 172, loss: 2.096433\n",
      "Epoch 173, loss: 2.190808\n",
      "Epoch 174, loss: 2.136418\n",
      "Epoch 175, loss: 2.106778\n",
      "Epoch 176, loss: 2.087794\n",
      "Epoch 177, loss: 2.147293\n",
      "Epoch 178, loss: 2.186284\n",
      "Epoch 179, loss: 2.076831\n",
      "Epoch 180, loss: 2.122676\n",
      "Epoch 181, loss: 2.087369\n",
      "Epoch 182, loss: 2.104042\n",
      "Epoch 183, loss: 2.103747\n",
      "Epoch 184, loss: 2.125804\n",
      "Epoch 185, loss: 2.054056\n",
      "Epoch 186, loss: 2.109768\n",
      "Epoch 187, loss: 2.063041\n",
      "Epoch 188, loss: 2.077441\n",
      "Epoch 189, loss: 2.082337\n",
      "Epoch 190, loss: 2.143491\n",
      "Epoch 191, loss: 2.154019\n",
      "Epoch 192, loss: 2.096995\n",
      "Epoch 193, loss: 2.085568\n",
      "Epoch 194, loss: 2.067219\n",
      "Epoch 195, loss: 2.123594\n",
      "Epoch 196, loss: 2.096629\n",
      "Epoch 197, loss: 2.124392\n",
      "Epoch 198, loss: 2.054388\n",
      "Epoch 199, loss: 2.104133\n",
      "learning rate = 0.01 reg = 0.0001 accuracy = 0.242\n",
      "Epoch 0, loss: 2.300216\n",
      "Epoch 1, loss: 2.287245\n",
      "Epoch 2, loss: 2.280153\n",
      "Epoch 3, loss: 2.271700\n",
      "Epoch 4, loss: 2.269742\n",
      "Epoch 5, loss: 2.256142\n",
      "Epoch 6, loss: 2.265077\n",
      "Epoch 7, loss: 2.242979\n",
      "Epoch 8, loss: 2.247714\n",
      "Epoch 9, loss: 2.238306\n",
      "Epoch 10, loss: 2.230581\n",
      "Epoch 11, loss: 2.228376\n",
      "Epoch 12, loss: 2.229384\n",
      "Epoch 13, loss: 2.213359\n",
      "Epoch 14, loss: 2.188343\n",
      "Epoch 15, loss: 2.212881\n",
      "Epoch 16, loss: 2.214980\n",
      "Epoch 17, loss: 2.222388\n",
      "Epoch 18, loss: 2.177850\n",
      "Epoch 19, loss: 2.201852\n",
      "Epoch 20, loss: 2.198068\n",
      "Epoch 21, loss: 2.161955\n",
      "Epoch 22, loss: 2.212947\n",
      "Epoch 23, loss: 2.191148\n",
      "Epoch 24, loss: 2.185749\n",
      "Epoch 25, loss: 2.208723\n",
      "Epoch 26, loss: 2.171084\n",
      "Epoch 27, loss: 2.186364\n",
      "Epoch 28, loss: 2.181942\n",
      "Epoch 29, loss: 2.181116\n",
      "Epoch 30, loss: 2.174945\n",
      "Epoch 31, loss: 2.207960\n",
      "Epoch 32, loss: 2.187762\n",
      "Epoch 33, loss: 2.161690\n",
      "Epoch 34, loss: 2.173640\n",
      "Epoch 35, loss: 2.172049\n",
      "Epoch 36, loss: 2.160272\n",
      "Epoch 37, loss: 2.156805\n",
      "Epoch 38, loss: 2.179172\n",
      "Epoch 39, loss: 2.143127\n",
      "Epoch 40, loss: 2.190292\n",
      "Epoch 41, loss: 2.180953\n",
      "Epoch 42, loss: 2.136803\n",
      "Epoch 43, loss: 2.202523\n",
      "Epoch 44, loss: 2.136149\n",
      "Epoch 45, loss: 2.220751\n",
      "Epoch 46, loss: 2.170264\n",
      "Epoch 47, loss: 2.140128\n",
      "Epoch 48, loss: 2.171551\n",
      "Epoch 49, loss: 2.107187\n",
      "Epoch 50, loss: 2.168632\n",
      "Epoch 51, loss: 2.156034\n",
      "Epoch 52, loss: 2.134755\n",
      "Epoch 53, loss: 2.136574\n",
      "Epoch 54, loss: 2.144397\n",
      "Epoch 55, loss: 2.141633\n",
      "Epoch 56, loss: 2.183429\n",
      "Epoch 57, loss: 2.112804\n",
      "Epoch 58, loss: 2.142859\n",
      "Epoch 59, loss: 2.158704\n",
      "Epoch 60, loss: 2.096793\n",
      "Epoch 61, loss: 2.102564\n",
      "Epoch 62, loss: 2.163171\n",
      "Epoch 63, loss: 2.148476\n",
      "Epoch 64, loss: 2.165242\n",
      "Epoch 65, loss: 2.150064\n",
      "Epoch 66, loss: 2.107921\n",
      "Epoch 67, loss: 2.180025\n",
      "Epoch 68, loss: 2.183731\n",
      "Epoch 69, loss: 2.127881\n",
      "Epoch 70, loss: 2.127560\n",
      "Epoch 71, loss: 2.156008\n",
      "Epoch 72, loss: 2.093756\n",
      "Epoch 73, loss: 2.125621\n",
      "Epoch 74, loss: 2.105408\n",
      "Epoch 75, loss: 2.138576\n",
      "Epoch 76, loss: 2.121618\n",
      "Epoch 77, loss: 2.114346\n",
      "Epoch 78, loss: 2.141002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, loss: 2.103758\n",
      "Epoch 80, loss: 2.131900\n",
      "Epoch 81, loss: 2.118069\n",
      "Epoch 82, loss: 2.108502\n",
      "Epoch 83, loss: 2.075456\n",
      "Epoch 84, loss: 2.100687\n",
      "Epoch 85, loss: 2.177878\n",
      "Epoch 86, loss: 2.143376\n",
      "Epoch 87, loss: 2.087031\n",
      "Epoch 88, loss: 2.094710\n",
      "Epoch 89, loss: 2.116433\n",
      "Epoch 90, loss: 2.120600\n",
      "Epoch 91, loss: 2.134797\n",
      "Epoch 92, loss: 2.131428\n",
      "Epoch 93, loss: 2.121844\n",
      "Epoch 94, loss: 2.103549\n",
      "Epoch 95, loss: 2.110280\n",
      "Epoch 96, loss: 2.110582\n",
      "Epoch 97, loss: 2.151364\n",
      "Epoch 98, loss: 2.155583\n",
      "Epoch 99, loss: 2.121312\n",
      "Epoch 100, loss: 2.092864\n",
      "Epoch 101, loss: 2.071964\n",
      "Epoch 102, loss: 2.133130\n",
      "Epoch 103, loss: 2.157924\n",
      "Epoch 104, loss: 2.189610\n",
      "Epoch 105, loss: 2.118812\n",
      "Epoch 106, loss: 2.105064\n",
      "Epoch 107, loss: 2.123274\n",
      "Epoch 108, loss: 2.122352\n",
      "Epoch 109, loss: 2.145070\n",
      "Epoch 110, loss: 2.146090\n",
      "Epoch 111, loss: 2.090389\n",
      "Epoch 112, loss: 2.101342\n",
      "Epoch 113, loss: 2.087179\n",
      "Epoch 114, loss: 2.152430\n",
      "Epoch 115, loss: 2.106766\n",
      "Epoch 116, loss: 2.163768\n",
      "Epoch 117, loss: 2.145205\n",
      "Epoch 118, loss: 2.106890\n",
      "Epoch 119, loss: 2.101636\n",
      "Epoch 120, loss: 2.072574\n",
      "Epoch 121, loss: 2.116448\n",
      "Epoch 122, loss: 2.165685\n",
      "Epoch 123, loss: 2.110272\n",
      "Epoch 124, loss: 2.181067\n",
      "Epoch 125, loss: 2.155091\n",
      "Epoch 126, loss: 2.150594\n",
      "Epoch 127, loss: 2.139419\n",
      "Epoch 128, loss: 2.132760\n",
      "Epoch 129, loss: 2.090643\n",
      "Epoch 130, loss: 2.201900\n",
      "Epoch 131, loss: 2.114273\n",
      "Epoch 132, loss: 2.163789\n",
      "Epoch 133, loss: 2.106456\n",
      "Epoch 134, loss: 2.104412\n",
      "Epoch 135, loss: 2.064162\n",
      "Epoch 136, loss: 2.088395\n",
      "Epoch 137, loss: 2.094376\n",
      "Epoch 138, loss: 2.159634\n",
      "Epoch 139, loss: 2.087311\n",
      "Epoch 140, loss: 2.060400\n",
      "Epoch 141, loss: 2.124900\n",
      "Epoch 142, loss: 2.131091\n",
      "Epoch 143, loss: 2.144759\n",
      "Epoch 144, loss: 2.112756\n",
      "Epoch 145, loss: 2.132043\n",
      "Epoch 146, loss: 2.173308\n",
      "Epoch 147, loss: 2.117280\n",
      "Epoch 148, loss: 2.129993\n",
      "Epoch 149, loss: 2.079593\n",
      "Epoch 150, loss: 2.132781\n",
      "Epoch 151, loss: 2.189683\n",
      "Epoch 152, loss: 2.114863\n",
      "Epoch 153, loss: 2.067046\n",
      "Epoch 154, loss: 2.050889\n",
      "Epoch 155, loss: 2.126348\n",
      "Epoch 156, loss: 2.101456\n",
      "Epoch 157, loss: 2.057627\n",
      "Epoch 158, loss: 2.112834\n",
      "Epoch 159, loss: 2.098729\n",
      "Epoch 160, loss: 2.073867\n",
      "Epoch 161, loss: 2.099631\n",
      "Epoch 162, loss: 2.142718\n",
      "Epoch 163, loss: 2.112576\n",
      "Epoch 164, loss: 2.083313\n",
      "Epoch 165, loss: 2.092826\n",
      "Epoch 166, loss: 2.052315\n",
      "Epoch 167, loss: 2.163631\n",
      "Epoch 168, loss: 2.104761\n",
      "Epoch 169, loss: 2.085199\n",
      "Epoch 170, loss: 2.158410\n",
      "Epoch 171, loss: 2.126447\n",
      "Epoch 172, loss: 2.118061\n",
      "Epoch 173, loss: 2.151701\n",
      "Epoch 174, loss: 2.073537\n",
      "Epoch 175, loss: 2.059933\n",
      "Epoch 176, loss: 2.115847\n",
      "Epoch 177, loss: 2.054698\n",
      "Epoch 178, loss: 2.130967\n",
      "Epoch 179, loss: 2.130073\n",
      "Epoch 180, loss: 2.066922\n",
      "Epoch 181, loss: 2.113243\n",
      "Epoch 182, loss: 2.099180\n",
      "Epoch 183, loss: 2.113422\n",
      "Epoch 184, loss: 2.106120\n",
      "Epoch 185, loss: 2.115692\n",
      "Epoch 186, loss: 2.073162\n",
      "Epoch 187, loss: 2.168190\n",
      "Epoch 188, loss: 2.097393\n",
      "Epoch 189, loss: 2.148463\n",
      "Epoch 190, loss: 2.099918\n",
      "Epoch 191, loss: 2.119430\n",
      "Epoch 192, loss: 2.086039\n",
      "Epoch 193, loss: 2.119556\n",
      "Epoch 194, loss: 2.095556\n",
      "Epoch 195, loss: 2.137035\n",
      "Epoch 196, loss: 2.110149\n",
      "Epoch 197, loss: 2.158785\n",
      "Epoch 198, loss: 2.091497\n",
      "Epoch 199, loss: 2.121160\n",
      "learning rate = 0.01 reg = 1e-05 accuracy = 0.246\n",
      "Epoch 0, loss: 2.296651\n",
      "Epoch 1, loss: 2.283449\n",
      "Epoch 2, loss: 2.280405\n",
      "Epoch 3, loss: 2.267359\n",
      "Epoch 4, loss: 2.259425\n",
      "Epoch 5, loss: 2.266005\n",
      "Epoch 6, loss: 2.261363\n",
      "Epoch 7, loss: 2.258857\n",
      "Epoch 8, loss: 2.254345\n",
      "Epoch 9, loss: 2.236563\n",
      "Epoch 10, loss: 2.239632\n",
      "Epoch 11, loss: 2.239060\n",
      "Epoch 12, loss: 2.207195\n",
      "Epoch 13, loss: 2.223546\n",
      "Epoch 14, loss: 2.211265\n",
      "Epoch 15, loss: 2.233511\n",
      "Epoch 16, loss: 2.202677\n",
      "Epoch 17, loss: 2.196018\n",
      "Epoch 18, loss: 2.221596\n",
      "Epoch 19, loss: 2.204984\n",
      "Epoch 20, loss: 2.184067\n",
      "Epoch 21, loss: 2.205678\n",
      "Epoch 22, loss: 2.206254\n",
      "Epoch 23, loss: 2.208746\n",
      "Epoch 24, loss: 2.172913\n",
      "Epoch 25, loss: 2.199028\n",
      "Epoch 26, loss: 2.181327\n",
      "Epoch 27, loss: 2.182592\n",
      "Epoch 28, loss: 2.178689\n",
      "Epoch 29, loss: 2.181128\n",
      "Epoch 30, loss: 2.158140\n",
      "Epoch 31, loss: 2.203557\n",
      "Epoch 32, loss: 2.194848\n",
      "Epoch 33, loss: 2.211955\n",
      "Epoch 34, loss: 2.182448\n",
      "Epoch 35, loss: 2.202025\n",
      "Epoch 36, loss: 2.186003\n",
      "Epoch 37, loss: 2.110301\n",
      "Epoch 38, loss: 2.163213\n",
      "Epoch 39, loss: 2.155774\n",
      "Epoch 40, loss: 2.199989\n",
      "Epoch 41, loss: 2.118908\n",
      "Epoch 42, loss: 2.171761\n",
      "Epoch 43, loss: 2.137657\n",
      "Epoch 44, loss: 2.204438\n",
      "Epoch 45, loss: 2.164868\n",
      "Epoch 46, loss: 2.172068\n",
      "Epoch 47, loss: 2.177034\n",
      "Epoch 48, loss: 2.132598\n",
      "Epoch 49, loss: 2.160881\n",
      "Epoch 50, loss: 2.153401\n",
      "Epoch 51, loss: 2.140237\n",
      "Epoch 52, loss: 2.183189\n",
      "Epoch 53, loss: 2.145351\n",
      "Epoch 54, loss: 2.145280\n",
      "Epoch 55, loss: 2.210559\n",
      "Epoch 56, loss: 2.106858\n",
      "Epoch 57, loss: 2.155858\n",
      "Epoch 58, loss: 2.169278\n",
      "Epoch 59, loss: 2.104036\n",
      "Epoch 60, loss: 2.147359\n",
      "Epoch 61, loss: 2.138607\n",
      "Epoch 62, loss: 2.181764\n",
      "Epoch 63, loss: 2.151462\n",
      "Epoch 64, loss: 2.123158\n",
      "Epoch 65, loss: 2.136337\n",
      "Epoch 66, loss: 2.192961\n",
      "Epoch 67, loss: 2.117933\n",
      "Epoch 68, loss: 2.143415\n",
      "Epoch 69, loss: 2.181088\n",
      "Epoch 70, loss: 2.133557\n",
      "Epoch 71, loss: 2.102039\n",
      "Epoch 72, loss: 2.134300\n",
      "Epoch 73, loss: 2.155676\n",
      "Epoch 74, loss: 2.174375\n",
      "Epoch 75, loss: 2.166155\n",
      "Epoch 76, loss: 2.104327\n",
      "Epoch 77, loss: 2.153562\n",
      "Epoch 78, loss: 2.107518\n",
      "Epoch 79, loss: 2.122527\n",
      "Epoch 80, loss: 2.153249\n",
      "Epoch 81, loss: 2.123416\n",
      "Epoch 82, loss: 2.111106\n",
      "Epoch 83, loss: 2.151991\n",
      "Epoch 84, loss: 2.057341\n",
      "Epoch 85, loss: 2.136665\n",
      "Epoch 86, loss: 2.128247\n",
      "Epoch 87, loss: 2.117830\n",
      "Epoch 88, loss: 2.177792\n",
      "Epoch 89, loss: 2.179421\n",
      "Epoch 90, loss: 2.102988\n",
      "Epoch 91, loss: 2.109658\n",
      "Epoch 92, loss: 2.109096\n",
      "Epoch 93, loss: 2.089408\n",
      "Epoch 94, loss: 2.083638\n",
      "Epoch 95, loss: 2.138160\n",
      "Epoch 96, loss: 2.111197\n",
      "Epoch 97, loss: 2.088171\n",
      "Epoch 98, loss: 2.114427\n",
      "Epoch 99, loss: 2.137467\n",
      "Epoch 100, loss: 2.164319\n",
      "Epoch 101, loss: 2.142336\n",
      "Epoch 102, loss: 2.074503\n",
      "Epoch 103, loss: 2.136725\n",
      "Epoch 104, loss: 2.118169\n",
      "Epoch 105, loss: 2.131694\n",
      "Epoch 106, loss: 2.176649\n",
      "Epoch 107, loss: 2.135473\n",
      "Epoch 108, loss: 2.127892\n",
      "Epoch 109, loss: 2.122528\n",
      "Epoch 110, loss: 2.125317\n",
      "Epoch 111, loss: 2.137807\n",
      "Epoch 112, loss: 2.172809\n",
      "Epoch 113, loss: 2.150390\n",
      "Epoch 114, loss: 2.116727\n",
      "Epoch 115, loss: 2.158088\n",
      "Epoch 116, loss: 2.095812\n",
      "Epoch 117, loss: 2.099513\n",
      "Epoch 118, loss: 2.139586\n",
      "Epoch 119, loss: 2.127990\n",
      "Epoch 120, loss: 2.147053\n",
      "Epoch 121, loss: 2.147717\n",
      "Epoch 122, loss: 2.146921\n",
      "Epoch 123, loss: 2.108560\n",
      "Epoch 124, loss: 2.141214\n",
      "Epoch 125, loss: 2.146288\n",
      "Epoch 126, loss: 2.116363\n",
      "Epoch 127, loss: 2.120083\n",
      "Epoch 128, loss: 2.113945\n",
      "Epoch 129, loss: 2.040509\n",
      "Epoch 130, loss: 2.089738\n",
      "Epoch 131, loss: 2.134253\n",
      "Epoch 132, loss: 2.060727\n",
      "Epoch 133, loss: 2.104371\n",
      "Epoch 134, loss: 2.114150\n",
      "Epoch 135, loss: 2.097527\n",
      "Epoch 136, loss: 2.101180\n",
      "Epoch 137, loss: 2.121065\n",
      "Epoch 138, loss: 2.117507\n",
      "Epoch 139, loss: 2.133339\n",
      "Epoch 140, loss: 2.139009\n",
      "Epoch 141, loss: 2.115615\n",
      "Epoch 142, loss: 2.077253\n",
      "Epoch 143, loss: 2.157099\n",
      "Epoch 144, loss: 2.117462\n",
      "Epoch 145, loss: 2.098680\n",
      "Epoch 146, loss: 2.133026\n",
      "Epoch 147, loss: 2.084461\n",
      "Epoch 148, loss: 2.036580\n",
      "Epoch 149, loss: 2.104265\n",
      "Epoch 150, loss: 2.083606\n",
      "Epoch 151, loss: 2.093063\n",
      "Epoch 152, loss: 2.122713\n",
      "Epoch 153, loss: 2.081148\n",
      "Epoch 154, loss: 2.060993\n",
      "Epoch 155, loss: 2.119824\n",
      "Epoch 156, loss: 2.146540\n",
      "Epoch 157, loss: 2.137805\n",
      "Epoch 158, loss: 2.181101\n",
      "Epoch 159, loss: 2.020680\n",
      "Epoch 160, loss: 2.062015\n",
      "Epoch 161, loss: 2.160719\n",
      "Epoch 162, loss: 2.102568\n",
      "Epoch 163, loss: 2.068304\n",
      "Epoch 164, loss: 2.089281\n",
      "Epoch 165, loss: 2.077097\n",
      "Epoch 166, loss: 2.094421\n",
      "Epoch 167, loss: 2.144484\n",
      "Epoch 168, loss: 2.110936\n",
      "Epoch 169, loss: 2.092643\n",
      "Epoch 170, loss: 2.146116\n",
      "Epoch 171, loss: 2.069746\n",
      "Epoch 172, loss: 2.115838\n",
      "Epoch 173, loss: 2.105762\n",
      "Epoch 174, loss: 2.074685\n",
      "Epoch 175, loss: 2.018278\n",
      "Epoch 176, loss: 2.136978\n",
      "Epoch 177, loss: 2.108161\n",
      "Epoch 178, loss: 2.166090\n",
      "Epoch 179, loss: 2.103420\n",
      "Epoch 180, loss: 2.078699\n",
      "Epoch 181, loss: 2.108033\n",
      "Epoch 182, loss: 2.079402\n",
      "Epoch 183, loss: 2.094013\n",
      "Epoch 184, loss: 2.128559\n",
      "Epoch 185, loss: 2.050965\n",
      "Epoch 186, loss: 2.183909\n",
      "Epoch 187, loss: 2.079426\n",
      "Epoch 188, loss: 2.066745\n",
      "Epoch 189, loss: 2.080718\n",
      "Epoch 190, loss: 2.066111\n",
      "Epoch 191, loss: 2.044751\n",
      "Epoch 192, loss: 2.144040\n",
      "Epoch 193, loss: 2.054252\n",
      "Epoch 194, loss: 2.125849\n",
      "Epoch 195, loss: 2.084056\n",
      "Epoch 196, loss: 2.130195\n",
      "Epoch 197, loss: 2.082474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198, loss: 2.129608\n",
      "Epoch 199, loss: 2.076361\n",
      "learning rate = 0.01 reg = 1e-06 accuracy = 0.243\n",
      "Epoch 0, loss: 2.397219\n",
      "Epoch 1, loss: 2.330136\n",
      "Epoch 2, loss: 2.310056\n",
      "Epoch 3, loss: 2.304369\n",
      "Epoch 4, loss: 2.303122\n",
      "Epoch 5, loss: 2.302177\n",
      "Epoch 6, loss: 2.302061\n",
      "Epoch 7, loss: 2.302764\n",
      "Epoch 8, loss: 2.301854\n",
      "Epoch 9, loss: 2.301870\n",
      "Epoch 10, loss: 2.302564\n",
      "Epoch 11, loss: 2.301190\n",
      "Epoch 12, loss: 2.301830\n",
      "Epoch 13, loss: 2.300999\n",
      "Epoch 14, loss: 2.302303\n",
      "Epoch 15, loss: 2.302485\n",
      "Epoch 16, loss: 2.301208\n",
      "Epoch 17, loss: 2.302017\n",
      "Epoch 18, loss: 2.302387\n",
      "Epoch 19, loss: 2.301887\n",
      "Epoch 20, loss: 2.302333\n",
      "Epoch 21, loss: 2.300987\n",
      "Epoch 22, loss: 2.301644\n",
      "Epoch 23, loss: 2.301677\n",
      "Epoch 24, loss: 2.302556\n",
      "Epoch 25, loss: 2.302193\n",
      "Epoch 26, loss: 2.302227\n",
      "Epoch 27, loss: 2.302349\n",
      "Epoch 28, loss: 2.301359\n",
      "Epoch 29, loss: 2.301409\n",
      "Epoch 30, loss: 2.302842\n",
      "Epoch 31, loss: 2.303137\n",
      "Epoch 32, loss: 2.301928\n",
      "Epoch 33, loss: 2.301659\n",
      "Epoch 34, loss: 2.302121\n",
      "Epoch 35, loss: 2.301525\n",
      "Epoch 36, loss: 2.301408\n",
      "Epoch 37, loss: 2.302612\n",
      "Epoch 38, loss: 2.301538\n",
      "Epoch 39, loss: 2.301467\n",
      "Epoch 40, loss: 2.302018\n",
      "Epoch 41, loss: 2.301115\n",
      "Epoch 42, loss: 2.302427\n",
      "Epoch 43, loss: 2.302300\n",
      "Epoch 44, loss: 2.301716\n",
      "Epoch 45, loss: 2.301825\n",
      "Epoch 46, loss: 2.301976\n",
      "Epoch 47, loss: 2.301323\n",
      "Epoch 48, loss: 2.301269\n",
      "Epoch 49, loss: 2.302708\n",
      "Epoch 50, loss: 2.301756\n",
      "Epoch 51, loss: 2.301677\n",
      "Epoch 52, loss: 2.301440\n",
      "Epoch 53, loss: 2.302054\n",
      "Epoch 54, loss: 2.301210\n",
      "Epoch 55, loss: 2.301536\n",
      "Epoch 56, loss: 2.301429\n",
      "Epoch 57, loss: 2.301800\n",
      "Epoch 58, loss: 2.301325\n",
      "Epoch 59, loss: 2.301959\n",
      "Epoch 60, loss: 2.303076\n",
      "Epoch 61, loss: 2.301857\n",
      "Epoch 62, loss: 2.301831\n",
      "Epoch 63, loss: 2.301768\n",
      "Epoch 64, loss: 2.302838\n",
      "Epoch 65, loss: 2.302251\n",
      "Epoch 66, loss: 2.301763\n",
      "Epoch 67, loss: 2.302411\n",
      "Epoch 68, loss: 2.302569\n",
      "Epoch 69, loss: 2.301058\n",
      "Epoch 70, loss: 2.302421\n",
      "Epoch 71, loss: 2.302506\n",
      "Epoch 72, loss: 2.302042\n",
      "Epoch 73, loss: 2.301976\n",
      "Epoch 74, loss: 2.301777\n",
      "Epoch 75, loss: 2.301665\n",
      "Epoch 76, loss: 2.301978\n",
      "Epoch 77, loss: 2.302289\n",
      "Epoch 78, loss: 2.301405\n",
      "Epoch 79, loss: 2.301994\n",
      "Epoch 80, loss: 2.302164\n",
      "Epoch 81, loss: 2.302421\n",
      "Epoch 82, loss: 2.302179\n",
      "Epoch 83, loss: 2.301067\n",
      "Epoch 84, loss: 2.302015\n",
      "Epoch 85, loss: 2.301648\n",
      "Epoch 86, loss: 2.302964\n",
      "Epoch 87, loss: 2.301786\n",
      "Epoch 88, loss: 2.301664\n",
      "Epoch 89, loss: 2.302866\n",
      "Epoch 90, loss: 2.302221\n",
      "Epoch 91, loss: 2.301870\n",
      "Epoch 92, loss: 2.302998\n",
      "Epoch 93, loss: 2.301209\n",
      "Epoch 94, loss: 2.301237\n",
      "Epoch 95, loss: 2.301375\n",
      "Epoch 96, loss: 2.302046\n",
      "Epoch 97, loss: 2.302061\n",
      "Epoch 98, loss: 2.302837\n",
      "Epoch 99, loss: 2.302443\n",
      "Epoch 100, loss: 2.302223\n",
      "Epoch 101, loss: 2.302270\n",
      "Epoch 102, loss: 2.302625\n",
      "Epoch 103, loss: 2.301808\n",
      "Epoch 104, loss: 2.301169\n",
      "Epoch 105, loss: 2.303342\n",
      "Epoch 106, loss: 2.301383\n",
      "Epoch 107, loss: 2.302610\n",
      "Epoch 108, loss: 2.301588\n",
      "Epoch 109, loss: 2.301587\n",
      "Epoch 110, loss: 2.301789\n",
      "Epoch 111, loss: 2.301625\n",
      "Epoch 112, loss: 2.302853\n",
      "Epoch 113, loss: 2.301597\n",
      "Epoch 114, loss: 2.302372\n",
      "Epoch 115, loss: 2.301779\n",
      "Epoch 116, loss: 2.301972\n",
      "Epoch 117, loss: 2.302391\n",
      "Epoch 118, loss: 2.302431\n",
      "Epoch 119, loss: 2.302582\n",
      "Epoch 120, loss: 2.302504\n",
      "Epoch 121, loss: 2.301767\n",
      "Epoch 122, loss: 2.301955\n",
      "Epoch 123, loss: 2.302285\n",
      "Epoch 124, loss: 2.302298\n",
      "Epoch 125, loss: 2.301847\n",
      "Epoch 126, loss: 2.301942\n",
      "Epoch 127, loss: 2.302085\n",
      "Epoch 128, loss: 2.302106\n",
      "Epoch 129, loss: 2.302287\n",
      "Epoch 130, loss: 2.302326\n",
      "Epoch 131, loss: 2.301245\n",
      "Epoch 132, loss: 2.302517\n",
      "Epoch 133, loss: 2.302087\n",
      "Epoch 134, loss: 2.301749\n",
      "Epoch 135, loss: 2.302007\n",
      "Epoch 136, loss: 2.302423\n",
      "Epoch 137, loss: 2.302158\n",
      "Epoch 138, loss: 2.302148\n",
      "Epoch 139, loss: 2.301780\n",
      "Epoch 140, loss: 2.301997\n",
      "Epoch 141, loss: 2.301713\n",
      "Epoch 142, loss: 2.301999\n",
      "Epoch 143, loss: 2.302015\n",
      "Epoch 144, loss: 2.302285\n",
      "Epoch 145, loss: 2.302117\n",
      "Epoch 146, loss: 2.301501\n",
      "Epoch 147, loss: 2.302353\n",
      "Epoch 148, loss: 2.301515\n",
      "Epoch 149, loss: 2.302135\n",
      "Epoch 150, loss: 2.301346\n",
      "Epoch 151, loss: 2.301857\n",
      "Epoch 152, loss: 2.301976\n",
      "Epoch 153, loss: 2.302315\n",
      "Epoch 154, loss: 2.302022\n",
      "Epoch 155, loss: 2.301805\n",
      "Epoch 156, loss: 2.302584\n",
      "Epoch 157, loss: 2.301989\n",
      "Epoch 158, loss: 2.302004\n",
      "Epoch 159, loss: 2.301685\n",
      "Epoch 160, loss: 2.301881\n",
      "Epoch 161, loss: 2.302261\n",
      "Epoch 162, loss: 2.302572\n",
      "Epoch 163, loss: 2.302208\n",
      "Epoch 164, loss: 2.301510\n",
      "Epoch 165, loss: 2.301325\n",
      "Epoch 166, loss: 2.301193\n",
      "Epoch 167, loss: 2.301815\n",
      "Epoch 168, loss: 2.303149\n",
      "Epoch 169, loss: 2.302202\n",
      "Epoch 170, loss: 2.302019\n",
      "Epoch 171, loss: 2.301525\n",
      "Epoch 172, loss: 2.301236\n",
      "Epoch 173, loss: 2.301597\n",
      "Epoch 174, loss: 2.301482\n",
      "Epoch 175, loss: 2.302964\n",
      "Epoch 176, loss: 2.301240\n",
      "Epoch 177, loss: 2.301072\n",
      "Epoch 178, loss: 2.302096\n",
      "Epoch 179, loss: 2.301785\n",
      "Epoch 180, loss: 2.301189\n",
      "Epoch 181, loss: 2.301823\n",
      "Epoch 182, loss: 2.301390\n",
      "Epoch 183, loss: 2.301732\n",
      "Epoch 184, loss: 2.301536\n",
      "Epoch 185, loss: 2.301802\n",
      "Epoch 186, loss: 2.301875\n",
      "Epoch 187, loss: 2.301481\n",
      "Epoch 188, loss: 2.301579\n",
      "Epoch 189, loss: 2.304354\n",
      "Epoch 190, loss: 2.301238\n",
      "Epoch 191, loss: 2.302207\n",
      "Epoch 192, loss: 2.301372\n",
      "Epoch 193, loss: 2.302181\n",
      "Epoch 194, loss: 2.302312\n",
      "Epoch 195, loss: 2.302300\n",
      "Epoch 196, loss: 2.302179\n",
      "Epoch 197, loss: 2.302148\n",
      "Epoch 198, loss: 2.301722\n",
      "Epoch 199, loss: 2.302521\n",
      "learning rate = 0.001 reg = 10 accuracy = 0.128\n",
      "Epoch 0, loss: 2.329685\n",
      "Epoch 1, loss: 2.324018\n",
      "Epoch 2, loss: 2.322898\n",
      "Epoch 3, loss: 2.318362\n",
      "Epoch 4, loss: 2.315550\n",
      "Epoch 5, loss: 2.314145\n",
      "Epoch 6, loss: 2.311300\n",
      "Epoch 7, loss: 2.313513\n",
      "Epoch 8, loss: 2.307987\n",
      "Epoch 9, loss: 2.307249\n",
      "Epoch 10, loss: 2.307223\n",
      "Epoch 11, loss: 2.303197\n",
      "Epoch 12, loss: 2.304179\n",
      "Epoch 13, loss: 2.306185\n",
      "Epoch 14, loss: 2.299040\n",
      "Epoch 15, loss: 2.302473\n",
      "Epoch 16, loss: 2.300710\n",
      "Epoch 17, loss: 2.301457\n",
      "Epoch 18, loss: 2.301472\n",
      "Epoch 19, loss: 2.300774\n",
      "Epoch 20, loss: 2.298943\n",
      "Epoch 21, loss: 2.298024\n",
      "Epoch 22, loss: 2.299015\n",
      "Epoch 23, loss: 2.293762\n",
      "Epoch 24, loss: 2.296836\n",
      "Epoch 25, loss: 2.296106\n",
      "Epoch 26, loss: 2.297757\n",
      "Epoch 27, loss: 2.299536\n",
      "Epoch 28, loss: 2.298416\n",
      "Epoch 29, loss: 2.296244\n",
      "Epoch 30, loss: 2.294346\n",
      "Epoch 31, loss: 2.296178\n",
      "Epoch 32, loss: 2.301298\n",
      "Epoch 33, loss: 2.296081\n",
      "Epoch 34, loss: 2.296003\n",
      "Epoch 35, loss: 2.299422\n",
      "Epoch 36, loss: 2.293157\n",
      "Epoch 37, loss: 2.296262\n",
      "Epoch 38, loss: 2.295089\n",
      "Epoch 39, loss: 2.290890\n",
      "Epoch 40, loss: 2.292179\n",
      "Epoch 41, loss: 2.296346\n",
      "Epoch 42, loss: 2.297473\n",
      "Epoch 43, loss: 2.298130\n",
      "Epoch 44, loss: 2.297855\n",
      "Epoch 45, loss: 2.296438\n",
      "Epoch 46, loss: 2.297718\n",
      "Epoch 47, loss: 2.296765\n",
      "Epoch 48, loss: 2.299897\n",
      "Epoch 49, loss: 2.297176\n",
      "Epoch 50, loss: 2.293277\n",
      "Epoch 51, loss: 2.295120\n",
      "Epoch 52, loss: 2.294273\n",
      "Epoch 53, loss: 2.292494\n",
      "Epoch 54, loss: 2.294324\n",
      "Epoch 55, loss: 2.292633\n",
      "Epoch 56, loss: 2.296655\n",
      "Epoch 57, loss: 2.292893\n",
      "Epoch 58, loss: 2.296487\n",
      "Epoch 59, loss: 2.296161\n",
      "Epoch 60, loss: 2.294744\n",
      "Epoch 61, loss: 2.296477\n",
      "Epoch 62, loss: 2.293624\n",
      "Epoch 63, loss: 2.297125\n",
      "Epoch 64, loss: 2.295597\n",
      "Epoch 65, loss: 2.295189\n",
      "Epoch 66, loss: 2.300412\n",
      "Epoch 67, loss: 2.295494\n",
      "Epoch 68, loss: 2.295445\n",
      "Epoch 69, loss: 2.296485\n",
      "Epoch 70, loss: 2.294506\n",
      "Epoch 71, loss: 2.298491\n",
      "Epoch 72, loss: 2.299902\n",
      "Epoch 73, loss: 2.298865\n",
      "Epoch 74, loss: 2.297368\n",
      "Epoch 75, loss: 2.295560\n",
      "Epoch 76, loss: 2.290951\n",
      "Epoch 77, loss: 2.294113\n",
      "Epoch 78, loss: 2.301256\n",
      "Epoch 79, loss: 2.294058\n",
      "Epoch 80, loss: 2.297889\n",
      "Epoch 81, loss: 2.294983\n",
      "Epoch 82, loss: 2.292637\n",
      "Epoch 83, loss: 2.300624\n",
      "Epoch 84, loss: 2.294737\n",
      "Epoch 85, loss: 2.297556\n",
      "Epoch 86, loss: 2.293171\n",
      "Epoch 87, loss: 2.294733\n",
      "Epoch 88, loss: 2.294290\n",
      "Epoch 89, loss: 2.291899\n",
      "Epoch 90, loss: 2.295289\n",
      "Epoch 91, loss: 2.301559\n",
      "Epoch 92, loss: 2.293913\n",
      "Epoch 93, loss: 2.296743\n",
      "Epoch 94, loss: 2.293086\n",
      "Epoch 95, loss: 2.296303\n",
      "Epoch 96, loss: 2.292107\n",
      "Epoch 97, loss: 2.295192\n",
      "Epoch 98, loss: 2.294569\n",
      "Epoch 99, loss: 2.294290\n",
      "Epoch 100, loss: 2.294794\n",
      "Epoch 101, loss: 2.300618\n",
      "Epoch 102, loss: 2.296819\n",
      "Epoch 103, loss: 2.300856\n",
      "Epoch 104, loss: 2.297438\n",
      "Epoch 105, loss: 2.297196\n",
      "Epoch 106, loss: 2.296357\n",
      "Epoch 107, loss: 2.294110\n",
      "Epoch 108, loss: 2.298014\n",
      "Epoch 109, loss: 2.296911\n",
      "Epoch 110, loss: 2.295596\n",
      "Epoch 111, loss: 2.296385\n",
      "Epoch 112, loss: 2.289564\n",
      "Epoch 113, loss: 2.300160\n",
      "Epoch 114, loss: 2.293118\n",
      "Epoch 115, loss: 2.292959\n",
      "Epoch 116, loss: 2.300094\n",
      "Epoch 117, loss: 2.295435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, loss: 2.293251\n",
      "Epoch 119, loss: 2.295405\n",
      "Epoch 120, loss: 2.292471\n",
      "Epoch 121, loss: 2.297248\n",
      "Epoch 122, loss: 2.293791\n",
      "Epoch 123, loss: 2.297629\n",
      "Epoch 124, loss: 2.297276\n",
      "Epoch 125, loss: 2.298001\n",
      "Epoch 126, loss: 2.298484\n",
      "Epoch 127, loss: 2.292595\n",
      "Epoch 128, loss: 2.300264\n",
      "Epoch 129, loss: 2.298551\n",
      "Epoch 130, loss: 2.292717\n",
      "Epoch 131, loss: 2.293915\n",
      "Epoch 132, loss: 2.295585\n",
      "Epoch 133, loss: 2.296604\n",
      "Epoch 134, loss: 2.292257\n",
      "Epoch 135, loss: 2.295245\n",
      "Epoch 136, loss: 2.296571\n",
      "Epoch 137, loss: 2.291059\n",
      "Epoch 138, loss: 2.293104\n",
      "Epoch 139, loss: 2.294571\n",
      "Epoch 140, loss: 2.290717\n",
      "Epoch 141, loss: 2.298048\n",
      "Epoch 142, loss: 2.294463\n",
      "Epoch 143, loss: 2.293744\n",
      "Epoch 144, loss: 2.296963\n",
      "Epoch 145, loss: 2.295855\n",
      "Epoch 146, loss: 2.294507\n",
      "Epoch 147, loss: 2.294329\n",
      "Epoch 148, loss: 2.298230\n",
      "Epoch 149, loss: 2.293997\n",
      "Epoch 150, loss: 2.296614\n",
      "Epoch 151, loss: 2.291204\n",
      "Epoch 152, loss: 2.295005\n",
      "Epoch 153, loss: 2.294621\n",
      "Epoch 154, loss: 2.296738\n",
      "Epoch 155, loss: 2.293575\n",
      "Epoch 156, loss: 2.293471\n",
      "Epoch 157, loss: 2.297241\n",
      "Epoch 158, loss: 2.295105\n",
      "Epoch 159, loss: 2.290874\n",
      "Epoch 160, loss: 2.293966\n",
      "Epoch 161, loss: 2.293777\n",
      "Epoch 162, loss: 2.294177\n",
      "Epoch 163, loss: 2.293321\n",
      "Epoch 164, loss: 2.296760\n",
      "Epoch 165, loss: 2.296470\n",
      "Epoch 166, loss: 2.297136\n",
      "Epoch 167, loss: 2.295179\n",
      "Epoch 168, loss: 2.295981\n",
      "Epoch 169, loss: 2.295886\n",
      "Epoch 170, loss: 2.296644\n",
      "Epoch 171, loss: 2.300294\n",
      "Epoch 172, loss: 2.295113\n",
      "Epoch 173, loss: 2.293486\n",
      "Epoch 174, loss: 2.294755\n",
      "Epoch 175, loss: 2.295225\n",
      "Epoch 176, loss: 2.290833\n",
      "Epoch 177, loss: 2.298808\n",
      "Epoch 178, loss: 2.299059\n",
      "Epoch 179, loss: 2.296265\n",
      "Epoch 180, loss: 2.301403\n",
      "Epoch 181, loss: 2.293416\n",
      "Epoch 182, loss: 2.295973\n",
      "Epoch 183, loss: 2.293801\n",
      "Epoch 184, loss: 2.295274\n",
      "Epoch 185, loss: 2.296979\n",
      "Epoch 186, loss: 2.294347\n",
      "Epoch 187, loss: 2.294121\n",
      "Epoch 188, loss: 2.300633\n",
      "Epoch 189, loss: 2.297373\n",
      "Epoch 190, loss: 2.292389\n",
      "Epoch 191, loss: 2.297358\n",
      "Epoch 192, loss: 2.298791\n",
      "Epoch 193, loss: 2.294504\n",
      "Epoch 194, loss: 2.293469\n",
      "Epoch 195, loss: 2.297317\n",
      "Epoch 196, loss: 2.296748\n",
      "Epoch 197, loss: 2.297711\n",
      "Epoch 198, loss: 2.293146\n",
      "Epoch 199, loss: 2.296753\n",
      "learning rate = 0.001 reg = 1 accuracy = 0.173\n",
      "Epoch 0, loss: 2.304181\n",
      "Epoch 1, loss: 2.304448\n",
      "Epoch 2, loss: 2.302112\n",
      "Epoch 3, loss: 2.300982\n",
      "Epoch 4, loss: 2.301552\n",
      "Epoch 5, loss: 2.302839\n",
      "Epoch 6, loss: 2.297373\n",
      "Epoch 7, loss: 2.297096\n",
      "Epoch 8, loss: 2.298330\n",
      "Epoch 9, loss: 2.297110\n",
      "Epoch 10, loss: 2.293847\n",
      "Epoch 11, loss: 2.296178\n",
      "Epoch 12, loss: 2.293011\n",
      "Epoch 13, loss: 2.292648\n",
      "Epoch 14, loss: 2.290278\n",
      "Epoch 15, loss: 2.290063\n",
      "Epoch 16, loss: 2.292186\n",
      "Epoch 17, loss: 2.294964\n",
      "Epoch 18, loss: 2.290445\n",
      "Epoch 19, loss: 2.290211\n",
      "Epoch 20, loss: 2.290093\n",
      "Epoch 21, loss: 2.290751\n",
      "Epoch 22, loss: 2.289572\n",
      "Epoch 23, loss: 2.292488\n",
      "Epoch 24, loss: 2.280953\n",
      "Epoch 25, loss: 2.284362\n",
      "Epoch 26, loss: 2.285449\n",
      "Epoch 27, loss: 2.290975\n",
      "Epoch 28, loss: 2.288254\n",
      "Epoch 29, loss: 2.289954\n",
      "Epoch 30, loss: 2.281997\n",
      "Epoch 31, loss: 2.284535\n",
      "Epoch 32, loss: 2.282937\n",
      "Epoch 33, loss: 2.281815\n",
      "Epoch 34, loss: 2.276493\n",
      "Epoch 35, loss: 2.277443\n",
      "Epoch 36, loss: 2.280568\n",
      "Epoch 37, loss: 2.279555\n",
      "Epoch 38, loss: 2.278341\n",
      "Epoch 39, loss: 2.280507\n",
      "Epoch 40, loss: 2.287229\n",
      "Epoch 41, loss: 2.287751\n",
      "Epoch 42, loss: 2.267067\n",
      "Epoch 43, loss: 2.279703\n",
      "Epoch 44, loss: 2.270466\n",
      "Epoch 45, loss: 2.271664\n",
      "Epoch 46, loss: 2.267358\n",
      "Epoch 47, loss: 2.275327\n",
      "Epoch 48, loss: 2.279034\n",
      "Epoch 49, loss: 2.274831\n",
      "Epoch 50, loss: 2.283699\n",
      "Epoch 51, loss: 2.269469\n",
      "Epoch 52, loss: 2.268482\n",
      "Epoch 53, loss: 2.273909\n",
      "Epoch 54, loss: 2.265570\n",
      "Epoch 55, loss: 2.269762\n",
      "Epoch 56, loss: 2.278669\n",
      "Epoch 57, loss: 2.263091\n",
      "Epoch 58, loss: 2.269804\n",
      "Epoch 59, loss: 2.272652\n",
      "Epoch 60, loss: 2.264911\n",
      "Epoch 61, loss: 2.274801\n",
      "Epoch 62, loss: 2.266227\n",
      "Epoch 63, loss: 2.275000\n",
      "Epoch 64, loss: 2.267400\n",
      "Epoch 65, loss: 2.266555\n",
      "Epoch 66, loss: 2.276529\n",
      "Epoch 67, loss: 2.262143\n",
      "Epoch 68, loss: 2.275975\n",
      "Epoch 69, loss: 2.269604\n",
      "Epoch 70, loss: 2.268415\n",
      "Epoch 71, loss: 2.267145\n",
      "Epoch 72, loss: 2.278167\n",
      "Epoch 73, loss: 2.257459\n",
      "Epoch 74, loss: 2.281229\n",
      "Epoch 75, loss: 2.265757\n",
      "Epoch 76, loss: 2.258072\n",
      "Epoch 77, loss: 2.271310\n",
      "Epoch 78, loss: 2.267442\n",
      "Epoch 79, loss: 2.259274\n",
      "Epoch 80, loss: 2.262315\n",
      "Epoch 81, loss: 2.260676\n",
      "Epoch 82, loss: 2.269962\n",
      "Epoch 83, loss: 2.272681\n",
      "Epoch 84, loss: 2.254467\n",
      "Epoch 85, loss: 2.272846\n",
      "Epoch 86, loss: 2.264370\n",
      "Epoch 87, loss: 2.268376\n",
      "Epoch 88, loss: 2.261172\n",
      "Epoch 89, loss: 2.256989\n",
      "Epoch 90, loss: 2.267228\n",
      "Epoch 91, loss: 2.274251\n",
      "Epoch 92, loss: 2.260231\n",
      "Epoch 93, loss: 2.274846\n",
      "Epoch 94, loss: 2.251192\n",
      "Epoch 95, loss: 2.252538\n",
      "Epoch 96, loss: 2.261393\n",
      "Epoch 97, loss: 2.265724\n",
      "Epoch 98, loss: 2.273198\n",
      "Epoch 99, loss: 2.263500\n",
      "Epoch 100, loss: 2.242278\n",
      "Epoch 101, loss: 2.271789\n",
      "Epoch 102, loss: 2.257319\n",
      "Epoch 103, loss: 2.267729\n",
      "Epoch 104, loss: 2.258935\n",
      "Epoch 105, loss: 2.276577\n",
      "Epoch 106, loss: 2.257703\n",
      "Epoch 107, loss: 2.260395\n",
      "Epoch 108, loss: 2.266633\n",
      "Epoch 109, loss: 2.256706\n",
      "Epoch 110, loss: 2.264534\n",
      "Epoch 111, loss: 2.272596\n",
      "Epoch 112, loss: 2.245445\n",
      "Epoch 113, loss: 2.261272\n",
      "Epoch 114, loss: 2.261391\n",
      "Epoch 115, loss: 2.259004\n",
      "Epoch 116, loss: 2.265116\n",
      "Epoch 117, loss: 2.264186\n",
      "Epoch 118, loss: 2.256761\n",
      "Epoch 119, loss: 2.260196\n",
      "Epoch 120, loss: 2.265951\n",
      "Epoch 121, loss: 2.256837\n",
      "Epoch 122, loss: 2.264679\n",
      "Epoch 123, loss: 2.268219\n",
      "Epoch 124, loss: 2.258737\n",
      "Epoch 125, loss: 2.257978\n",
      "Epoch 126, loss: 2.257378\n",
      "Epoch 127, loss: 2.254451\n",
      "Epoch 128, loss: 2.246981\n",
      "Epoch 129, loss: 2.273096\n",
      "Epoch 130, loss: 2.253685\n",
      "Epoch 131, loss: 2.266373\n",
      "Epoch 132, loss: 2.259385\n",
      "Epoch 133, loss: 2.266546\n",
      "Epoch 134, loss: 2.242679\n",
      "Epoch 135, loss: 2.269448\n",
      "Epoch 136, loss: 2.263505\n",
      "Epoch 137, loss: 2.251089\n",
      "Epoch 138, loss: 2.250768\n",
      "Epoch 139, loss: 2.265779\n",
      "Epoch 140, loss: 2.242984\n",
      "Epoch 141, loss: 2.251529\n",
      "Epoch 142, loss: 2.261227\n",
      "Epoch 143, loss: 2.259456\n",
      "Epoch 144, loss: 2.244495\n",
      "Epoch 145, loss: 2.270903\n",
      "Epoch 146, loss: 2.263017\n",
      "Epoch 147, loss: 2.248589\n",
      "Epoch 148, loss: 2.258161\n",
      "Epoch 149, loss: 2.274854\n",
      "Epoch 150, loss: 2.257622\n",
      "Epoch 151, loss: 2.262500\n",
      "Epoch 152, loss: 2.269660\n",
      "Epoch 153, loss: 2.259428\n",
      "Epoch 154, loss: 2.249158\n",
      "Epoch 155, loss: 2.260458\n",
      "Epoch 156, loss: 2.261873\n",
      "Epoch 157, loss: 2.275561\n",
      "Epoch 158, loss: 2.250635\n",
      "Epoch 159, loss: 2.284017\n",
      "Epoch 160, loss: 2.237881\n",
      "Epoch 161, loss: 2.255738\n",
      "Epoch 162, loss: 2.253040\n",
      "Epoch 163, loss: 2.259161\n",
      "Epoch 164, loss: 2.274754\n",
      "Epoch 165, loss: 2.244696\n",
      "Epoch 166, loss: 2.249278\n",
      "Epoch 167, loss: 2.257602\n",
      "Epoch 168, loss: 2.259105\n",
      "Epoch 169, loss: 2.263047\n",
      "Epoch 170, loss: 2.267441\n",
      "Epoch 171, loss: 2.247302\n",
      "Epoch 172, loss: 2.247942\n",
      "Epoch 173, loss: 2.266356\n",
      "Epoch 174, loss: 2.262101\n",
      "Epoch 175, loss: 2.239782\n",
      "Epoch 176, loss: 2.248277\n",
      "Epoch 177, loss: 2.244156\n",
      "Epoch 178, loss: 2.249735\n",
      "Epoch 179, loss: 2.247261\n",
      "Epoch 180, loss: 2.257397\n",
      "Epoch 181, loss: 2.248316\n",
      "Epoch 182, loss: 2.244591\n",
      "Epoch 183, loss: 2.267872\n",
      "Epoch 184, loss: 2.254003\n",
      "Epoch 185, loss: 2.273819\n",
      "Epoch 186, loss: 2.247631\n",
      "Epoch 187, loss: 2.254018\n",
      "Epoch 188, loss: 2.250234\n",
      "Epoch 189, loss: 2.226436\n",
      "Epoch 190, loss: 2.263458\n",
      "Epoch 191, loss: 2.265496\n",
      "Epoch 192, loss: 2.247101\n",
      "Epoch 193, loss: 2.258569\n",
      "Epoch 194, loss: 2.254705\n",
      "Epoch 195, loss: 2.236087\n",
      "Epoch 196, loss: 2.265637\n",
      "Epoch 197, loss: 2.269611\n",
      "Epoch 198, loss: 2.262128\n",
      "Epoch 199, loss: 2.268401\n",
      "learning rate = 0.001 reg = 0.1 accuracy = 0.224\n",
      "Epoch 0, loss: 2.303343\n",
      "Epoch 1, loss: 2.301789\n",
      "Epoch 2, loss: 2.299288\n",
      "Epoch 3, loss: 2.298385\n",
      "Epoch 4, loss: 2.299421\n",
      "Epoch 5, loss: 2.296082\n",
      "Epoch 6, loss: 2.300193\n",
      "Epoch 7, loss: 2.299029\n",
      "Epoch 8, loss: 2.293046\n",
      "Epoch 9, loss: 2.293734\n",
      "Epoch 10, loss: 2.291178\n",
      "Epoch 11, loss: 2.292710\n",
      "Epoch 12, loss: 2.293009\n",
      "Epoch 13, loss: 2.291955\n",
      "Epoch 14, loss: 2.288201\n",
      "Epoch 15, loss: 2.287188\n",
      "Epoch 16, loss: 2.290010\n",
      "Epoch 17, loss: 2.290168\n",
      "Epoch 18, loss: 2.285354\n",
      "Epoch 19, loss: 2.282195\n",
      "Epoch 20, loss: 2.283849\n",
      "Epoch 21, loss: 2.288822\n",
      "Epoch 22, loss: 2.281779\n",
      "Epoch 23, loss: 2.287414\n",
      "Epoch 24, loss: 2.282489\n",
      "Epoch 25, loss: 2.283909\n",
      "Epoch 26, loss: 2.283640\n",
      "Epoch 27, loss: 2.284386\n",
      "Epoch 28, loss: 2.276454\n",
      "Epoch 29, loss: 2.275501\n",
      "Epoch 30, loss: 2.282545\n",
      "Epoch 31, loss: 2.270951\n",
      "Epoch 32, loss: 2.274015\n",
      "Epoch 33, loss: 2.279313\n",
      "Epoch 34, loss: 2.274477\n",
      "Epoch 35, loss: 2.276861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, loss: 2.267720\n",
      "Epoch 37, loss: 2.282214\n",
      "Epoch 38, loss: 2.272053\n",
      "Epoch 39, loss: 2.276218\n",
      "Epoch 40, loss: 2.274730\n",
      "Epoch 41, loss: 2.273892\n",
      "Epoch 42, loss: 2.269666\n",
      "Epoch 43, loss: 2.266071\n",
      "Epoch 44, loss: 2.276899\n",
      "Epoch 45, loss: 2.275490\n",
      "Epoch 46, loss: 2.265626\n",
      "Epoch 47, loss: 2.266612\n",
      "Epoch 48, loss: 2.270767\n",
      "Epoch 49, loss: 2.266934\n",
      "Epoch 50, loss: 2.270927\n",
      "Epoch 51, loss: 2.273412\n",
      "Epoch 52, loss: 2.267160\n",
      "Epoch 53, loss: 2.264306\n",
      "Epoch 54, loss: 2.263596\n",
      "Epoch 55, loss: 2.272520\n",
      "Epoch 56, loss: 2.253044\n",
      "Epoch 57, loss: 2.261377\n",
      "Epoch 58, loss: 2.248891\n",
      "Epoch 59, loss: 2.265077\n",
      "Epoch 60, loss: 2.266875\n",
      "Epoch 61, loss: 2.251980\n",
      "Epoch 62, loss: 2.263014\n",
      "Epoch 63, loss: 2.255964\n",
      "Epoch 64, loss: 2.259818\n",
      "Epoch 65, loss: 2.258286\n",
      "Epoch 66, loss: 2.244265\n",
      "Epoch 67, loss: 2.250879\n",
      "Epoch 68, loss: 2.246656\n",
      "Epoch 69, loss: 2.263656\n",
      "Epoch 70, loss: 2.259756\n",
      "Epoch 71, loss: 2.249586\n",
      "Epoch 72, loss: 2.253019\n",
      "Epoch 73, loss: 2.266103\n",
      "Epoch 74, loss: 2.248413\n",
      "Epoch 75, loss: 2.241400\n",
      "Epoch 76, loss: 2.244671\n",
      "Epoch 77, loss: 2.248264\n",
      "Epoch 78, loss: 2.258323\n",
      "Epoch 79, loss: 2.268702\n",
      "Epoch 80, loss: 2.243801\n",
      "Epoch 81, loss: 2.253101\n",
      "Epoch 82, loss: 2.263489\n",
      "Epoch 83, loss: 2.248713\n",
      "Epoch 84, loss: 2.242103\n",
      "Epoch 85, loss: 2.249442\n",
      "Epoch 86, loss: 2.245756\n",
      "Epoch 87, loss: 2.236525\n",
      "Epoch 88, loss: 2.249163\n",
      "Epoch 89, loss: 2.249665\n",
      "Epoch 90, loss: 2.238524\n",
      "Epoch 91, loss: 2.244718\n",
      "Epoch 92, loss: 2.226838\n",
      "Epoch 93, loss: 2.244190\n",
      "Epoch 94, loss: 2.256065\n",
      "Epoch 95, loss: 2.269534\n",
      "Epoch 96, loss: 2.239506\n",
      "Epoch 97, loss: 2.236712\n",
      "Epoch 98, loss: 2.238643\n",
      "Epoch 99, loss: 2.241299\n",
      "Epoch 100, loss: 2.243148\n",
      "Epoch 101, loss: 2.214713\n",
      "Epoch 102, loss: 2.245907\n",
      "Epoch 103, loss: 2.250123\n",
      "Epoch 104, loss: 2.226875\n",
      "Epoch 105, loss: 2.234766\n",
      "Epoch 106, loss: 2.220848\n",
      "Epoch 107, loss: 2.251191\n",
      "Epoch 108, loss: 2.223757\n",
      "Epoch 109, loss: 2.256994\n",
      "Epoch 110, loss: 2.236375\n",
      "Epoch 111, loss: 2.248736\n",
      "Epoch 112, loss: 2.233701\n",
      "Epoch 113, loss: 2.235225\n",
      "Epoch 114, loss: 2.229630\n",
      "Epoch 115, loss: 2.220251\n",
      "Epoch 116, loss: 2.248020\n",
      "Epoch 117, loss: 2.243388\n",
      "Epoch 118, loss: 2.222059\n",
      "Epoch 119, loss: 2.223333\n",
      "Epoch 120, loss: 2.245084\n",
      "Epoch 121, loss: 2.230284\n",
      "Epoch 122, loss: 2.222660\n",
      "Epoch 123, loss: 2.227236\n",
      "Epoch 124, loss: 2.247248\n",
      "Epoch 125, loss: 2.243700\n",
      "Epoch 126, loss: 2.220955\n",
      "Epoch 127, loss: 2.227090\n",
      "Epoch 128, loss: 2.227648\n",
      "Epoch 129, loss: 2.250988\n",
      "Epoch 130, loss: 2.233735\n",
      "Epoch 131, loss: 2.230493\n",
      "Epoch 132, loss: 2.241477\n",
      "Epoch 133, loss: 2.236169\n",
      "Epoch 134, loss: 2.204298\n",
      "Epoch 135, loss: 2.258671\n",
      "Epoch 136, loss: 2.203914\n",
      "Epoch 137, loss: 2.234742\n",
      "Epoch 138, loss: 2.232095\n",
      "Epoch 139, loss: 2.223698\n",
      "Epoch 140, loss: 2.220814\n",
      "Epoch 141, loss: 2.226153\n",
      "Epoch 142, loss: 2.219118\n",
      "Epoch 143, loss: 2.211956\n",
      "Epoch 144, loss: 2.206288\n",
      "Epoch 145, loss: 2.199004\n",
      "Epoch 146, loss: 2.226651\n",
      "Epoch 147, loss: 2.242591\n",
      "Epoch 148, loss: 2.227856\n",
      "Epoch 149, loss: 2.215743\n",
      "Epoch 150, loss: 2.235477\n",
      "Epoch 151, loss: 2.220964\n",
      "Epoch 152, loss: 2.237532\n",
      "Epoch 153, loss: 2.216040\n",
      "Epoch 154, loss: 2.237783\n",
      "Epoch 155, loss: 2.236214\n",
      "Epoch 156, loss: 2.212397\n",
      "Epoch 157, loss: 2.206389\n",
      "Epoch 158, loss: 2.231966\n",
      "Epoch 159, loss: 2.216823\n",
      "Epoch 160, loss: 2.213608\n",
      "Epoch 161, loss: 2.220350\n",
      "Epoch 162, loss: 2.228281\n",
      "Epoch 163, loss: 2.219318\n",
      "Epoch 164, loss: 2.210458\n",
      "Epoch 165, loss: 2.240952\n",
      "Epoch 166, loss: 2.231244\n",
      "Epoch 167, loss: 2.224137\n",
      "Epoch 168, loss: 2.224363\n",
      "Epoch 169, loss: 2.183297\n",
      "Epoch 170, loss: 2.197071\n",
      "Epoch 171, loss: 2.210589\n",
      "Epoch 172, loss: 2.214837\n",
      "Epoch 173, loss: 2.208836\n",
      "Epoch 174, loss: 2.204463\n",
      "Epoch 175, loss: 2.208870\n",
      "Epoch 176, loss: 2.222238\n",
      "Epoch 177, loss: 2.228287\n",
      "Epoch 178, loss: 2.204198\n",
      "Epoch 179, loss: 2.241430\n",
      "Epoch 180, loss: 2.215390\n",
      "Epoch 181, loss: 2.222371\n",
      "Epoch 182, loss: 2.220710\n",
      "Epoch 183, loss: 2.249055\n",
      "Epoch 184, loss: 2.201320\n",
      "Epoch 185, loss: 2.220423\n",
      "Epoch 186, loss: 2.206237\n",
      "Epoch 187, loss: 2.221880\n",
      "Epoch 188, loss: 2.231226\n",
      "Epoch 189, loss: 2.202327\n",
      "Epoch 190, loss: 2.216555\n",
      "Epoch 191, loss: 2.230327\n",
      "Epoch 192, loss: 2.211386\n",
      "Epoch 193, loss: 2.213997\n",
      "Epoch 194, loss: 2.200723\n",
      "Epoch 195, loss: 2.227195\n",
      "Epoch 196, loss: 2.191408\n",
      "Epoch 197, loss: 2.242167\n",
      "Epoch 198, loss: 2.197379\n",
      "Epoch 199, loss: 2.219128\n",
      "learning rate = 0.001 reg = 0.01 accuracy = 0.228\n",
      "Epoch 0, loss: 2.302971\n",
      "Epoch 1, loss: 2.300827\n",
      "Epoch 2, loss: 2.300429\n",
      "Epoch 3, loss: 2.300656\n",
      "Epoch 4, loss: 2.296574\n",
      "Epoch 5, loss: 2.297264\n",
      "Epoch 6, loss: 2.297893\n",
      "Epoch 7, loss: 2.296368\n",
      "Epoch 8, loss: 2.294850\n",
      "Epoch 9, loss: 2.293831\n",
      "Epoch 10, loss: 2.292618\n",
      "Epoch 11, loss: 2.293059\n",
      "Epoch 12, loss: 2.287885\n",
      "Epoch 13, loss: 2.290451\n",
      "Epoch 14, loss: 2.288063\n",
      "Epoch 15, loss: 2.283389\n",
      "Epoch 16, loss: 2.286850\n",
      "Epoch 17, loss: 2.290738\n",
      "Epoch 18, loss: 2.287417\n",
      "Epoch 19, loss: 2.282646\n",
      "Epoch 20, loss: 2.286131\n",
      "Epoch 21, loss: 2.286004\n",
      "Epoch 22, loss: 2.280676\n",
      "Epoch 23, loss: 2.279556\n",
      "Epoch 24, loss: 2.274922\n",
      "Epoch 25, loss: 2.284444\n",
      "Epoch 26, loss: 2.281932\n",
      "Epoch 27, loss: 2.283179\n",
      "Epoch 28, loss: 2.278218\n",
      "Epoch 29, loss: 2.273420\n",
      "Epoch 30, loss: 2.279980\n",
      "Epoch 31, loss: 2.277421\n",
      "Epoch 32, loss: 2.275937\n",
      "Epoch 33, loss: 2.280455\n",
      "Epoch 34, loss: 2.272686\n",
      "Epoch 35, loss: 2.268405\n",
      "Epoch 36, loss: 2.271848\n",
      "Epoch 37, loss: 2.276059\n",
      "Epoch 38, loss: 2.265309\n",
      "Epoch 39, loss: 2.264577\n",
      "Epoch 40, loss: 2.263359\n",
      "Epoch 41, loss: 2.266221\n",
      "Epoch 42, loss: 2.259342\n",
      "Epoch 43, loss: 2.273268\n",
      "Epoch 44, loss: 2.273702\n",
      "Epoch 45, loss: 2.263719\n",
      "Epoch 46, loss: 2.276575\n",
      "Epoch 47, loss: 2.264526\n",
      "Epoch 48, loss: 2.267443\n",
      "Epoch 49, loss: 2.268838\n",
      "Epoch 50, loss: 2.257433\n",
      "Epoch 51, loss: 2.273979\n",
      "Epoch 52, loss: 2.260579\n",
      "Epoch 53, loss: 2.262489\n",
      "Epoch 54, loss: 2.264250\n",
      "Epoch 55, loss: 2.268594\n",
      "Epoch 56, loss: 2.254483\n",
      "Epoch 57, loss: 2.260267\n",
      "Epoch 58, loss: 2.250343\n",
      "Epoch 59, loss: 2.258581\n",
      "Epoch 60, loss: 2.256451\n",
      "Epoch 61, loss: 2.263134\n",
      "Epoch 62, loss: 2.250682\n",
      "Epoch 63, loss: 2.259804\n",
      "Epoch 64, loss: 2.259805\n",
      "Epoch 65, loss: 2.251870\n",
      "Epoch 66, loss: 2.257736\n",
      "Epoch 67, loss: 2.251209\n",
      "Epoch 68, loss: 2.253365\n",
      "Epoch 69, loss: 2.243510\n",
      "Epoch 70, loss: 2.268161\n",
      "Epoch 71, loss: 2.258789\n",
      "Epoch 72, loss: 2.251111\n",
      "Epoch 73, loss: 2.256977\n",
      "Epoch 74, loss: 2.241421\n",
      "Epoch 75, loss: 2.247342\n",
      "Epoch 76, loss: 2.257099\n",
      "Epoch 77, loss: 2.255326\n",
      "Epoch 78, loss: 2.239127\n",
      "Epoch 79, loss: 2.240031\n",
      "Epoch 80, loss: 2.243741\n",
      "Epoch 81, loss: 2.243697\n",
      "Epoch 82, loss: 2.254810\n",
      "Epoch 83, loss: 2.246446\n",
      "Epoch 84, loss: 2.252898\n",
      "Epoch 85, loss: 2.237635\n",
      "Epoch 86, loss: 2.243074\n",
      "Epoch 87, loss: 2.250451\n",
      "Epoch 88, loss: 2.241576\n",
      "Epoch 89, loss: 2.252834\n",
      "Epoch 90, loss: 2.218958\n",
      "Epoch 91, loss: 2.242462\n",
      "Epoch 92, loss: 2.237024\n",
      "Epoch 93, loss: 2.237838\n",
      "Epoch 94, loss: 2.226725\n",
      "Epoch 95, loss: 2.246196\n",
      "Epoch 96, loss: 2.245190\n",
      "Epoch 97, loss: 2.245434\n",
      "Epoch 98, loss: 2.235365\n",
      "Epoch 99, loss: 2.250096\n",
      "Epoch 100, loss: 2.234621\n",
      "Epoch 101, loss: 2.246974\n",
      "Epoch 102, loss: 2.242493\n",
      "Epoch 103, loss: 2.229961\n",
      "Epoch 104, loss: 2.242485\n",
      "Epoch 105, loss: 2.234868\n",
      "Epoch 106, loss: 2.250099\n",
      "Epoch 107, loss: 2.233872\n",
      "Epoch 108, loss: 2.243780\n",
      "Epoch 109, loss: 2.223694\n",
      "Epoch 110, loss: 2.228487\n",
      "Epoch 111, loss: 2.242862\n",
      "Epoch 112, loss: 2.229612\n",
      "Epoch 113, loss: 2.230393\n",
      "Epoch 114, loss: 2.256487\n",
      "Epoch 115, loss: 2.239018\n",
      "Epoch 116, loss: 2.216608\n",
      "Epoch 117, loss: 2.234692\n",
      "Epoch 118, loss: 2.229553\n",
      "Epoch 119, loss: 2.238314\n",
      "Epoch 120, loss: 2.240279\n",
      "Epoch 121, loss: 2.241116\n",
      "Epoch 122, loss: 2.228588\n",
      "Epoch 123, loss: 2.222104\n",
      "Epoch 124, loss: 2.229220\n",
      "Epoch 125, loss: 2.242933\n",
      "Epoch 126, loss: 2.207542\n",
      "Epoch 127, loss: 2.222320\n",
      "Epoch 128, loss: 2.226783\n",
      "Epoch 129, loss: 2.205866\n",
      "Epoch 130, loss: 2.229804\n",
      "Epoch 131, loss: 2.239401\n",
      "Epoch 132, loss: 2.229809\n",
      "Epoch 133, loss: 2.240228\n",
      "Epoch 134, loss: 2.201395\n",
      "Epoch 135, loss: 2.223122\n",
      "Epoch 136, loss: 2.197349\n",
      "Epoch 137, loss: 2.224973\n",
      "Epoch 138, loss: 2.239222\n",
      "Epoch 139, loss: 2.220806\n",
      "Epoch 140, loss: 2.237450\n",
      "Epoch 141, loss: 2.224813\n",
      "Epoch 142, loss: 2.244027\n",
      "Epoch 143, loss: 2.242548\n",
      "Epoch 144, loss: 2.240353\n",
      "Epoch 145, loss: 2.218616\n",
      "Epoch 146, loss: 2.231302\n",
      "Epoch 147, loss: 2.214394\n",
      "Epoch 148, loss: 2.223714\n",
      "Epoch 149, loss: 2.222591\n",
      "Epoch 150, loss: 2.193078\n",
      "Epoch 151, loss: 2.223609\n",
      "Epoch 152, loss: 2.245722\n",
      "Epoch 153, loss: 2.191663\n",
      "Epoch 154, loss: 2.206303\n",
      "Epoch 155, loss: 2.194678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156, loss: 2.190489\n",
      "Epoch 157, loss: 2.228730\n",
      "Epoch 158, loss: 2.211712\n",
      "Epoch 159, loss: 2.239295\n",
      "Epoch 160, loss: 2.212553\n",
      "Epoch 161, loss: 2.235127\n",
      "Epoch 162, loss: 2.202754\n",
      "Epoch 163, loss: 2.214029\n",
      "Epoch 164, loss: 2.223823\n",
      "Epoch 165, loss: 2.222356\n",
      "Epoch 166, loss: 2.236768\n",
      "Epoch 167, loss: 2.191636\n",
      "Epoch 168, loss: 2.200793\n",
      "Epoch 169, loss: 2.199239\n",
      "Epoch 170, loss: 2.227164\n",
      "Epoch 171, loss: 2.176968\n",
      "Epoch 172, loss: 2.219747\n",
      "Epoch 173, loss: 2.202123\n",
      "Epoch 174, loss: 2.210134\n",
      "Epoch 175, loss: 2.187161\n",
      "Epoch 176, loss: 2.208057\n",
      "Epoch 177, loss: 2.207163\n",
      "Epoch 178, loss: 2.216131\n",
      "Epoch 179, loss: 2.222805\n",
      "Epoch 180, loss: 2.192647\n",
      "Epoch 181, loss: 2.205628\n",
      "Epoch 182, loss: 2.208592\n",
      "Epoch 183, loss: 2.217663\n",
      "Epoch 184, loss: 2.206671\n",
      "Epoch 185, loss: 2.219629\n",
      "Epoch 186, loss: 2.232404\n",
      "Epoch 187, loss: 2.173267\n",
      "Epoch 188, loss: 2.203049\n",
      "Epoch 189, loss: 2.222524\n",
      "Epoch 190, loss: 2.201713\n",
      "Epoch 191, loss: 2.201022\n",
      "Epoch 192, loss: 2.208576\n",
      "Epoch 193, loss: 2.215364\n",
      "Epoch 194, loss: 2.178269\n",
      "Epoch 195, loss: 2.189794\n",
      "Epoch 196, loss: 2.167738\n",
      "Epoch 197, loss: 2.182017\n",
      "Epoch 198, loss: 2.193583\n",
      "Epoch 199, loss: 2.167369\n",
      "learning rate = 0.001 reg = 0.001 accuracy = 0.227\n",
      "Epoch 0, loss: 2.302736\n",
      "Epoch 1, loss: 2.299893\n",
      "Epoch 2, loss: 2.300290\n",
      "Epoch 3, loss: 2.299579\n",
      "Epoch 4, loss: 2.298350\n",
      "Epoch 5, loss: 2.301222\n",
      "Epoch 6, loss: 2.297277\n",
      "Epoch 7, loss: 2.296926\n",
      "Epoch 8, loss: 2.294147\n",
      "Epoch 9, loss: 2.292730\n",
      "Epoch 10, loss: 2.294813\n",
      "Epoch 11, loss: 2.290878\n",
      "Epoch 12, loss: 2.292913\n",
      "Epoch 13, loss: 2.290424\n",
      "Epoch 14, loss: 2.291894\n",
      "Epoch 15, loss: 2.290023\n",
      "Epoch 16, loss: 2.284842\n",
      "Epoch 17, loss: 2.284882\n",
      "Epoch 18, loss: 2.285914\n",
      "Epoch 19, loss: 2.286167\n",
      "Epoch 20, loss: 2.285860\n",
      "Epoch 21, loss: 2.286849\n",
      "Epoch 22, loss: 2.285076\n",
      "Epoch 23, loss: 2.282805\n",
      "Epoch 24, loss: 2.279380\n",
      "Epoch 25, loss: 2.278327\n",
      "Epoch 26, loss: 2.277430\n",
      "Epoch 27, loss: 2.291142\n",
      "Epoch 28, loss: 2.276949\n",
      "Epoch 29, loss: 2.276628\n",
      "Epoch 30, loss: 2.278026\n",
      "Epoch 31, loss: 2.284349\n",
      "Epoch 32, loss: 2.274499\n",
      "Epoch 33, loss: 2.275060\n",
      "Epoch 34, loss: 2.276862\n",
      "Epoch 35, loss: 2.281989\n",
      "Epoch 36, loss: 2.263800\n",
      "Epoch 37, loss: 2.275354\n",
      "Epoch 38, loss: 2.276100\n",
      "Epoch 39, loss: 2.267382\n",
      "Epoch 40, loss: 2.260946\n",
      "Epoch 41, loss: 2.272812\n",
      "Epoch 42, loss: 2.276127\n",
      "Epoch 43, loss: 2.273202\n",
      "Epoch 44, loss: 2.267088\n",
      "Epoch 45, loss: 2.266265\n",
      "Epoch 46, loss: 2.260672\n",
      "Epoch 47, loss: 2.270669\n",
      "Epoch 48, loss: 2.263139\n",
      "Epoch 49, loss: 2.264516\n",
      "Epoch 50, loss: 2.261827\n",
      "Epoch 51, loss: 2.264846\n",
      "Epoch 52, loss: 2.256315\n",
      "Epoch 53, loss: 2.265641\n",
      "Epoch 54, loss: 2.269950\n",
      "Epoch 55, loss: 2.257076\n",
      "Epoch 56, loss: 2.262653\n",
      "Epoch 57, loss: 2.261046\n",
      "Epoch 58, loss: 2.252326\n",
      "Epoch 59, loss: 2.256053\n",
      "Epoch 60, loss: 2.250798\n",
      "Epoch 61, loss: 2.269368\n",
      "Epoch 62, loss: 2.262760\n",
      "Epoch 63, loss: 2.242620\n",
      "Epoch 64, loss: 2.260160\n",
      "Epoch 65, loss: 2.253418\n",
      "Epoch 66, loss: 2.257527\n",
      "Epoch 67, loss: 2.236660\n",
      "Epoch 68, loss: 2.263540\n",
      "Epoch 69, loss: 2.244769\n",
      "Epoch 70, loss: 2.234814\n",
      "Epoch 71, loss: 2.249011\n",
      "Epoch 72, loss: 2.265172\n",
      "Epoch 73, loss: 2.248453\n",
      "Epoch 74, loss: 2.251536\n",
      "Epoch 75, loss: 2.255092\n",
      "Epoch 76, loss: 2.248955\n",
      "Epoch 77, loss: 2.244026\n",
      "Epoch 78, loss: 2.237910\n",
      "Epoch 79, loss: 2.255516\n",
      "Epoch 80, loss: 2.254417\n",
      "Epoch 81, loss: 2.251031\n",
      "Epoch 82, loss: 2.240890\n",
      "Epoch 83, loss: 2.246827\n",
      "Epoch 84, loss: 2.246390\n",
      "Epoch 85, loss: 2.259934\n",
      "Epoch 86, loss: 2.233082\n",
      "Epoch 87, loss: 2.250013\n",
      "Epoch 88, loss: 2.233308\n",
      "Epoch 89, loss: 2.255231\n",
      "Epoch 90, loss: 2.225353\n",
      "Epoch 91, loss: 2.244291\n",
      "Epoch 92, loss: 2.251218\n",
      "Epoch 93, loss: 2.245857\n",
      "Epoch 94, loss: 2.228744\n",
      "Epoch 95, loss: 2.230426\n",
      "Epoch 96, loss: 2.242533\n",
      "Epoch 97, loss: 2.234178\n",
      "Epoch 98, loss: 2.241029\n",
      "Epoch 99, loss: 2.244299\n",
      "Epoch 100, loss: 2.259058\n",
      "Epoch 101, loss: 2.235417\n",
      "Epoch 102, loss: 2.233229\n",
      "Epoch 103, loss: 2.250835\n",
      "Epoch 104, loss: 2.221670\n",
      "Epoch 105, loss: 2.230207\n",
      "Epoch 106, loss: 2.235232\n",
      "Epoch 107, loss: 2.224950\n",
      "Epoch 108, loss: 2.218422\n",
      "Epoch 109, loss: 2.255189\n",
      "Epoch 110, loss: 2.224252\n",
      "Epoch 111, loss: 2.250896\n",
      "Epoch 112, loss: 2.231825\n",
      "Epoch 113, loss: 2.244593\n",
      "Epoch 114, loss: 2.238562\n",
      "Epoch 115, loss: 2.234046\n",
      "Epoch 116, loss: 2.227177\n",
      "Epoch 117, loss: 2.235885\n",
      "Epoch 118, loss: 2.234619\n",
      "Epoch 119, loss: 2.231980\n",
      "Epoch 120, loss: 2.219187\n",
      "Epoch 121, loss: 2.225326\n",
      "Epoch 122, loss: 2.219791\n",
      "Epoch 123, loss: 2.233922\n",
      "Epoch 124, loss: 2.215208\n",
      "Epoch 125, loss: 2.223927\n",
      "Epoch 126, loss: 2.235345\n",
      "Epoch 127, loss: 2.226117\n",
      "Epoch 128, loss: 2.223903\n",
      "Epoch 129, loss: 2.220251\n",
      "Epoch 130, loss: 2.220018\n",
      "Epoch 131, loss: 2.218700\n",
      "Epoch 132, loss: 2.203392\n",
      "Epoch 133, loss: 2.235601\n",
      "Epoch 134, loss: 2.227690\n",
      "Epoch 135, loss: 2.206861\n",
      "Epoch 136, loss: 2.262875\n",
      "Epoch 137, loss: 2.230062\n",
      "Epoch 138, loss: 2.191275\n",
      "Epoch 139, loss: 2.219104\n",
      "Epoch 140, loss: 2.216705\n",
      "Epoch 141, loss: 2.225663\n",
      "Epoch 142, loss: 2.237807\n",
      "Epoch 143, loss: 2.230948\n",
      "Epoch 144, loss: 2.225472\n",
      "Epoch 145, loss: 2.202638\n",
      "Epoch 146, loss: 2.215682\n",
      "Epoch 147, loss: 2.235185\n",
      "Epoch 148, loss: 2.219844\n",
      "Epoch 149, loss: 2.199453\n",
      "Epoch 150, loss: 2.224206\n",
      "Epoch 151, loss: 2.218133\n",
      "Epoch 152, loss: 2.194312\n",
      "Epoch 153, loss: 2.227828\n",
      "Epoch 154, loss: 2.206413\n",
      "Epoch 155, loss: 2.210614\n",
      "Epoch 156, loss: 2.215035\n",
      "Epoch 157, loss: 2.213479\n",
      "Epoch 158, loss: 2.234687\n",
      "Epoch 159, loss: 2.204959\n",
      "Epoch 160, loss: 2.203143\n",
      "Epoch 161, loss: 2.197535\n",
      "Epoch 162, loss: 2.217084\n",
      "Epoch 163, loss: 2.209558\n",
      "Epoch 164, loss: 2.219000\n",
      "Epoch 165, loss: 2.215769\n",
      "Epoch 166, loss: 2.202474\n",
      "Epoch 167, loss: 2.185428\n",
      "Epoch 168, loss: 2.224680\n",
      "Epoch 169, loss: 2.208521\n",
      "Epoch 170, loss: 2.241789\n",
      "Epoch 171, loss: 2.220260\n",
      "Epoch 172, loss: 2.200730\n",
      "Epoch 173, loss: 2.220637\n",
      "Epoch 174, loss: 2.217118\n",
      "Epoch 175, loss: 2.211940\n",
      "Epoch 176, loss: 2.200842\n",
      "Epoch 177, loss: 2.206542\n",
      "Epoch 178, loss: 2.228496\n",
      "Epoch 179, loss: 2.227405\n",
      "Epoch 180, loss: 2.225505\n",
      "Epoch 181, loss: 2.190395\n",
      "Epoch 182, loss: 2.212588\n",
      "Epoch 183, loss: 2.187744\n",
      "Epoch 184, loss: 2.207878\n",
      "Epoch 185, loss: 2.191673\n",
      "Epoch 186, loss: 2.209746\n",
      "Epoch 187, loss: 2.205488\n",
      "Epoch 188, loss: 2.206246\n",
      "Epoch 189, loss: 2.241190\n",
      "Epoch 190, loss: 2.215249\n",
      "Epoch 191, loss: 2.203141\n",
      "Epoch 192, loss: 2.196908\n",
      "Epoch 193, loss: 2.193428\n",
      "Epoch 194, loss: 2.205540\n",
      "Epoch 195, loss: 2.190173\n",
      "Epoch 196, loss: 2.204639\n",
      "Epoch 197, loss: 2.190510\n",
      "Epoch 198, loss: 2.198059\n",
      "Epoch 199, loss: 2.207863\n",
      "learning rate = 0.001 reg = 0.0001 accuracy = 0.226\n",
      "Epoch 0, loss: 2.302059\n",
      "Epoch 1, loss: 2.300982\n",
      "Epoch 2, loss: 2.299643\n",
      "Epoch 3, loss: 2.299980\n",
      "Epoch 4, loss: 2.297152\n",
      "Epoch 5, loss: 2.297037\n",
      "Epoch 6, loss: 2.298243\n",
      "Epoch 7, loss: 2.294787\n",
      "Epoch 8, loss: 2.293840\n",
      "Epoch 9, loss: 2.290304\n",
      "Epoch 10, loss: 2.291472\n",
      "Epoch 11, loss: 2.290478\n",
      "Epoch 12, loss: 2.287305\n",
      "Epoch 13, loss: 2.293729\n",
      "Epoch 14, loss: 2.283954\n",
      "Epoch 15, loss: 2.287090\n",
      "Epoch 16, loss: 2.286195\n",
      "Epoch 17, loss: 2.280342\n",
      "Epoch 18, loss: 2.292691\n",
      "Epoch 19, loss: 2.287306\n",
      "Epoch 20, loss: 2.289281\n",
      "Epoch 21, loss: 2.278723\n",
      "Epoch 22, loss: 2.282389\n",
      "Epoch 23, loss: 2.280119\n",
      "Epoch 24, loss: 2.279515\n",
      "Epoch 25, loss: 2.281294\n",
      "Epoch 26, loss: 2.275861\n",
      "Epoch 27, loss: 2.274148\n",
      "Epoch 28, loss: 2.276364\n",
      "Epoch 29, loss: 2.279837\n",
      "Epoch 30, loss: 2.274490\n",
      "Epoch 31, loss: 2.280320\n",
      "Epoch 32, loss: 2.271689\n",
      "Epoch 33, loss: 2.275982\n",
      "Epoch 34, loss: 2.272731\n",
      "Epoch 35, loss: 2.264185\n",
      "Epoch 36, loss: 2.271194\n",
      "Epoch 37, loss: 2.274492\n",
      "Epoch 38, loss: 2.268444\n",
      "Epoch 39, loss: 2.276336\n",
      "Epoch 40, loss: 2.270263\n",
      "Epoch 41, loss: 2.271101\n",
      "Epoch 42, loss: 2.270975\n",
      "Epoch 43, loss: 2.265959\n",
      "Epoch 44, loss: 2.261700\n",
      "Epoch 45, loss: 2.267753\n",
      "Epoch 46, loss: 2.256715\n",
      "Epoch 47, loss: 2.249522\n",
      "Epoch 48, loss: 2.271548\n",
      "Epoch 49, loss: 2.262832\n",
      "Epoch 50, loss: 2.260123\n",
      "Epoch 51, loss: 2.260168\n",
      "Epoch 52, loss: 2.266620\n",
      "Epoch 53, loss: 2.255207\n",
      "Epoch 54, loss: 2.270137\n",
      "Epoch 55, loss: 2.255736\n",
      "Epoch 56, loss: 2.259153\n",
      "Epoch 57, loss: 2.250688\n",
      "Epoch 58, loss: 2.270613\n",
      "Epoch 59, loss: 2.255309\n",
      "Epoch 60, loss: 2.263814\n",
      "Epoch 61, loss: 2.257953\n",
      "Epoch 62, loss: 2.252787\n",
      "Epoch 63, loss: 2.254822\n",
      "Epoch 64, loss: 2.250575\n",
      "Epoch 65, loss: 2.260177\n",
      "Epoch 66, loss: 2.260119\n",
      "Epoch 67, loss: 2.239945\n",
      "Epoch 68, loss: 2.242740\n",
      "Epoch 69, loss: 2.252065\n",
      "Epoch 70, loss: 2.249419\n",
      "Epoch 71, loss: 2.259936\n",
      "Epoch 72, loss: 2.243275\n",
      "Epoch 73, loss: 2.252836\n",
      "Epoch 74, loss: 2.242056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, loss: 2.238053\n",
      "Epoch 76, loss: 2.243066\n",
      "Epoch 77, loss: 2.260214\n",
      "Epoch 78, loss: 2.254053\n",
      "Epoch 79, loss: 2.255657\n",
      "Epoch 80, loss: 2.250098\n",
      "Epoch 81, loss: 2.253088\n",
      "Epoch 82, loss: 2.240790\n",
      "Epoch 83, loss: 2.250553\n",
      "Epoch 84, loss: 2.245931\n",
      "Epoch 85, loss: 2.247440\n",
      "Epoch 86, loss: 2.230967\n",
      "Epoch 87, loss: 2.248272\n",
      "Epoch 88, loss: 2.246096\n",
      "Epoch 89, loss: 2.263285\n",
      "Epoch 90, loss: 2.248297\n",
      "Epoch 91, loss: 2.223182\n",
      "Epoch 92, loss: 2.250998\n",
      "Epoch 93, loss: 2.240566\n",
      "Epoch 94, loss: 2.229208\n",
      "Epoch 95, loss: 2.235544\n",
      "Epoch 96, loss: 2.240387\n",
      "Epoch 97, loss: 2.229390\n",
      "Epoch 98, loss: 2.235430\n",
      "Epoch 99, loss: 2.229429\n",
      "Epoch 100, loss: 2.252598\n",
      "Epoch 101, loss: 2.234296\n",
      "Epoch 102, loss: 2.248741\n",
      "Epoch 103, loss: 2.251311\n",
      "Epoch 104, loss: 2.230298\n",
      "Epoch 105, loss: 2.236433\n",
      "Epoch 106, loss: 2.239400\n",
      "Epoch 107, loss: 2.249255\n",
      "Epoch 108, loss: 2.228641\n",
      "Epoch 109, loss: 2.228563\n",
      "Epoch 110, loss: 2.233059\n",
      "Epoch 111, loss: 2.235300\n",
      "Epoch 112, loss: 2.228127\n",
      "Epoch 113, loss: 2.215014\n",
      "Epoch 114, loss: 2.236130\n",
      "Epoch 115, loss: 2.238201\n",
      "Epoch 116, loss: 2.218312\n",
      "Epoch 117, loss: 2.236265\n",
      "Epoch 118, loss: 2.222540\n",
      "Epoch 119, loss: 2.227445\n",
      "Epoch 120, loss: 2.228563\n",
      "Epoch 121, loss: 2.246789\n",
      "Epoch 122, loss: 2.219510\n",
      "Epoch 123, loss: 2.234553\n",
      "Epoch 124, loss: 2.224646\n",
      "Epoch 125, loss: 2.226389\n",
      "Epoch 126, loss: 2.239942\n",
      "Epoch 127, loss: 2.217221\n",
      "Epoch 128, loss: 2.218859\n",
      "Epoch 129, loss: 2.237484\n",
      "Epoch 130, loss: 2.219366\n",
      "Epoch 131, loss: 2.217341\n",
      "Epoch 132, loss: 2.216289\n",
      "Epoch 133, loss: 2.213407\n",
      "Epoch 134, loss: 2.230069\n",
      "Epoch 135, loss: 2.202503\n",
      "Epoch 136, loss: 2.229889\n",
      "Epoch 137, loss: 2.212219\n",
      "Epoch 138, loss: 2.218216\n",
      "Epoch 139, loss: 2.201915\n",
      "Epoch 140, loss: 2.214661\n",
      "Epoch 141, loss: 2.222248\n",
      "Epoch 142, loss: 2.222115\n",
      "Epoch 143, loss: 2.197673\n",
      "Epoch 144, loss: 2.199526\n",
      "Epoch 145, loss: 2.222420\n",
      "Epoch 146, loss: 2.227357\n",
      "Epoch 147, loss: 2.248137\n",
      "Epoch 148, loss: 2.227462\n",
      "Epoch 149, loss: 2.220135\n",
      "Epoch 150, loss: 2.239316\n",
      "Epoch 151, loss: 2.196500\n",
      "Epoch 152, loss: 2.236327\n",
      "Epoch 153, loss: 2.200931\n",
      "Epoch 154, loss: 2.254016\n",
      "Epoch 155, loss: 2.208856\n",
      "Epoch 156, loss: 2.245850\n",
      "Epoch 157, loss: 2.212372\n",
      "Epoch 158, loss: 2.217928\n",
      "Epoch 159, loss: 2.214588\n",
      "Epoch 160, loss: 2.198555\n",
      "Epoch 161, loss: 2.202670\n",
      "Epoch 162, loss: 2.197228\n",
      "Epoch 163, loss: 2.197630\n",
      "Epoch 164, loss: 2.228029\n",
      "Epoch 165, loss: 2.197084\n",
      "Epoch 166, loss: 2.203664\n",
      "Epoch 167, loss: 2.230237\n",
      "Epoch 168, loss: 2.237852\n",
      "Epoch 169, loss: 2.198957\n",
      "Epoch 170, loss: 2.224771\n",
      "Epoch 171, loss: 2.217295\n",
      "Epoch 172, loss: 2.221208\n",
      "Epoch 173, loss: 2.218457\n",
      "Epoch 174, loss: 2.205835\n",
      "Epoch 175, loss: 2.229351\n",
      "Epoch 176, loss: 2.185546\n",
      "Epoch 177, loss: 2.198522\n",
      "Epoch 178, loss: 2.218284\n",
      "Epoch 179, loss: 2.218763\n",
      "Epoch 180, loss: 2.205092\n",
      "Epoch 181, loss: 2.213805\n",
      "Epoch 182, loss: 2.228920\n",
      "Epoch 183, loss: 2.184027\n",
      "Epoch 184, loss: 2.189117\n",
      "Epoch 185, loss: 2.198832\n",
      "Epoch 186, loss: 2.204913\n",
      "Epoch 187, loss: 2.220146\n",
      "Epoch 188, loss: 2.210178\n",
      "Epoch 189, loss: 2.194035\n",
      "Epoch 190, loss: 2.178585\n",
      "Epoch 191, loss: 2.232894\n",
      "Epoch 192, loss: 2.172344\n",
      "Epoch 193, loss: 2.221102\n",
      "Epoch 194, loss: 2.207094\n",
      "Epoch 195, loss: 2.180762\n",
      "Epoch 196, loss: 2.192282\n",
      "Epoch 197, loss: 2.190712\n",
      "Epoch 198, loss: 2.192508\n",
      "Epoch 199, loss: 2.208280\n",
      "learning rate = 0.001 reg = 1e-05 accuracy = 0.228\n",
      "Epoch 0, loss: 2.301488\n",
      "Epoch 1, loss: 2.301651\n",
      "Epoch 2, loss: 2.300559\n",
      "Epoch 3, loss: 2.299107\n",
      "Epoch 4, loss: 2.297563\n",
      "Epoch 5, loss: 2.294902\n",
      "Epoch 6, loss: 2.297542\n",
      "Epoch 7, loss: 2.295832\n",
      "Epoch 8, loss: 2.297050\n",
      "Epoch 9, loss: 2.291373\n",
      "Epoch 10, loss: 2.291415\n",
      "Epoch 11, loss: 2.291832\n",
      "Epoch 12, loss: 2.288763\n",
      "Epoch 13, loss: 2.288021\n",
      "Epoch 14, loss: 2.288149\n",
      "Epoch 15, loss: 2.285734\n",
      "Epoch 16, loss: 2.286878\n",
      "Epoch 17, loss: 2.284578\n",
      "Epoch 18, loss: 2.286306\n",
      "Epoch 19, loss: 2.283085\n",
      "Epoch 20, loss: 2.284692\n",
      "Epoch 21, loss: 2.285041\n",
      "Epoch 22, loss: 2.287157\n",
      "Epoch 23, loss: 2.282609\n",
      "Epoch 24, loss: 2.283437\n",
      "Epoch 25, loss: 2.280579\n",
      "Epoch 26, loss: 2.274157\n",
      "Epoch 27, loss: 2.279813\n",
      "Epoch 28, loss: 2.275891\n",
      "Epoch 29, loss: 2.276266\n",
      "Epoch 30, loss: 2.275053\n",
      "Epoch 31, loss: 2.274312\n",
      "Epoch 32, loss: 2.274049\n",
      "Epoch 33, loss: 2.281334\n",
      "Epoch 34, loss: 2.269900\n",
      "Epoch 35, loss: 2.271384\n",
      "Epoch 36, loss: 2.280820\n",
      "Epoch 37, loss: 2.269646\n",
      "Epoch 38, loss: 2.274477\n",
      "Epoch 39, loss: 2.274455\n",
      "Epoch 40, loss: 2.272228\n",
      "Epoch 41, loss: 2.259798\n",
      "Epoch 42, loss: 2.271040\n",
      "Epoch 43, loss: 2.276484\n",
      "Epoch 44, loss: 2.263393\n",
      "Epoch 45, loss: 2.265348\n",
      "Epoch 46, loss: 2.264438\n",
      "Epoch 47, loss: 2.265533\n",
      "Epoch 48, loss: 2.274674\n",
      "Epoch 49, loss: 2.264117\n",
      "Epoch 50, loss: 2.270803\n",
      "Epoch 51, loss: 2.260019\n",
      "Epoch 52, loss: 2.258469\n",
      "Epoch 53, loss: 2.262287\n",
      "Epoch 54, loss: 2.259583\n",
      "Epoch 55, loss: 2.251260\n",
      "Epoch 56, loss: 2.255599\n",
      "Epoch 57, loss: 2.265878\n",
      "Epoch 58, loss: 2.267627\n",
      "Epoch 59, loss: 2.263562\n",
      "Epoch 60, loss: 2.253610\n",
      "Epoch 61, loss: 2.253238\n",
      "Epoch 62, loss: 2.256333\n",
      "Epoch 63, loss: 2.264280\n",
      "Epoch 64, loss: 2.267628\n",
      "Epoch 65, loss: 2.244481\n",
      "Epoch 66, loss: 2.257579\n",
      "Epoch 67, loss: 2.249880\n",
      "Epoch 68, loss: 2.253337\n",
      "Epoch 69, loss: 2.257653\n",
      "Epoch 70, loss: 2.277772\n",
      "Epoch 71, loss: 2.233477\n",
      "Epoch 72, loss: 2.248939\n",
      "Epoch 73, loss: 2.250362\n",
      "Epoch 74, loss: 2.241003\n",
      "Epoch 75, loss: 2.255875\n",
      "Epoch 76, loss: 2.249125\n",
      "Epoch 77, loss: 2.254056\n",
      "Epoch 78, loss: 2.257822\n",
      "Epoch 79, loss: 2.247988\n",
      "Epoch 80, loss: 2.256960\n",
      "Epoch 81, loss: 2.237711\n",
      "Epoch 82, loss: 2.242062\n",
      "Epoch 83, loss: 2.247051\n",
      "Epoch 84, loss: 2.241453\n",
      "Epoch 85, loss: 2.248882\n",
      "Epoch 86, loss: 2.232909\n",
      "Epoch 87, loss: 2.246462\n",
      "Epoch 88, loss: 2.239276\n",
      "Epoch 89, loss: 2.242000\n",
      "Epoch 90, loss: 2.248511\n",
      "Epoch 91, loss: 2.241645\n",
      "Epoch 92, loss: 2.257898\n",
      "Epoch 93, loss: 2.235865\n",
      "Epoch 94, loss: 2.244088\n",
      "Epoch 95, loss: 2.240053\n",
      "Epoch 96, loss: 2.238081\n",
      "Epoch 97, loss: 2.236987\n",
      "Epoch 98, loss: 2.222150\n",
      "Epoch 99, loss: 2.223802\n",
      "Epoch 100, loss: 2.225929\n",
      "Epoch 101, loss: 2.253899\n",
      "Epoch 102, loss: 2.222893\n",
      "Epoch 103, loss: 2.240430\n",
      "Epoch 104, loss: 2.235040\n",
      "Epoch 105, loss: 2.233118\n",
      "Epoch 106, loss: 2.232552\n",
      "Epoch 107, loss: 2.252834\n",
      "Epoch 108, loss: 2.224650\n",
      "Epoch 109, loss: 2.240029\n",
      "Epoch 110, loss: 2.250645\n",
      "Epoch 111, loss: 2.250658\n",
      "Epoch 112, loss: 2.231992\n",
      "Epoch 113, loss: 2.223946\n",
      "Epoch 114, loss: 2.231605\n",
      "Epoch 115, loss: 2.237743\n",
      "Epoch 116, loss: 2.236423\n",
      "Epoch 117, loss: 2.225090\n",
      "Epoch 118, loss: 2.218645\n",
      "Epoch 119, loss: 2.244033\n",
      "Epoch 120, loss: 2.223577\n",
      "Epoch 121, loss: 2.235765\n",
      "Epoch 122, loss: 2.245380\n",
      "Epoch 123, loss: 2.221582\n",
      "Epoch 124, loss: 2.229487\n",
      "Epoch 125, loss: 2.208824\n",
      "Epoch 126, loss: 2.225543\n",
      "Epoch 127, loss: 2.234516\n",
      "Epoch 128, loss: 2.227355\n",
      "Epoch 129, loss: 2.248349\n",
      "Epoch 130, loss: 2.238737\n",
      "Epoch 131, loss: 2.223301\n",
      "Epoch 132, loss: 2.227068\n",
      "Epoch 133, loss: 2.206827\n",
      "Epoch 134, loss: 2.216827\n",
      "Epoch 135, loss: 2.187166\n",
      "Epoch 136, loss: 2.241839\n",
      "Epoch 137, loss: 2.204872\n",
      "Epoch 138, loss: 2.219387\n",
      "Epoch 139, loss: 2.222487\n",
      "Epoch 140, loss: 2.221627\n",
      "Epoch 141, loss: 2.233108\n",
      "Epoch 142, loss: 2.216816\n",
      "Epoch 143, loss: 2.206103\n",
      "Epoch 144, loss: 2.205691\n",
      "Epoch 145, loss: 2.210273\n",
      "Epoch 146, loss: 2.208905\n",
      "Epoch 147, loss: 2.210639\n",
      "Epoch 148, loss: 2.209552\n",
      "Epoch 149, loss: 2.205648\n",
      "Epoch 150, loss: 2.211521\n",
      "Epoch 151, loss: 2.228576\n",
      "Epoch 152, loss: 2.228177\n",
      "Epoch 153, loss: 2.228223\n",
      "Epoch 154, loss: 2.212310\n",
      "Epoch 155, loss: 2.240287\n",
      "Epoch 156, loss: 2.248671\n",
      "Epoch 157, loss: 2.225022\n",
      "Epoch 158, loss: 2.213168\n",
      "Epoch 159, loss: 2.237343\n",
      "Epoch 160, loss: 2.205574\n",
      "Epoch 161, loss: 2.221758\n",
      "Epoch 162, loss: 2.220873\n",
      "Epoch 163, loss: 2.200254\n",
      "Epoch 164, loss: 2.214651\n",
      "Epoch 165, loss: 2.220911\n",
      "Epoch 166, loss: 2.197446\n",
      "Epoch 167, loss: 2.206621\n",
      "Epoch 168, loss: 2.194159\n",
      "Epoch 169, loss: 2.221674\n",
      "Epoch 170, loss: 2.216213\n",
      "Epoch 171, loss: 2.214357\n",
      "Epoch 172, loss: 2.196023\n",
      "Epoch 173, loss: 2.189044\n",
      "Epoch 174, loss: 2.220996\n",
      "Epoch 175, loss: 2.208775\n",
      "Epoch 176, loss: 2.226865\n",
      "Epoch 177, loss: 2.220164\n",
      "Epoch 178, loss: 2.228866\n",
      "Epoch 179, loss: 2.243642\n",
      "Epoch 180, loss: 2.204134\n",
      "Epoch 181, loss: 2.202482\n",
      "Epoch 182, loss: 2.185536\n",
      "Epoch 183, loss: 2.232295\n",
      "Epoch 184, loss: 2.205561\n",
      "Epoch 185, loss: 2.207052\n",
      "Epoch 186, loss: 2.183851\n",
      "Epoch 187, loss: 2.202593\n",
      "Epoch 188, loss: 2.205753\n",
      "Epoch 189, loss: 2.185756\n",
      "Epoch 190, loss: 2.207468\n",
      "Epoch 191, loss: 2.222333\n",
      "Epoch 192, loss: 2.192311\n",
      "Epoch 193, loss: 2.194895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, loss: 2.221916\n",
      "Epoch 195, loss: 2.207495\n",
      "Epoch 196, loss: 2.192514\n",
      "Epoch 197, loss: 2.209776\n",
      "Epoch 198, loss: 2.200625\n",
      "Epoch 199, loss: 2.207118\n",
      "learning rate = 0.001 reg = 1e-06 accuracy = 0.228\n",
      "Epoch 0, loss: 2.579454\n",
      "Epoch 1, loss: 2.546205\n",
      "Epoch 2, loss: 2.518229\n",
      "Epoch 3, loss: 2.494540\n",
      "Epoch 4, loss: 2.472315\n",
      "Epoch 5, loss: 2.452820\n",
      "Epoch 6, loss: 2.435934\n",
      "Epoch 7, loss: 2.421173\n",
      "Epoch 8, loss: 2.407135\n",
      "Epoch 9, loss: 2.395202\n",
      "Epoch 10, loss: 2.384444\n",
      "Epoch 11, loss: 2.374506\n",
      "Epoch 12, loss: 2.366217\n",
      "Epoch 13, loss: 2.360558\n",
      "Epoch 14, loss: 2.353113\n",
      "Epoch 15, loss: 2.347505\n",
      "Epoch 16, loss: 2.341959\n",
      "Epoch 17, loss: 2.337451\n",
      "Epoch 18, loss: 2.333219\n",
      "Epoch 19, loss: 2.330071\n",
      "Epoch 20, loss: 2.325924\n",
      "Epoch 21, loss: 2.323526\n",
      "Epoch 22, loss: 2.321978\n",
      "Epoch 23, loss: 2.319029\n",
      "Epoch 24, loss: 2.316566\n",
      "Epoch 25, loss: 2.316195\n",
      "Epoch 26, loss: 2.314354\n",
      "Epoch 27, loss: 2.312269\n",
      "Epoch 28, loss: 2.311590\n",
      "Epoch 29, loss: 2.309716\n",
      "Epoch 30, loss: 2.309613\n",
      "Epoch 31, loss: 2.308565\n",
      "Epoch 32, loss: 2.307277\n",
      "Epoch 33, loss: 2.306563\n",
      "Epoch 34, loss: 2.306200\n",
      "Epoch 35, loss: 2.306187\n",
      "Epoch 36, loss: 2.304953\n",
      "Epoch 37, loss: 2.304398\n",
      "Epoch 38, loss: 2.304542\n",
      "Epoch 39, loss: 2.304302\n",
      "Epoch 40, loss: 2.304270\n",
      "Epoch 41, loss: 2.303435\n",
      "Epoch 42, loss: 2.302390\n",
      "Epoch 43, loss: 2.303812\n",
      "Epoch 44, loss: 2.302841\n",
      "Epoch 45, loss: 2.303538\n",
      "Epoch 46, loss: 2.303043\n",
      "Epoch 47, loss: 2.302595\n",
      "Epoch 48, loss: 2.302303\n",
      "Epoch 49, loss: 2.302761\n",
      "Epoch 50, loss: 2.302695\n",
      "Epoch 51, loss: 2.301474\n",
      "Epoch 52, loss: 2.301190\n",
      "Epoch 53, loss: 2.302258\n",
      "Epoch 54, loss: 2.301905\n",
      "Epoch 55, loss: 2.302492\n",
      "Epoch 56, loss: 2.302271\n",
      "Epoch 57, loss: 2.301290\n",
      "Epoch 58, loss: 2.301158\n",
      "Epoch 59, loss: 2.302138\n",
      "Epoch 60, loss: 2.301273\n",
      "Epoch 61, loss: 2.302919\n",
      "Epoch 62, loss: 2.302053\n",
      "Epoch 63, loss: 2.302017\n",
      "Epoch 64, loss: 2.302271\n",
      "Epoch 65, loss: 2.302056\n",
      "Epoch 66, loss: 2.302025\n",
      "Epoch 67, loss: 2.300829\n",
      "Epoch 68, loss: 2.301828\n",
      "Epoch 69, loss: 2.302384\n",
      "Epoch 70, loss: 2.302245\n",
      "Epoch 71, loss: 2.301831\n",
      "Epoch 72, loss: 2.302740\n",
      "Epoch 73, loss: 2.301745\n",
      "Epoch 74, loss: 2.301401\n",
      "Epoch 75, loss: 2.302026\n",
      "Epoch 76, loss: 2.300995\n",
      "Epoch 77, loss: 2.302532\n",
      "Epoch 78, loss: 2.301671\n",
      "Epoch 79, loss: 2.301732\n",
      "Epoch 80, loss: 2.302001\n",
      "Epoch 81, loss: 2.301488\n",
      "Epoch 82, loss: 2.302621\n",
      "Epoch 83, loss: 2.302043\n",
      "Epoch 84, loss: 2.301920\n",
      "Epoch 85, loss: 2.302169\n",
      "Epoch 86, loss: 2.302211\n",
      "Epoch 87, loss: 2.302231\n",
      "Epoch 88, loss: 2.301786\n",
      "Epoch 89, loss: 2.301467\n",
      "Epoch 90, loss: 2.301619\n",
      "Epoch 91, loss: 2.301064\n",
      "Epoch 92, loss: 2.301503\n",
      "Epoch 93, loss: 2.301806\n",
      "Epoch 94, loss: 2.301791\n",
      "Epoch 95, loss: 2.301673\n",
      "Epoch 96, loss: 2.302045\n",
      "Epoch 97, loss: 2.302453\n",
      "Epoch 98, loss: 2.301194\n",
      "Epoch 99, loss: 2.302104\n",
      "Epoch 100, loss: 2.302198\n",
      "Epoch 101, loss: 2.302226\n",
      "Epoch 102, loss: 2.302143\n",
      "Epoch 103, loss: 2.301517\n",
      "Epoch 104, loss: 2.302478\n",
      "Epoch 105, loss: 2.301999\n",
      "Epoch 106, loss: 2.301988\n",
      "Epoch 107, loss: 2.302020\n",
      "Epoch 108, loss: 2.302121\n",
      "Epoch 109, loss: 2.302208\n",
      "Epoch 110, loss: 2.301724\n",
      "Epoch 111, loss: 2.301721\n",
      "Epoch 112, loss: 2.301870\n",
      "Epoch 113, loss: 2.301492\n",
      "Epoch 114, loss: 2.301962\n",
      "Epoch 115, loss: 2.302096\n",
      "Epoch 116, loss: 2.301131\n",
      "Epoch 117, loss: 2.301221\n",
      "Epoch 118, loss: 2.302057\n",
      "Epoch 119, loss: 2.301443\n",
      "Epoch 120, loss: 2.302068\n",
      "Epoch 121, loss: 2.301383\n",
      "Epoch 122, loss: 2.301793\n",
      "Epoch 123, loss: 2.300832\n",
      "Epoch 124, loss: 2.302040\n",
      "Epoch 125, loss: 2.301767\n",
      "Epoch 126, loss: 2.301234\n",
      "Epoch 127, loss: 2.302458\n",
      "Epoch 128, loss: 2.301210\n",
      "Epoch 129, loss: 2.302139\n",
      "Epoch 130, loss: 2.301782\n",
      "Epoch 131, loss: 2.301428\n",
      "Epoch 132, loss: 2.300265\n",
      "Epoch 133, loss: 2.300954\n",
      "Epoch 134, loss: 2.301312\n",
      "Epoch 135, loss: 2.302008\n",
      "Epoch 136, loss: 2.301342\n",
      "Epoch 137, loss: 2.301914\n",
      "Epoch 138, loss: 2.302867\n",
      "Epoch 139, loss: 2.302191\n",
      "Epoch 140, loss: 2.302518\n",
      "Epoch 141, loss: 2.301084\n",
      "Epoch 142, loss: 2.301761\n",
      "Epoch 143, loss: 2.302004\n",
      "Epoch 144, loss: 2.302063\n",
      "Epoch 145, loss: 2.302162\n",
      "Epoch 146, loss: 2.301925\n",
      "Epoch 147, loss: 2.301640\n",
      "Epoch 148, loss: 2.301703\n",
      "Epoch 149, loss: 2.300779\n",
      "Epoch 150, loss: 2.301818\n",
      "Epoch 151, loss: 2.302291\n",
      "Epoch 152, loss: 2.300902\n",
      "Epoch 153, loss: 2.301424\n",
      "Epoch 154, loss: 2.301981\n",
      "Epoch 155, loss: 2.301973\n",
      "Epoch 156, loss: 2.302460\n",
      "Epoch 157, loss: 2.302107\n",
      "Epoch 158, loss: 2.301899\n",
      "Epoch 159, loss: 2.301726\n",
      "Epoch 160, loss: 2.302160\n",
      "Epoch 161, loss: 2.301679\n",
      "Epoch 162, loss: 2.302296\n",
      "Epoch 163, loss: 2.301621\n",
      "Epoch 164, loss: 2.302061\n",
      "Epoch 165, loss: 2.302255\n",
      "Epoch 166, loss: 2.301199\n",
      "Epoch 167, loss: 2.300881\n",
      "Epoch 168, loss: 2.301601\n",
      "Epoch 169, loss: 2.301471\n",
      "Epoch 170, loss: 2.301159\n",
      "Epoch 171, loss: 2.300969\n",
      "Epoch 172, loss: 2.301391\n",
      "Epoch 173, loss: 2.301536\n",
      "Epoch 174, loss: 2.300104\n",
      "Epoch 175, loss: 2.301136\n",
      "Epoch 176, loss: 2.301294\n",
      "Epoch 177, loss: 2.301947\n",
      "Epoch 178, loss: 2.301900\n",
      "Epoch 179, loss: 2.300564\n",
      "Epoch 180, loss: 2.302329\n",
      "Epoch 181, loss: 2.302292\n",
      "Epoch 182, loss: 2.301135\n",
      "Epoch 183, loss: 2.301272\n",
      "Epoch 184, loss: 2.301572\n",
      "Epoch 185, loss: 2.301906\n",
      "Epoch 186, loss: 2.301975\n",
      "Epoch 187, loss: 2.302265\n",
      "Epoch 188, loss: 2.301903\n",
      "Epoch 189, loss: 2.301104\n",
      "Epoch 190, loss: 2.301330\n",
      "Epoch 191, loss: 2.302041\n",
      "Epoch 192, loss: 2.300934\n",
      "Epoch 193, loss: 2.301346\n",
      "Epoch 194, loss: 2.302324\n",
      "Epoch 195, loss: 2.301273\n",
      "Epoch 196, loss: 2.301414\n",
      "Epoch 197, loss: 2.301927\n",
      "Epoch 198, loss: 2.302075\n",
      "Epoch 199, loss: 2.301695\n",
      "learning rate = 0.0001 reg = 10 accuracy = 0.121\n",
      "Epoch 0, loss: 2.332246\n",
      "Epoch 1, loss: 2.331490\n",
      "Epoch 2, loss: 2.331622\n",
      "Epoch 3, loss: 2.331812\n",
      "Epoch 4, loss: 2.330835\n",
      "Epoch 5, loss: 2.329966\n",
      "Epoch 6, loss: 2.329345\n",
      "Epoch 7, loss: 2.329472\n",
      "Epoch 8, loss: 2.329502\n",
      "Epoch 9, loss: 2.328253\n",
      "Epoch 10, loss: 2.328207\n",
      "Epoch 11, loss: 2.329002\n",
      "Epoch 12, loss: 2.327339\n",
      "Epoch 13, loss: 2.327384\n",
      "Epoch 14, loss: 2.326798\n",
      "Epoch 15, loss: 2.325583\n",
      "Epoch 16, loss: 2.327048\n",
      "Epoch 17, loss: 2.326017\n",
      "Epoch 18, loss: 2.325875\n",
      "Epoch 19, loss: 2.325321\n",
      "Epoch 20, loss: 2.324505\n",
      "Epoch 21, loss: 2.323343\n",
      "Epoch 22, loss: 2.325585\n",
      "Epoch 23, loss: 2.323713\n",
      "Epoch 24, loss: 2.321911\n",
      "Epoch 25, loss: 2.323115\n",
      "Epoch 26, loss: 2.321649\n",
      "Epoch 27, loss: 2.321222\n",
      "Epoch 28, loss: 2.322301\n",
      "Epoch 29, loss: 2.318570\n",
      "Epoch 30, loss: 2.320120\n",
      "Epoch 31, loss: 2.320849\n",
      "Epoch 32, loss: 2.319889\n",
      "Epoch 33, loss: 2.318923\n",
      "Epoch 34, loss: 2.320329\n",
      "Epoch 35, loss: 2.317022\n",
      "Epoch 36, loss: 2.320417\n",
      "Epoch 37, loss: 2.320110\n",
      "Epoch 38, loss: 2.319321\n",
      "Epoch 39, loss: 2.318064\n",
      "Epoch 40, loss: 2.318382\n",
      "Epoch 41, loss: 2.318185\n",
      "Epoch 42, loss: 2.317307\n",
      "Epoch 43, loss: 2.316275\n",
      "Epoch 44, loss: 2.318050\n",
      "Epoch 45, loss: 2.315983\n",
      "Epoch 46, loss: 2.317592\n",
      "Epoch 47, loss: 2.315107\n",
      "Epoch 48, loss: 2.318669\n",
      "Epoch 49, loss: 2.311736\n",
      "Epoch 50, loss: 2.314574\n",
      "Epoch 51, loss: 2.314392\n",
      "Epoch 52, loss: 2.314884\n",
      "Epoch 53, loss: 2.312934\n",
      "Epoch 54, loss: 2.314098\n",
      "Epoch 55, loss: 2.314184\n",
      "Epoch 56, loss: 2.313040\n",
      "Epoch 57, loss: 2.312610\n",
      "Epoch 58, loss: 2.313785\n",
      "Epoch 59, loss: 2.314095\n",
      "Epoch 60, loss: 2.312957\n",
      "Epoch 61, loss: 2.310631\n",
      "Epoch 62, loss: 2.314733\n",
      "Epoch 63, loss: 2.312137\n",
      "Epoch 64, loss: 2.310121\n",
      "Epoch 65, loss: 2.311963\n",
      "Epoch 66, loss: 2.313431\n",
      "Epoch 67, loss: 2.311140\n",
      "Epoch 68, loss: 2.310691\n",
      "Epoch 69, loss: 2.310350\n",
      "Epoch 70, loss: 2.310665\n",
      "Epoch 71, loss: 2.311121\n",
      "Epoch 72, loss: 2.310640\n",
      "Epoch 73, loss: 2.310562\n",
      "Epoch 74, loss: 2.310355\n",
      "Epoch 75, loss: 2.309917\n",
      "Epoch 76, loss: 2.309208\n",
      "Epoch 77, loss: 2.307686\n",
      "Epoch 78, loss: 2.309233\n",
      "Epoch 79, loss: 2.308470\n",
      "Epoch 80, loss: 2.308126\n",
      "Epoch 81, loss: 2.308711\n",
      "Epoch 82, loss: 2.308092\n",
      "Epoch 83, loss: 2.305242\n",
      "Epoch 84, loss: 2.308327\n",
      "Epoch 85, loss: 2.309179\n",
      "Epoch 86, loss: 2.307107\n",
      "Epoch 87, loss: 2.308501\n",
      "Epoch 88, loss: 2.307545\n",
      "Epoch 89, loss: 2.309915\n",
      "Epoch 90, loss: 2.308375\n",
      "Epoch 91, loss: 2.307710\n",
      "Epoch 92, loss: 2.305277\n",
      "Epoch 93, loss: 2.306779\n",
      "Epoch 94, loss: 2.309000\n",
      "Epoch 95, loss: 2.304890\n",
      "Epoch 96, loss: 2.307546\n",
      "Epoch 97, loss: 2.306019\n",
      "Epoch 98, loss: 2.306620\n",
      "Epoch 99, loss: 2.308569\n",
      "Epoch 100, loss: 2.309396\n",
      "Epoch 101, loss: 2.306085\n",
      "Epoch 102, loss: 2.305668\n",
      "Epoch 103, loss: 2.310661\n",
      "Epoch 104, loss: 2.305220\n",
      "Epoch 105, loss: 2.304286\n",
      "Epoch 106, loss: 2.307050\n",
      "Epoch 107, loss: 2.305769\n",
      "Epoch 108, loss: 2.305021\n",
      "Epoch 109, loss: 2.308438\n",
      "Epoch 110, loss: 2.305823\n",
      "Epoch 111, loss: 2.306754\n",
      "Epoch 112, loss: 2.305031\n",
      "Epoch 113, loss: 2.305717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114, loss: 2.306695\n",
      "Epoch 115, loss: 2.305100\n",
      "Epoch 116, loss: 2.308607\n",
      "Epoch 117, loss: 2.305440\n",
      "Epoch 118, loss: 2.305403\n",
      "Epoch 119, loss: 2.304822\n",
      "Epoch 120, loss: 2.303490\n",
      "Epoch 121, loss: 2.301171\n",
      "Epoch 122, loss: 2.302395\n",
      "Epoch 123, loss: 2.304951\n",
      "Epoch 124, loss: 2.303121\n",
      "Epoch 125, loss: 2.304341\n",
      "Epoch 126, loss: 2.302801\n",
      "Epoch 127, loss: 2.299369\n",
      "Epoch 128, loss: 2.303499\n",
      "Epoch 129, loss: 2.305223\n",
      "Epoch 130, loss: 2.300892\n",
      "Epoch 131, loss: 2.303190\n",
      "Epoch 132, loss: 2.304751\n",
      "Epoch 133, loss: 2.303576\n",
      "Epoch 134, loss: 2.304506\n",
      "Epoch 135, loss: 2.299508\n",
      "Epoch 136, loss: 2.306109\n",
      "Epoch 137, loss: 2.303211\n",
      "Epoch 138, loss: 2.301326\n",
      "Epoch 139, loss: 2.300000\n",
      "Epoch 140, loss: 2.300643\n",
      "Epoch 141, loss: 2.302598\n",
      "Epoch 142, loss: 2.305997\n",
      "Epoch 143, loss: 2.300890\n",
      "Epoch 144, loss: 2.301011\n",
      "Epoch 145, loss: 2.300684\n",
      "Epoch 146, loss: 2.298675\n",
      "Epoch 147, loss: 2.300648\n",
      "Epoch 148, loss: 2.303242\n",
      "Epoch 149, loss: 2.302453\n",
      "Epoch 150, loss: 2.300854\n",
      "Epoch 151, loss: 2.299805\n",
      "Epoch 152, loss: 2.297184\n",
      "Epoch 153, loss: 2.299497\n",
      "Epoch 154, loss: 2.301172\n",
      "Epoch 155, loss: 2.299757\n",
      "Epoch 156, loss: 2.301423\n",
      "Epoch 157, loss: 2.298181\n",
      "Epoch 158, loss: 2.298474\n",
      "Epoch 159, loss: 2.301318\n",
      "Epoch 160, loss: 2.302625\n",
      "Epoch 161, loss: 2.300346\n",
      "Epoch 162, loss: 2.299907\n",
      "Epoch 163, loss: 2.299425\n",
      "Epoch 164, loss: 2.301846\n",
      "Epoch 165, loss: 2.300579\n",
      "Epoch 166, loss: 2.298498\n",
      "Epoch 167, loss: 2.301428\n",
      "Epoch 168, loss: 2.303625\n",
      "Epoch 169, loss: 2.299203\n",
      "Epoch 170, loss: 2.299401\n",
      "Epoch 171, loss: 2.302428\n",
      "Epoch 172, loss: 2.301068\n",
      "Epoch 173, loss: 2.298113\n",
      "Epoch 174, loss: 2.302250\n",
      "Epoch 175, loss: 2.303918\n",
      "Epoch 176, loss: 2.300261\n",
      "Epoch 177, loss: 2.300874\n",
      "Epoch 178, loss: 2.297752\n",
      "Epoch 179, loss: 2.298990\n",
      "Epoch 180, loss: 2.297926\n",
      "Epoch 181, loss: 2.297161\n",
      "Epoch 182, loss: 2.299695\n",
      "Epoch 183, loss: 2.301612\n",
      "Epoch 184, loss: 2.297188\n",
      "Epoch 185, loss: 2.300141\n",
      "Epoch 186, loss: 2.298235\n",
      "Epoch 187, loss: 2.298275\n",
      "Epoch 188, loss: 2.298881\n",
      "Epoch 189, loss: 2.296958\n",
      "Epoch 190, loss: 2.299269\n",
      "Epoch 191, loss: 2.296832\n",
      "Epoch 192, loss: 2.296981\n",
      "Epoch 193, loss: 2.300022\n",
      "Epoch 194, loss: 2.300564\n",
      "Epoch 195, loss: 2.292928\n",
      "Epoch 196, loss: 2.297735\n",
      "Epoch 197, loss: 2.299351\n",
      "Epoch 198, loss: 2.298798\n",
      "Epoch 199, loss: 2.301022\n",
      "learning rate = 0.0001 reg = 1 accuracy = 0.163\n",
      "Epoch 0, loss: 2.304488\n",
      "Epoch 1, loss: 2.305838\n",
      "Epoch 2, loss: 2.303974\n",
      "Epoch 3, loss: 2.305344\n",
      "Epoch 4, loss: 2.304323\n",
      "Epoch 5, loss: 2.303646\n",
      "Epoch 6, loss: 2.303498\n",
      "Epoch 7, loss: 2.305079\n",
      "Epoch 8, loss: 2.304113\n",
      "Epoch 9, loss: 2.304357\n",
      "Epoch 10, loss: 2.304815\n",
      "Epoch 11, loss: 2.305438\n",
      "Epoch 12, loss: 2.303299\n",
      "Epoch 13, loss: 2.303346\n",
      "Epoch 14, loss: 2.302703\n",
      "Epoch 15, loss: 2.303146\n",
      "Epoch 16, loss: 2.303467\n",
      "Epoch 17, loss: 2.304782\n",
      "Epoch 18, loss: 2.303629\n",
      "Epoch 19, loss: 2.303261\n",
      "Epoch 20, loss: 2.302743\n",
      "Epoch 21, loss: 2.302640\n",
      "Epoch 22, loss: 2.302570\n",
      "Epoch 23, loss: 2.303317\n",
      "Epoch 24, loss: 2.301911\n",
      "Epoch 25, loss: 2.303192\n",
      "Epoch 26, loss: 2.301410\n",
      "Epoch 27, loss: 2.303979\n",
      "Epoch 28, loss: 2.302001\n",
      "Epoch 29, loss: 2.301643\n",
      "Epoch 30, loss: 2.301673\n",
      "Epoch 31, loss: 2.304080\n",
      "Epoch 32, loss: 2.300598\n",
      "Epoch 33, loss: 2.302147\n",
      "Epoch 34, loss: 2.301873\n",
      "Epoch 35, loss: 2.301486\n",
      "Epoch 36, loss: 2.300878\n",
      "Epoch 37, loss: 2.299747\n",
      "Epoch 38, loss: 2.300940\n",
      "Epoch 39, loss: 2.301188\n",
      "Epoch 40, loss: 2.300899\n",
      "Epoch 41, loss: 2.300857\n",
      "Epoch 42, loss: 2.301263\n",
      "Epoch 43, loss: 2.300310\n",
      "Epoch 44, loss: 2.301112\n",
      "Epoch 45, loss: 2.298511\n",
      "Epoch 46, loss: 2.300870\n",
      "Epoch 47, loss: 2.301268\n",
      "Epoch 48, loss: 2.299352\n",
      "Epoch 49, loss: 2.300756\n",
      "Epoch 50, loss: 2.300403\n",
      "Epoch 51, loss: 2.299829\n",
      "Epoch 52, loss: 2.300144\n",
      "Epoch 53, loss: 2.299550\n",
      "Epoch 54, loss: 2.300445\n",
      "Epoch 55, loss: 2.301366\n",
      "Epoch 56, loss: 2.301768\n",
      "Epoch 57, loss: 2.300951\n",
      "Epoch 58, loss: 2.300763\n",
      "Epoch 59, loss: 2.301656\n",
      "Epoch 60, loss: 2.301156\n",
      "Epoch 61, loss: 2.301669\n",
      "Epoch 62, loss: 2.301217\n",
      "Epoch 63, loss: 2.299368\n",
      "Epoch 64, loss: 2.298287\n",
      "Epoch 65, loss: 2.301501\n",
      "Epoch 66, loss: 2.299214\n",
      "Epoch 67, loss: 2.299151\n",
      "Epoch 68, loss: 2.303152\n",
      "Epoch 69, loss: 2.301028\n",
      "Epoch 70, loss: 2.298702\n",
      "Epoch 71, loss: 2.297219\n",
      "Epoch 72, loss: 2.299898\n",
      "Epoch 73, loss: 2.299150\n",
      "Epoch 74, loss: 2.299109\n",
      "Epoch 75, loss: 2.297272\n",
      "Epoch 76, loss: 2.295384\n",
      "Epoch 77, loss: 2.299126\n",
      "Epoch 78, loss: 2.296335\n",
      "Epoch 79, loss: 2.294586\n",
      "Epoch 80, loss: 2.297528\n",
      "Epoch 81, loss: 2.296118\n",
      "Epoch 82, loss: 2.297959\n",
      "Epoch 83, loss: 2.294300\n",
      "Epoch 84, loss: 2.298398\n",
      "Epoch 85, loss: 2.299241\n",
      "Epoch 86, loss: 2.298665\n",
      "Epoch 87, loss: 2.294892\n",
      "Epoch 88, loss: 2.295834\n",
      "Epoch 89, loss: 2.298538\n",
      "Epoch 90, loss: 2.298859\n",
      "Epoch 91, loss: 2.298163\n",
      "Epoch 92, loss: 2.301138\n",
      "Epoch 93, loss: 2.300594\n",
      "Epoch 94, loss: 2.296658\n",
      "Epoch 95, loss: 2.296183\n",
      "Epoch 96, loss: 2.298730\n",
      "Epoch 97, loss: 2.295525\n",
      "Epoch 98, loss: 2.297900\n",
      "Epoch 99, loss: 2.297634\n",
      "Epoch 100, loss: 2.295796\n",
      "Epoch 101, loss: 2.295826\n",
      "Epoch 102, loss: 2.297373\n",
      "Epoch 103, loss: 2.294389\n",
      "Epoch 104, loss: 2.296955\n",
      "Epoch 105, loss: 2.296096\n",
      "Epoch 106, loss: 2.294627\n",
      "Epoch 107, loss: 2.297300\n",
      "Epoch 108, loss: 2.293659\n",
      "Epoch 109, loss: 2.295377\n",
      "Epoch 110, loss: 2.296414\n",
      "Epoch 111, loss: 2.292860\n",
      "Epoch 112, loss: 2.293936\n",
      "Epoch 113, loss: 2.295319\n",
      "Epoch 114, loss: 2.291236\n",
      "Epoch 115, loss: 2.294604\n",
      "Epoch 116, loss: 2.295733\n",
      "Epoch 117, loss: 2.295298\n",
      "Epoch 118, loss: 2.298411\n",
      "Epoch 119, loss: 2.293126\n",
      "Epoch 120, loss: 2.295916\n",
      "Epoch 121, loss: 2.294121\n",
      "Epoch 122, loss: 2.294069\n",
      "Epoch 123, loss: 2.297248\n",
      "Epoch 124, loss: 2.295471\n",
      "Epoch 125, loss: 2.296412\n",
      "Epoch 126, loss: 2.293274\n",
      "Epoch 127, loss: 2.293873\n",
      "Epoch 128, loss: 2.293150\n",
      "Epoch 129, loss: 2.296613\n",
      "Epoch 130, loss: 2.294168\n",
      "Epoch 131, loss: 2.293234\n",
      "Epoch 132, loss: 2.294302\n",
      "Epoch 133, loss: 2.292755\n",
      "Epoch 134, loss: 2.291187\n",
      "Epoch 135, loss: 2.292520\n",
      "Epoch 136, loss: 2.290778\n",
      "Epoch 137, loss: 2.294392\n",
      "Epoch 138, loss: 2.298592\n",
      "Epoch 139, loss: 2.294970\n",
      "Epoch 140, loss: 2.295506\n",
      "Epoch 141, loss: 2.294580\n",
      "Epoch 142, loss: 2.293207\n",
      "Epoch 143, loss: 2.289909\n",
      "Epoch 144, loss: 2.294922\n",
      "Epoch 145, loss: 2.293061\n",
      "Epoch 146, loss: 2.290834\n",
      "Epoch 147, loss: 2.292078\n",
      "Epoch 148, loss: 2.293899\n",
      "Epoch 149, loss: 2.296414\n",
      "Epoch 150, loss: 2.294298\n",
      "Epoch 151, loss: 2.291015\n",
      "Epoch 152, loss: 2.289229\n",
      "Epoch 153, loss: 2.290572\n",
      "Epoch 154, loss: 2.291769\n",
      "Epoch 155, loss: 2.289391\n",
      "Epoch 156, loss: 2.294462\n",
      "Epoch 157, loss: 2.291462\n",
      "Epoch 158, loss: 2.287022\n",
      "Epoch 159, loss: 2.296719\n",
      "Epoch 160, loss: 2.290384\n",
      "Epoch 161, loss: 2.295997\n",
      "Epoch 162, loss: 2.290129\n",
      "Epoch 163, loss: 2.292293\n",
      "Epoch 164, loss: 2.291818\n",
      "Epoch 165, loss: 2.289600\n",
      "Epoch 166, loss: 2.290100\n",
      "Epoch 167, loss: 2.287742\n",
      "Epoch 168, loss: 2.295073\n",
      "Epoch 169, loss: 2.290799\n",
      "Epoch 170, loss: 2.290237\n",
      "Epoch 171, loss: 2.291673\n",
      "Epoch 172, loss: 2.291911\n",
      "Epoch 173, loss: 2.293223\n",
      "Epoch 174, loss: 2.286768\n",
      "Epoch 175, loss: 2.296512\n",
      "Epoch 176, loss: 2.284772\n",
      "Epoch 177, loss: 2.287767\n",
      "Epoch 178, loss: 2.292294\n",
      "Epoch 179, loss: 2.291828\n",
      "Epoch 180, loss: 2.285149\n",
      "Epoch 181, loss: 2.291026\n",
      "Epoch 182, loss: 2.288128\n",
      "Epoch 183, loss: 2.294803\n",
      "Epoch 184, loss: 2.285341\n",
      "Epoch 185, loss: 2.283709\n",
      "Epoch 186, loss: 2.295370\n",
      "Epoch 187, loss: 2.291574\n",
      "Epoch 188, loss: 2.289719\n",
      "Epoch 189, loss: 2.288574\n",
      "Epoch 190, loss: 2.288672\n",
      "Epoch 191, loss: 2.293335\n",
      "Epoch 192, loss: 2.290481\n",
      "Epoch 193, loss: 2.289429\n",
      "Epoch 194, loss: 2.294736\n",
      "Epoch 195, loss: 2.288944\n",
      "Epoch 196, loss: 2.288372\n",
      "Epoch 197, loss: 2.291755\n",
      "Epoch 198, loss: 2.287504\n",
      "Epoch 199, loss: 2.289263\n",
      "learning rate = 0.0001 reg = 0.1 accuracy = 0.171\n",
      "Epoch 0, loss: 2.302571\n",
      "Epoch 1, loss: 2.302368\n",
      "Epoch 2, loss: 2.302400\n",
      "Epoch 3, loss: 2.302227\n",
      "Epoch 4, loss: 2.302511\n",
      "Epoch 5, loss: 2.301115\n",
      "Epoch 6, loss: 2.302872\n",
      "Epoch 7, loss: 2.300645\n",
      "Epoch 8, loss: 2.301534\n",
      "Epoch 9, loss: 2.301098\n",
      "Epoch 10, loss: 2.301808\n",
      "Epoch 11, loss: 2.301498\n",
      "Epoch 12, loss: 2.301909\n",
      "Epoch 13, loss: 2.301633\n",
      "Epoch 14, loss: 2.301158\n",
      "Epoch 15, loss: 2.300681\n",
      "Epoch 16, loss: 2.301388\n",
      "Epoch 17, loss: 2.301484\n",
      "Epoch 18, loss: 2.300625\n",
      "Epoch 19, loss: 2.301959\n",
      "Epoch 20, loss: 2.299436\n",
      "Epoch 21, loss: 2.301582\n",
      "Epoch 22, loss: 2.300263\n",
      "Epoch 23, loss: 2.299859\n",
      "Epoch 24, loss: 2.300077\n",
      "Epoch 25, loss: 2.300123\n",
      "Epoch 26, loss: 2.299850\n",
      "Epoch 27, loss: 2.299703\n",
      "Epoch 28, loss: 2.300465\n",
      "Epoch 29, loss: 2.298846\n",
      "Epoch 30, loss: 2.299352\n",
      "Epoch 31, loss: 2.299432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, loss: 2.299379\n",
      "Epoch 33, loss: 2.298740\n",
      "Epoch 34, loss: 2.300010\n",
      "Epoch 35, loss: 2.298870\n",
      "Epoch 36, loss: 2.298724\n",
      "Epoch 37, loss: 2.299246\n",
      "Epoch 38, loss: 2.298372\n",
      "Epoch 39, loss: 2.297946\n",
      "Epoch 40, loss: 2.299328\n",
      "Epoch 41, loss: 2.298692\n",
      "Epoch 42, loss: 2.300874\n",
      "Epoch 43, loss: 2.299383\n",
      "Epoch 44, loss: 2.298594\n",
      "Epoch 45, loss: 2.298081\n",
      "Epoch 46, loss: 2.297933\n",
      "Epoch 47, loss: 2.295560\n",
      "Epoch 48, loss: 2.295060\n",
      "Epoch 49, loss: 2.295380\n",
      "Epoch 50, loss: 2.298423\n",
      "Epoch 51, loss: 2.299358\n",
      "Epoch 52, loss: 2.296926\n",
      "Epoch 53, loss: 2.297629\n",
      "Epoch 54, loss: 2.297107\n",
      "Epoch 55, loss: 2.297537\n",
      "Epoch 56, loss: 2.296165\n",
      "Epoch 57, loss: 2.295932\n",
      "Epoch 58, loss: 2.297481\n",
      "Epoch 59, loss: 2.297090\n",
      "Epoch 60, loss: 2.294838\n",
      "Epoch 61, loss: 2.296643\n",
      "Epoch 62, loss: 2.297436\n",
      "Epoch 63, loss: 2.297383\n",
      "Epoch 64, loss: 2.296310\n",
      "Epoch 65, loss: 2.296838\n",
      "Epoch 66, loss: 2.296570\n",
      "Epoch 67, loss: 2.296568\n",
      "Epoch 68, loss: 2.297706\n",
      "Epoch 69, loss: 2.297076\n",
      "Epoch 70, loss: 2.292801\n",
      "Epoch 71, loss: 2.294498\n",
      "Epoch 72, loss: 2.295987\n",
      "Epoch 73, loss: 2.297111\n",
      "Epoch 74, loss: 2.298033\n",
      "Epoch 75, loss: 2.297114\n",
      "Epoch 76, loss: 2.295037\n",
      "Epoch 77, loss: 2.294074\n",
      "Epoch 78, loss: 2.294023\n",
      "Epoch 79, loss: 2.292425\n",
      "Epoch 80, loss: 2.296973\n",
      "Epoch 81, loss: 2.295319\n",
      "Epoch 82, loss: 2.296492\n",
      "Epoch 83, loss: 2.291853\n",
      "Epoch 84, loss: 2.295293\n",
      "Epoch 85, loss: 2.295575\n",
      "Epoch 86, loss: 2.291155\n",
      "Epoch 87, loss: 2.290303\n",
      "Epoch 88, loss: 2.298053\n",
      "Epoch 89, loss: 2.293779\n",
      "Epoch 90, loss: 2.297132\n",
      "Epoch 91, loss: 2.293265\n",
      "Epoch 92, loss: 2.297204\n",
      "Epoch 93, loss: 2.293365\n",
      "Epoch 94, loss: 2.294901\n",
      "Epoch 95, loss: 2.293439\n",
      "Epoch 96, loss: 2.295999\n",
      "Epoch 97, loss: 2.295648\n",
      "Epoch 98, loss: 2.289488\n",
      "Epoch 99, loss: 2.296910\n",
      "Epoch 100, loss: 2.289462\n",
      "Epoch 101, loss: 2.292491\n",
      "Epoch 102, loss: 2.296307\n",
      "Epoch 103, loss: 2.289673\n",
      "Epoch 104, loss: 2.293483\n",
      "Epoch 105, loss: 2.290463\n",
      "Epoch 106, loss: 2.293318\n",
      "Epoch 107, loss: 2.296891\n",
      "Epoch 108, loss: 2.293644\n",
      "Epoch 109, loss: 2.290289\n",
      "Epoch 110, loss: 2.290407\n",
      "Epoch 111, loss: 2.298821\n",
      "Epoch 112, loss: 2.290526\n",
      "Epoch 113, loss: 2.287636\n",
      "Epoch 114, loss: 2.293747\n",
      "Epoch 115, loss: 2.295963\n",
      "Epoch 116, loss: 2.294805\n",
      "Epoch 117, loss: 2.291678\n",
      "Epoch 118, loss: 2.290347\n",
      "Epoch 119, loss: 2.293438\n",
      "Epoch 120, loss: 2.290189\n",
      "Epoch 121, loss: 2.291014\n",
      "Epoch 122, loss: 2.290994\n",
      "Epoch 123, loss: 2.286539\n",
      "Epoch 124, loss: 2.296599\n",
      "Epoch 125, loss: 2.288456\n",
      "Epoch 126, loss: 2.293670\n",
      "Epoch 127, loss: 2.290920\n",
      "Epoch 128, loss: 2.289397\n",
      "Epoch 129, loss: 2.288618\n",
      "Epoch 130, loss: 2.286362\n",
      "Epoch 131, loss: 2.295125\n",
      "Epoch 132, loss: 2.291230\n",
      "Epoch 133, loss: 2.290536\n",
      "Epoch 134, loss: 2.294455\n",
      "Epoch 135, loss: 2.289821\n",
      "Epoch 136, loss: 2.289736\n",
      "Epoch 137, loss: 2.292013\n",
      "Epoch 138, loss: 2.289681\n",
      "Epoch 139, loss: 2.291129\n",
      "Epoch 140, loss: 2.291835\n",
      "Epoch 141, loss: 2.288792\n",
      "Epoch 142, loss: 2.291979\n",
      "Epoch 143, loss: 2.293691\n",
      "Epoch 144, loss: 2.288400\n",
      "Epoch 145, loss: 2.288343\n",
      "Epoch 146, loss: 2.289508\n",
      "Epoch 147, loss: 2.291671\n",
      "Epoch 148, loss: 2.291801\n",
      "Epoch 149, loss: 2.287620\n",
      "Epoch 150, loss: 2.294147\n",
      "Epoch 151, loss: 2.284626\n",
      "Epoch 152, loss: 2.287088\n",
      "Epoch 153, loss: 2.289443\n",
      "Epoch 154, loss: 2.287597\n",
      "Epoch 155, loss: 2.289193\n",
      "Epoch 156, loss: 2.289215\n",
      "Epoch 157, loss: 2.295170\n",
      "Epoch 158, loss: 2.290123\n",
      "Epoch 159, loss: 2.290207\n",
      "Epoch 160, loss: 2.286291\n",
      "Epoch 161, loss: 2.289164\n",
      "Epoch 162, loss: 2.287192\n",
      "Epoch 163, loss: 2.287625\n",
      "Epoch 164, loss: 2.286415\n",
      "Epoch 165, loss: 2.287003\n",
      "Epoch 166, loss: 2.290084\n",
      "Epoch 167, loss: 2.289467\n",
      "Epoch 168, loss: 2.287505\n",
      "Epoch 169, loss: 2.286190\n",
      "Epoch 170, loss: 2.284164\n",
      "Epoch 171, loss: 2.286328\n",
      "Epoch 172, loss: 2.284226\n",
      "Epoch 173, loss: 2.291429\n",
      "Epoch 174, loss: 2.288778\n",
      "Epoch 175, loss: 2.285299\n",
      "Epoch 176, loss: 2.288425\n",
      "Epoch 177, loss: 2.288857\n",
      "Epoch 178, loss: 2.289518\n",
      "Epoch 179, loss: 2.289496\n",
      "Epoch 180, loss: 2.287199\n",
      "Epoch 181, loss: 2.288249\n",
      "Epoch 182, loss: 2.286229\n",
      "Epoch 183, loss: 2.284153\n",
      "Epoch 184, loss: 2.291898\n",
      "Epoch 185, loss: 2.281706\n",
      "Epoch 186, loss: 2.288066\n",
      "Epoch 187, loss: 2.284826\n",
      "Epoch 188, loss: 2.293125\n",
      "Epoch 189, loss: 2.288343\n",
      "Epoch 190, loss: 2.283136\n",
      "Epoch 191, loss: 2.285174\n",
      "Epoch 192, loss: 2.284534\n",
      "Epoch 193, loss: 2.290715\n",
      "Epoch 194, loss: 2.283352\n",
      "Epoch 195, loss: 2.282754\n",
      "Epoch 196, loss: 2.286604\n",
      "Epoch 197, loss: 2.289487\n",
      "Epoch 198, loss: 2.287473\n",
      "Epoch 199, loss: 2.285197\n",
      "learning rate = 0.0001 reg = 0.01 accuracy = 0.174\n",
      "Epoch 0, loss: 2.302426\n",
      "Epoch 1, loss: 2.301600\n",
      "Epoch 2, loss: 2.302126\n",
      "Epoch 3, loss: 2.302110\n",
      "Epoch 4, loss: 2.303329\n",
      "Epoch 5, loss: 2.302246\n",
      "Epoch 6, loss: 2.301311\n",
      "Epoch 7, loss: 2.302970\n",
      "Epoch 8, loss: 2.300954\n",
      "Epoch 9, loss: 2.301789\n",
      "Epoch 10, loss: 2.301641\n",
      "Epoch 11, loss: 2.300750\n",
      "Epoch 12, loss: 2.301722\n",
      "Epoch 13, loss: 2.301725\n",
      "Epoch 14, loss: 2.301175\n",
      "Epoch 15, loss: 2.300735\n",
      "Epoch 16, loss: 2.300793\n",
      "Epoch 17, loss: 2.300708\n",
      "Epoch 18, loss: 2.301000\n",
      "Epoch 19, loss: 2.301159\n",
      "Epoch 20, loss: 2.302486\n",
      "Epoch 21, loss: 2.300543\n",
      "Epoch 22, loss: 2.301900\n",
      "Epoch 23, loss: 2.301172\n",
      "Epoch 24, loss: 2.301566\n",
      "Epoch 25, loss: 2.301232\n",
      "Epoch 26, loss: 2.300492\n",
      "Epoch 27, loss: 2.300761\n",
      "Epoch 28, loss: 2.300685\n",
      "Epoch 29, loss: 2.299330\n",
      "Epoch 30, loss: 2.301626\n",
      "Epoch 31, loss: 2.299938\n",
      "Epoch 32, loss: 2.300180\n",
      "Epoch 33, loss: 2.298512\n",
      "Epoch 34, loss: 2.298926\n",
      "Epoch 35, loss: 2.298471\n",
      "Epoch 36, loss: 2.298732\n",
      "Epoch 37, loss: 2.299326\n",
      "Epoch 38, loss: 2.299484\n",
      "Epoch 39, loss: 2.299330\n",
      "Epoch 40, loss: 2.297194\n",
      "Epoch 41, loss: 2.298968\n",
      "Epoch 42, loss: 2.299522\n",
      "Epoch 43, loss: 2.299009\n",
      "Epoch 44, loss: 2.298427\n",
      "Epoch 45, loss: 2.299291\n",
      "Epoch 46, loss: 2.299026\n",
      "Epoch 47, loss: 2.298646\n",
      "Epoch 48, loss: 2.296432\n",
      "Epoch 49, loss: 2.298297\n",
      "Epoch 50, loss: 2.298453\n",
      "Epoch 51, loss: 2.299749\n",
      "Epoch 52, loss: 2.298733\n",
      "Epoch 53, loss: 2.300131\n",
      "Epoch 54, loss: 2.296876\n",
      "Epoch 55, loss: 2.298380\n",
      "Epoch 56, loss: 2.297539\n",
      "Epoch 57, loss: 2.295828\n",
      "Epoch 58, loss: 2.296176\n",
      "Epoch 59, loss: 2.297018\n",
      "Epoch 60, loss: 2.295350\n",
      "Epoch 61, loss: 2.296978\n",
      "Epoch 62, loss: 2.296259\n",
      "Epoch 63, loss: 2.296764\n",
      "Epoch 64, loss: 2.295198\n",
      "Epoch 65, loss: 2.298354\n",
      "Epoch 66, loss: 2.296274\n",
      "Epoch 67, loss: 2.294356\n",
      "Epoch 68, loss: 2.297870\n",
      "Epoch 69, loss: 2.297905\n",
      "Epoch 70, loss: 2.296471\n",
      "Epoch 71, loss: 2.295142\n",
      "Epoch 72, loss: 2.293895\n",
      "Epoch 73, loss: 2.293646\n",
      "Epoch 74, loss: 2.295726\n",
      "Epoch 75, loss: 2.296133\n",
      "Epoch 76, loss: 2.294904\n",
      "Epoch 77, loss: 2.295489\n",
      "Epoch 78, loss: 2.294712\n",
      "Epoch 79, loss: 2.296135\n",
      "Epoch 80, loss: 2.295119\n",
      "Epoch 81, loss: 2.294835\n",
      "Epoch 82, loss: 2.295561\n",
      "Epoch 83, loss: 2.296472\n",
      "Epoch 84, loss: 2.297397\n",
      "Epoch 85, loss: 2.296221\n",
      "Epoch 86, loss: 2.295699\n",
      "Epoch 87, loss: 2.293211\n",
      "Epoch 88, loss: 2.293376\n",
      "Epoch 89, loss: 2.293276\n",
      "Epoch 90, loss: 2.294621\n",
      "Epoch 91, loss: 2.294058\n",
      "Epoch 92, loss: 2.293776\n",
      "Epoch 93, loss: 2.295098\n",
      "Epoch 94, loss: 2.294683\n",
      "Epoch 95, loss: 2.296650\n",
      "Epoch 96, loss: 2.293413\n",
      "Epoch 97, loss: 2.291047\n",
      "Epoch 98, loss: 2.291808\n",
      "Epoch 99, loss: 2.294660\n",
      "Epoch 100, loss: 2.292100\n",
      "Epoch 101, loss: 2.297720\n",
      "Epoch 102, loss: 2.295201\n",
      "Epoch 103, loss: 2.293602\n",
      "Epoch 104, loss: 2.292226\n",
      "Epoch 105, loss: 2.291046\n",
      "Epoch 106, loss: 2.291202\n",
      "Epoch 107, loss: 2.293290\n",
      "Epoch 108, loss: 2.295309\n",
      "Epoch 109, loss: 2.291062\n",
      "Epoch 110, loss: 2.288152\n",
      "Epoch 111, loss: 2.291611\n",
      "Epoch 112, loss: 2.292118\n",
      "Epoch 113, loss: 2.292365\n",
      "Epoch 114, loss: 2.291395\n",
      "Epoch 115, loss: 2.287616\n",
      "Epoch 116, loss: 2.297083\n",
      "Epoch 117, loss: 2.289023\n",
      "Epoch 118, loss: 2.293444\n",
      "Epoch 119, loss: 2.292644\n",
      "Epoch 120, loss: 2.291753\n",
      "Epoch 121, loss: 2.289613\n",
      "Epoch 122, loss: 2.292443\n",
      "Epoch 123, loss: 2.290204\n",
      "Epoch 124, loss: 2.291576\n",
      "Epoch 125, loss: 2.291781\n",
      "Epoch 126, loss: 2.293262\n",
      "Epoch 127, loss: 2.289193\n",
      "Epoch 128, loss: 2.287672\n",
      "Epoch 129, loss: 2.292743\n",
      "Epoch 130, loss: 2.292842\n",
      "Epoch 131, loss: 2.296999\n",
      "Epoch 132, loss: 2.288766\n",
      "Epoch 133, loss: 2.292989\n",
      "Epoch 134, loss: 2.284300\n",
      "Epoch 135, loss: 2.290189\n",
      "Epoch 136, loss: 2.295034\n",
      "Epoch 137, loss: 2.289053\n",
      "Epoch 138, loss: 2.293401\n",
      "Epoch 139, loss: 2.291524\n",
      "Epoch 140, loss: 2.288445\n",
      "Epoch 141, loss: 2.295011\n",
      "Epoch 142, loss: 2.290300\n",
      "Epoch 143, loss: 2.290973\n",
      "Epoch 144, loss: 2.287694\n",
      "Epoch 145, loss: 2.287944\n",
      "Epoch 146, loss: 2.291454\n",
      "Epoch 147, loss: 2.292044\n",
      "Epoch 148, loss: 2.288797\n",
      "Epoch 149, loss: 2.296160\n",
      "Epoch 150, loss: 2.290780\n",
      "Epoch 151, loss: 2.290309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152, loss: 2.294723\n",
      "Epoch 153, loss: 2.290910\n",
      "Epoch 154, loss: 2.289284\n",
      "Epoch 155, loss: 2.290514\n",
      "Epoch 156, loss: 2.290682\n",
      "Epoch 157, loss: 2.288914\n",
      "Epoch 158, loss: 2.290001\n",
      "Epoch 159, loss: 2.288814\n",
      "Epoch 160, loss: 2.288310\n",
      "Epoch 161, loss: 2.289845\n",
      "Epoch 162, loss: 2.292683\n",
      "Epoch 163, loss: 2.293610\n",
      "Epoch 164, loss: 2.288811\n",
      "Epoch 165, loss: 2.290381\n",
      "Epoch 166, loss: 2.285331\n",
      "Epoch 167, loss: 2.287258\n",
      "Epoch 168, loss: 2.287092\n",
      "Epoch 169, loss: 2.290066\n",
      "Epoch 170, loss: 2.287293\n",
      "Epoch 171, loss: 2.284372\n",
      "Epoch 172, loss: 2.288307\n",
      "Epoch 173, loss: 2.281652\n",
      "Epoch 174, loss: 2.287340\n",
      "Epoch 175, loss: 2.287421\n",
      "Epoch 176, loss: 2.292771\n",
      "Epoch 177, loss: 2.290586\n",
      "Epoch 178, loss: 2.290043\n",
      "Epoch 179, loss: 2.283218\n",
      "Epoch 180, loss: 2.289100\n",
      "Epoch 181, loss: 2.288434\n",
      "Epoch 182, loss: 2.290554\n",
      "Epoch 183, loss: 2.291686\n",
      "Epoch 184, loss: 2.285299\n",
      "Epoch 185, loss: 2.288780\n",
      "Epoch 186, loss: 2.286046\n",
      "Epoch 187, loss: 2.286105\n",
      "Epoch 188, loss: 2.282550\n",
      "Epoch 189, loss: 2.285968\n",
      "Epoch 190, loss: 2.287982\n",
      "Epoch 191, loss: 2.282853\n",
      "Epoch 192, loss: 2.292507\n",
      "Epoch 193, loss: 2.279363\n",
      "Epoch 194, loss: 2.282266\n",
      "Epoch 195, loss: 2.284219\n",
      "Epoch 196, loss: 2.285448\n",
      "Epoch 197, loss: 2.286893\n",
      "Epoch 198, loss: 2.286847\n",
      "Epoch 199, loss: 2.286653\n",
      "learning rate = 0.0001 reg = 0.001 accuracy = 0.176\n",
      "Epoch 0, loss: 2.302517\n",
      "Epoch 1, loss: 2.301592\n",
      "Epoch 2, loss: 2.302135\n",
      "Epoch 3, loss: 2.301720\n",
      "Epoch 4, loss: 2.302435\n",
      "Epoch 5, loss: 2.302381\n",
      "Epoch 6, loss: 2.301810\n",
      "Epoch 7, loss: 2.301389\n",
      "Epoch 8, loss: 2.301362\n",
      "Epoch 9, loss: 2.301556\n",
      "Epoch 10, loss: 2.300658\n",
      "Epoch 11, loss: 2.299573\n",
      "Epoch 12, loss: 2.301047\n",
      "Epoch 13, loss: 2.299599\n",
      "Epoch 14, loss: 2.301396\n",
      "Epoch 15, loss: 2.300941\n",
      "Epoch 16, loss: 2.300052\n",
      "Epoch 17, loss: 2.300242\n",
      "Epoch 18, loss: 2.300319\n",
      "Epoch 19, loss: 2.301340\n",
      "Epoch 20, loss: 2.301652\n",
      "Epoch 21, loss: 2.300217\n",
      "Epoch 22, loss: 2.301264\n",
      "Epoch 23, loss: 2.300498\n",
      "Epoch 24, loss: 2.300126\n",
      "Epoch 25, loss: 2.299811\n",
      "Epoch 26, loss: 2.299726\n",
      "Epoch 27, loss: 2.299391\n",
      "Epoch 28, loss: 2.300152\n",
      "Epoch 29, loss: 2.299389\n",
      "Epoch 30, loss: 2.301224\n",
      "Epoch 31, loss: 2.300042\n",
      "Epoch 32, loss: 2.301268\n",
      "Epoch 33, loss: 2.300566\n",
      "Epoch 34, loss: 2.298593\n",
      "Epoch 35, loss: 2.299775\n",
      "Epoch 36, loss: 2.296912\n",
      "Epoch 37, loss: 2.298023\n",
      "Epoch 38, loss: 2.299186\n",
      "Epoch 39, loss: 2.300622\n",
      "Epoch 40, loss: 2.298192\n",
      "Epoch 41, loss: 2.298199\n",
      "Epoch 42, loss: 2.298378\n",
      "Epoch 43, loss: 2.297171\n",
      "Epoch 44, loss: 2.298971\n",
      "Epoch 45, loss: 2.297134\n",
      "Epoch 46, loss: 2.297432\n",
      "Epoch 47, loss: 2.297822\n",
      "Epoch 48, loss: 2.297416\n",
      "Epoch 49, loss: 2.295525\n",
      "Epoch 50, loss: 2.297094\n",
      "Epoch 51, loss: 2.298733\n",
      "Epoch 52, loss: 2.298586\n",
      "Epoch 53, loss: 2.297956\n",
      "Epoch 54, loss: 2.297130\n",
      "Epoch 55, loss: 2.293112\n",
      "Epoch 56, loss: 2.297142\n",
      "Epoch 57, loss: 2.295345\n",
      "Epoch 58, loss: 2.295994\n",
      "Epoch 59, loss: 2.297254\n",
      "Epoch 60, loss: 2.296117\n",
      "Epoch 61, loss: 2.298530\n",
      "Epoch 62, loss: 2.296163\n",
      "Epoch 63, loss: 2.296433\n",
      "Epoch 64, loss: 2.293779\n",
      "Epoch 65, loss: 2.295001\n",
      "Epoch 66, loss: 2.295774\n",
      "Epoch 67, loss: 2.299310\n",
      "Epoch 68, loss: 2.293835\n",
      "Epoch 69, loss: 2.296118\n",
      "Epoch 70, loss: 2.298240\n",
      "Epoch 71, loss: 2.295667\n",
      "Epoch 72, loss: 2.294469\n",
      "Epoch 73, loss: 2.297959\n",
      "Epoch 74, loss: 2.293672\n",
      "Epoch 75, loss: 2.293902\n",
      "Epoch 76, loss: 2.295655\n",
      "Epoch 77, loss: 2.292670\n",
      "Epoch 78, loss: 2.293954\n",
      "Epoch 79, loss: 2.295769\n",
      "Epoch 80, loss: 2.293334\n",
      "Epoch 81, loss: 2.293585\n",
      "Epoch 82, loss: 2.292110\n",
      "Epoch 83, loss: 2.296915\n",
      "Epoch 84, loss: 2.294896\n",
      "Epoch 85, loss: 2.293404\n",
      "Epoch 86, loss: 2.293793\n",
      "Epoch 87, loss: 2.294391\n",
      "Epoch 88, loss: 2.293389\n",
      "Epoch 89, loss: 2.291826\n",
      "Epoch 90, loss: 2.294645\n",
      "Epoch 91, loss: 2.295927\n",
      "Epoch 92, loss: 2.294358\n",
      "Epoch 93, loss: 2.296052\n",
      "Epoch 94, loss: 2.292236\n",
      "Epoch 95, loss: 2.291441\n",
      "Epoch 96, loss: 2.291446\n",
      "Epoch 97, loss: 2.294882\n",
      "Epoch 98, loss: 2.292156\n",
      "Epoch 99, loss: 2.292015\n",
      "Epoch 100, loss: 2.291119\n",
      "Epoch 101, loss: 2.293082\n",
      "Epoch 102, loss: 2.295281\n",
      "Epoch 103, loss: 2.292317\n",
      "Epoch 104, loss: 2.294697\n",
      "Epoch 105, loss: 2.292426\n",
      "Epoch 106, loss: 2.293008\n",
      "Epoch 107, loss: 2.293159\n",
      "Epoch 108, loss: 2.290430\n",
      "Epoch 109, loss: 2.292611\n",
      "Epoch 110, loss: 2.290877\n",
      "Epoch 111, loss: 2.294340\n",
      "Epoch 112, loss: 2.292406\n",
      "Epoch 113, loss: 2.295884\n",
      "Epoch 114, loss: 2.293223\n",
      "Epoch 115, loss: 2.294275\n",
      "Epoch 116, loss: 2.291472\n",
      "Epoch 117, loss: 2.292918\n",
      "Epoch 118, loss: 2.293768\n",
      "Epoch 119, loss: 2.291357\n",
      "Epoch 120, loss: 2.292735\n",
      "Epoch 121, loss: 2.292079\n",
      "Epoch 122, loss: 2.292927\n",
      "Epoch 123, loss: 2.292377\n",
      "Epoch 124, loss: 2.296003\n",
      "Epoch 125, loss: 2.290338\n",
      "Epoch 126, loss: 2.288731\n",
      "Epoch 127, loss: 2.289406\n",
      "Epoch 128, loss: 2.291526\n",
      "Epoch 129, loss: 2.292221\n",
      "Epoch 130, loss: 2.291265\n",
      "Epoch 131, loss: 2.288621\n",
      "Epoch 132, loss: 2.289928\n",
      "Epoch 133, loss: 2.288713\n",
      "Epoch 134, loss: 2.289170\n",
      "Epoch 135, loss: 2.288470\n",
      "Epoch 136, loss: 2.289705\n",
      "Epoch 137, loss: 2.287714\n",
      "Epoch 138, loss: 2.287949\n",
      "Epoch 139, loss: 2.289532\n",
      "Epoch 140, loss: 2.290286\n",
      "Epoch 141, loss: 2.291338\n",
      "Epoch 142, loss: 2.291804\n",
      "Epoch 143, loss: 2.288298\n",
      "Epoch 144, loss: 2.287833\n",
      "Epoch 145, loss: 2.290302\n",
      "Epoch 146, loss: 2.289657\n",
      "Epoch 147, loss: 2.290933\n",
      "Epoch 148, loss: 2.296132\n",
      "Epoch 149, loss: 2.290553\n",
      "Epoch 150, loss: 2.289038\n",
      "Epoch 151, loss: 2.293790\n",
      "Epoch 152, loss: 2.290239\n",
      "Epoch 153, loss: 2.290231\n",
      "Epoch 154, loss: 2.290171\n",
      "Epoch 155, loss: 2.286362\n",
      "Epoch 156, loss: 2.288494\n",
      "Epoch 157, loss: 2.289112\n",
      "Epoch 158, loss: 2.289282\n",
      "Epoch 159, loss: 2.287886\n",
      "Epoch 160, loss: 2.286778\n",
      "Epoch 161, loss: 2.288319\n",
      "Epoch 162, loss: 2.291273\n",
      "Epoch 163, loss: 2.285820\n",
      "Epoch 164, loss: 2.286735\n",
      "Epoch 165, loss: 2.293167\n",
      "Epoch 166, loss: 2.284643\n",
      "Epoch 167, loss: 2.286861\n",
      "Epoch 168, loss: 2.284938\n",
      "Epoch 169, loss: 2.287792\n",
      "Epoch 170, loss: 2.284643\n",
      "Epoch 171, loss: 2.280903\n",
      "Epoch 172, loss: 2.287696\n",
      "Epoch 173, loss: 2.286195\n",
      "Epoch 174, loss: 2.287084\n",
      "Epoch 175, loss: 2.288303\n",
      "Epoch 176, loss: 2.286903\n",
      "Epoch 177, loss: 2.282587\n",
      "Epoch 178, loss: 2.285275\n",
      "Epoch 179, loss: 2.289271\n",
      "Epoch 180, loss: 2.283170\n",
      "Epoch 181, loss: 2.285293\n",
      "Epoch 182, loss: 2.286836\n",
      "Epoch 183, loss: 2.291135\n",
      "Epoch 184, loss: 2.285804\n",
      "Epoch 185, loss: 2.286916\n",
      "Epoch 186, loss: 2.285013\n",
      "Epoch 187, loss: 2.284903\n",
      "Epoch 188, loss: 2.284285\n",
      "Epoch 189, loss: 2.289602\n",
      "Epoch 190, loss: 2.289663\n",
      "Epoch 191, loss: 2.287841\n",
      "Epoch 192, loss: 2.289118\n",
      "Epoch 193, loss: 2.287660\n",
      "Epoch 194, loss: 2.287613\n",
      "Epoch 195, loss: 2.287599\n",
      "Epoch 196, loss: 2.285496\n",
      "Epoch 197, loss: 2.283548\n",
      "Epoch 198, loss: 2.284403\n",
      "Epoch 199, loss: 2.283972\n",
      "learning rate = 0.0001 reg = 0.0001 accuracy = 0.176\n",
      "Epoch 0, loss: 2.302239\n",
      "Epoch 1, loss: 2.301731\n",
      "Epoch 2, loss: 2.302222\n",
      "Epoch 3, loss: 2.301912\n",
      "Epoch 4, loss: 2.300778\n",
      "Epoch 5, loss: 2.300764\n",
      "Epoch 6, loss: 2.302057\n",
      "Epoch 7, loss: 2.302083\n",
      "Epoch 8, loss: 2.302034\n",
      "Epoch 9, loss: 2.301699\n",
      "Epoch 10, loss: 2.300543\n",
      "Epoch 11, loss: 2.300503\n",
      "Epoch 12, loss: 2.302495\n",
      "Epoch 13, loss: 2.301497\n",
      "Epoch 14, loss: 2.300565\n",
      "Epoch 15, loss: 2.300413\n",
      "Epoch 16, loss: 2.300414\n",
      "Epoch 17, loss: 2.300676\n",
      "Epoch 18, loss: 2.300198\n",
      "Epoch 19, loss: 2.299904\n",
      "Epoch 20, loss: 2.300518\n",
      "Epoch 21, loss: 2.298860\n",
      "Epoch 22, loss: 2.300453\n",
      "Epoch 23, loss: 2.301081\n",
      "Epoch 24, loss: 2.298940\n",
      "Epoch 25, loss: 2.298653\n",
      "Epoch 26, loss: 2.298838\n",
      "Epoch 27, loss: 2.301064\n",
      "Epoch 28, loss: 2.298043\n",
      "Epoch 29, loss: 2.298163\n",
      "Epoch 30, loss: 2.298115\n",
      "Epoch 31, loss: 2.298090\n",
      "Epoch 32, loss: 2.298793\n",
      "Epoch 33, loss: 2.297576\n",
      "Epoch 34, loss: 2.298926\n",
      "Epoch 35, loss: 2.299026\n",
      "Epoch 36, loss: 2.300779\n",
      "Epoch 37, loss: 2.298324\n",
      "Epoch 38, loss: 2.295656\n",
      "Epoch 39, loss: 2.300424\n",
      "Epoch 40, loss: 2.297856\n",
      "Epoch 41, loss: 2.297245\n",
      "Epoch 42, loss: 2.300078\n",
      "Epoch 43, loss: 2.295718\n",
      "Epoch 44, loss: 2.298905\n",
      "Epoch 45, loss: 2.299197\n",
      "Epoch 46, loss: 2.298438\n",
      "Epoch 47, loss: 2.296543\n",
      "Epoch 48, loss: 2.296185\n",
      "Epoch 49, loss: 2.298692\n",
      "Epoch 50, loss: 2.299578\n",
      "Epoch 51, loss: 2.300757\n",
      "Epoch 52, loss: 2.299150\n",
      "Epoch 53, loss: 2.297181\n",
      "Epoch 54, loss: 2.296610\n",
      "Epoch 55, loss: 2.298984\n",
      "Epoch 56, loss: 2.294886\n",
      "Epoch 57, loss: 2.295314\n",
      "Epoch 58, loss: 2.296690\n",
      "Epoch 59, loss: 2.293847\n",
      "Epoch 60, loss: 2.295450\n",
      "Epoch 61, loss: 2.294618\n",
      "Epoch 62, loss: 2.295832\n",
      "Epoch 63, loss: 2.296302\n",
      "Epoch 64, loss: 2.294370\n",
      "Epoch 65, loss: 2.297093\n",
      "Epoch 66, loss: 2.293200\n",
      "Epoch 67, loss: 2.298630\n",
      "Epoch 68, loss: 2.295740\n",
      "Epoch 69, loss: 2.297114\n",
      "Epoch 70, loss: 2.294713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, loss: 2.294215\n",
      "Epoch 72, loss: 2.293102\n",
      "Epoch 73, loss: 2.296149\n",
      "Epoch 74, loss: 2.297699\n",
      "Epoch 75, loss: 2.295264\n",
      "Epoch 76, loss: 2.293996\n",
      "Epoch 77, loss: 2.296413\n",
      "Epoch 78, loss: 2.298324\n",
      "Epoch 79, loss: 2.296959\n",
      "Epoch 80, loss: 2.292812\n",
      "Epoch 81, loss: 2.296075\n",
      "Epoch 82, loss: 2.294464\n",
      "Epoch 83, loss: 2.294934\n",
      "Epoch 84, loss: 2.292178\n",
      "Epoch 85, loss: 2.294034\n",
      "Epoch 86, loss: 2.295282\n",
      "Epoch 87, loss: 2.292548\n",
      "Epoch 88, loss: 2.291471\n",
      "Epoch 89, loss: 2.294210\n",
      "Epoch 90, loss: 2.295862\n",
      "Epoch 91, loss: 2.293826\n",
      "Epoch 92, loss: 2.293059\n",
      "Epoch 93, loss: 2.294631\n",
      "Epoch 94, loss: 2.294455\n",
      "Epoch 95, loss: 2.289946\n",
      "Epoch 96, loss: 2.292428\n",
      "Epoch 97, loss: 2.290872\n",
      "Epoch 98, loss: 2.291044\n",
      "Epoch 99, loss: 2.287889\n",
      "Epoch 100, loss: 2.293361\n",
      "Epoch 101, loss: 2.288932\n",
      "Epoch 102, loss: 2.291578\n",
      "Epoch 103, loss: 2.294850\n",
      "Epoch 104, loss: 2.294119\n",
      "Epoch 105, loss: 2.294086\n",
      "Epoch 106, loss: 2.295562\n",
      "Epoch 107, loss: 2.291234\n",
      "Epoch 108, loss: 2.289796\n",
      "Epoch 109, loss: 2.292338\n",
      "Epoch 110, loss: 2.292400\n",
      "Epoch 111, loss: 2.293801\n",
      "Epoch 112, loss: 2.295666\n",
      "Epoch 113, loss: 2.293576\n",
      "Epoch 114, loss: 2.293268\n",
      "Epoch 115, loss: 2.288909\n",
      "Epoch 116, loss: 2.294467\n",
      "Epoch 117, loss: 2.290228\n",
      "Epoch 118, loss: 2.291323\n",
      "Epoch 119, loss: 2.291813\n",
      "Epoch 120, loss: 2.287832\n",
      "Epoch 121, loss: 2.294245\n",
      "Epoch 122, loss: 2.291323\n",
      "Epoch 123, loss: 2.292076\n",
      "Epoch 124, loss: 2.290751\n",
      "Epoch 125, loss: 2.288619\n",
      "Epoch 126, loss: 2.289220\n",
      "Epoch 127, loss: 2.288230\n",
      "Epoch 128, loss: 2.289733\n",
      "Epoch 129, loss: 2.290869\n",
      "Epoch 130, loss: 2.288180\n",
      "Epoch 131, loss: 2.288095\n",
      "Epoch 132, loss: 2.292905\n",
      "Epoch 133, loss: 2.290710\n",
      "Epoch 134, loss: 2.291971\n",
      "Epoch 135, loss: 2.286100\n",
      "Epoch 136, loss: 2.288141\n",
      "Epoch 137, loss: 2.291472\n",
      "Epoch 138, loss: 2.291662\n",
      "Epoch 139, loss: 2.286436\n",
      "Epoch 140, loss: 2.290137\n",
      "Epoch 141, loss: 2.287795\n",
      "Epoch 142, loss: 2.289694\n",
      "Epoch 143, loss: 2.286817\n",
      "Epoch 144, loss: 2.287345\n",
      "Epoch 145, loss: 2.288729\n",
      "Epoch 146, loss: 2.288748\n",
      "Epoch 147, loss: 2.285673\n",
      "Epoch 148, loss: 2.290688\n",
      "Epoch 149, loss: 2.289919\n",
      "Epoch 150, loss: 2.286109\n",
      "Epoch 151, loss: 2.286536\n",
      "Epoch 152, loss: 2.291808\n",
      "Epoch 153, loss: 2.286554\n",
      "Epoch 154, loss: 2.292712\n",
      "Epoch 155, loss: 2.284982\n",
      "Epoch 156, loss: 2.287670\n",
      "Epoch 157, loss: 2.288540\n",
      "Epoch 158, loss: 2.285015\n",
      "Epoch 159, loss: 2.291461\n",
      "Epoch 160, loss: 2.283407\n",
      "Epoch 161, loss: 2.289745\n",
      "Epoch 162, loss: 2.287469\n",
      "Epoch 163, loss: 2.290705\n",
      "Epoch 164, loss: 2.283610\n",
      "Epoch 165, loss: 2.291708\n",
      "Epoch 166, loss: 2.290299\n",
      "Epoch 167, loss: 2.284730\n",
      "Epoch 168, loss: 2.282698\n",
      "Epoch 169, loss: 2.286629\n",
      "Epoch 170, loss: 2.289143\n",
      "Epoch 171, loss: 2.286640\n",
      "Epoch 172, loss: 2.292923\n",
      "Epoch 173, loss: 2.282830\n",
      "Epoch 174, loss: 2.286364\n",
      "Epoch 175, loss: 2.285273\n",
      "Epoch 176, loss: 2.285114\n",
      "Epoch 177, loss: 2.287412\n",
      "Epoch 178, loss: 2.289219\n",
      "Epoch 179, loss: 2.284433\n",
      "Epoch 180, loss: 2.288331\n",
      "Epoch 181, loss: 2.289725\n",
      "Epoch 182, loss: 2.288557\n",
      "Epoch 183, loss: 2.292768\n",
      "Epoch 184, loss: 2.281794\n",
      "Epoch 185, loss: 2.287066\n",
      "Epoch 186, loss: 2.284147\n",
      "Epoch 187, loss: 2.288308\n",
      "Epoch 188, loss: 2.289618\n",
      "Epoch 189, loss: 2.290272\n",
      "Epoch 190, loss: 2.282821\n",
      "Epoch 191, loss: 2.286305\n",
      "Epoch 192, loss: 2.287953\n",
      "Epoch 193, loss: 2.283441\n",
      "Epoch 194, loss: 2.282346\n",
      "Epoch 195, loss: 2.286937\n",
      "Epoch 196, loss: 2.285203\n",
      "Epoch 197, loss: 2.280104\n",
      "Epoch 198, loss: 2.288068\n",
      "Epoch 199, loss: 2.283398\n",
      "learning rate = 0.0001 reg = 1e-05 accuracy = 0.171\n",
      "Epoch 0, loss: 2.301277\n",
      "Epoch 1, loss: 2.302915\n",
      "Epoch 2, loss: 2.302252\n",
      "Epoch 3, loss: 2.302192\n",
      "Epoch 4, loss: 2.302392\n",
      "Epoch 5, loss: 2.302455\n",
      "Epoch 6, loss: 2.302006\n",
      "Epoch 7, loss: 2.301154\n",
      "Epoch 8, loss: 2.302504\n",
      "Epoch 9, loss: 2.301395\n",
      "Epoch 10, loss: 2.301391\n",
      "Epoch 11, loss: 2.300943\n",
      "Epoch 12, loss: 2.301909\n",
      "Epoch 13, loss: 2.301184\n",
      "Epoch 14, loss: 2.300422\n",
      "Epoch 15, loss: 2.301756\n",
      "Epoch 16, loss: 2.301093\n",
      "Epoch 17, loss: 2.300137\n",
      "Epoch 18, loss: 2.300853\n",
      "Epoch 19, loss: 2.299736\n",
      "Epoch 20, loss: 2.300277\n",
      "Epoch 21, loss: 2.299551\n",
      "Epoch 22, loss: 2.300679\n",
      "Epoch 23, loss: 2.301065\n",
      "Epoch 24, loss: 2.300087\n",
      "Epoch 25, loss: 2.299972\n",
      "Epoch 26, loss: 2.299093\n",
      "Epoch 27, loss: 2.299386\n",
      "Epoch 28, loss: 2.300744\n",
      "Epoch 29, loss: 2.299221\n",
      "Epoch 30, loss: 2.298550\n",
      "Epoch 31, loss: 2.299138\n",
      "Epoch 32, loss: 2.299388\n",
      "Epoch 33, loss: 2.297908\n",
      "Epoch 34, loss: 2.299704\n",
      "Epoch 35, loss: 2.299600\n",
      "Epoch 36, loss: 2.296523\n",
      "Epoch 37, loss: 2.300484\n",
      "Epoch 38, loss: 2.297651\n",
      "Epoch 39, loss: 2.300963\n",
      "Epoch 40, loss: 2.298086\n",
      "Epoch 41, loss: 2.297047\n",
      "Epoch 42, loss: 2.296350\n",
      "Epoch 43, loss: 2.299050\n",
      "Epoch 44, loss: 2.297667\n",
      "Epoch 45, loss: 2.298840\n",
      "Epoch 46, loss: 2.297928\n",
      "Epoch 47, loss: 2.300228\n",
      "Epoch 48, loss: 2.297779\n",
      "Epoch 49, loss: 2.295915\n",
      "Epoch 50, loss: 2.297747\n",
      "Epoch 51, loss: 2.298835\n",
      "Epoch 52, loss: 2.298352\n",
      "Epoch 53, loss: 2.295934\n",
      "Epoch 54, loss: 2.297274\n",
      "Epoch 55, loss: 2.297710\n",
      "Epoch 56, loss: 2.297445\n",
      "Epoch 57, loss: 2.296271\n",
      "Epoch 58, loss: 2.297935\n",
      "Epoch 59, loss: 2.296502\n",
      "Epoch 60, loss: 2.295295\n",
      "Epoch 61, loss: 2.296903\n",
      "Epoch 62, loss: 2.296552\n",
      "Epoch 63, loss: 2.296419\n",
      "Epoch 64, loss: 2.296378\n",
      "Epoch 65, loss: 2.294566\n",
      "Epoch 66, loss: 2.294565\n",
      "Epoch 67, loss: 2.298535\n",
      "Epoch 68, loss: 2.296267\n",
      "Epoch 69, loss: 2.296257\n",
      "Epoch 70, loss: 2.298628\n",
      "Epoch 71, loss: 2.295805\n",
      "Epoch 72, loss: 2.294733\n",
      "Epoch 73, loss: 2.294116\n",
      "Epoch 74, loss: 2.294227\n",
      "Epoch 75, loss: 2.292248\n",
      "Epoch 76, loss: 2.295170\n",
      "Epoch 77, loss: 2.297189\n",
      "Epoch 78, loss: 2.293552\n",
      "Epoch 79, loss: 2.294382\n",
      "Epoch 80, loss: 2.295323\n",
      "Epoch 81, loss: 2.294005\n",
      "Epoch 82, loss: 2.295999\n",
      "Epoch 83, loss: 2.295244\n",
      "Epoch 84, loss: 2.293501\n",
      "Epoch 85, loss: 2.296838\n",
      "Epoch 86, loss: 2.290240\n",
      "Epoch 87, loss: 2.293435\n",
      "Epoch 88, loss: 2.293266\n",
      "Epoch 89, loss: 2.292313\n",
      "Epoch 90, loss: 2.291542\n",
      "Epoch 91, loss: 2.289565\n",
      "Epoch 92, loss: 2.295125\n",
      "Epoch 93, loss: 2.294293\n",
      "Epoch 94, loss: 2.292931\n",
      "Epoch 95, loss: 2.291946\n",
      "Epoch 96, loss: 2.293825\n",
      "Epoch 97, loss: 2.290055\n",
      "Epoch 98, loss: 2.294934\n",
      "Epoch 99, loss: 2.292629\n",
      "Epoch 100, loss: 2.298121\n",
      "Epoch 101, loss: 2.293222\n",
      "Epoch 102, loss: 2.291775\n",
      "Epoch 103, loss: 2.294197\n",
      "Epoch 104, loss: 2.294261\n",
      "Epoch 105, loss: 2.292527\n",
      "Epoch 106, loss: 2.289530\n",
      "Epoch 107, loss: 2.294120\n",
      "Epoch 108, loss: 2.295575\n",
      "Epoch 109, loss: 2.291130\n",
      "Epoch 110, loss: 2.294211\n",
      "Epoch 111, loss: 2.293089\n",
      "Epoch 112, loss: 2.292095\n",
      "Epoch 113, loss: 2.294203\n",
      "Epoch 114, loss: 2.290829\n",
      "Epoch 115, loss: 2.293518\n",
      "Epoch 116, loss: 2.288950\n",
      "Epoch 117, loss: 2.293583\n",
      "Epoch 118, loss: 2.291347\n",
      "Epoch 119, loss: 2.294537\n",
      "Epoch 120, loss: 2.286874\n",
      "Epoch 121, loss: 2.289930\n",
      "Epoch 122, loss: 2.289947\n",
      "Epoch 123, loss: 2.289407\n",
      "Epoch 124, loss: 2.293229\n",
      "Epoch 125, loss: 2.293760\n",
      "Epoch 126, loss: 2.290182\n",
      "Epoch 127, loss: 2.287753\n",
      "Epoch 128, loss: 2.288529\n",
      "Epoch 129, loss: 2.292084\n",
      "Epoch 130, loss: 2.292918\n",
      "Epoch 131, loss: 2.290704\n",
      "Epoch 132, loss: 2.293174\n",
      "Epoch 133, loss: 2.289024\n",
      "Epoch 134, loss: 2.289466\n",
      "Epoch 135, loss: 2.290394\n",
      "Epoch 136, loss: 2.289546\n",
      "Epoch 137, loss: 2.288962\n",
      "Epoch 138, loss: 2.289651\n",
      "Epoch 139, loss: 2.288903\n",
      "Epoch 140, loss: 2.288834\n",
      "Epoch 141, loss: 2.287645\n",
      "Epoch 142, loss: 2.294425\n",
      "Epoch 143, loss: 2.291232\n",
      "Epoch 144, loss: 2.292089\n",
      "Epoch 145, loss: 2.286118\n",
      "Epoch 146, loss: 2.290037\n",
      "Epoch 147, loss: 2.287228\n",
      "Epoch 148, loss: 2.292214\n",
      "Epoch 149, loss: 2.290163\n",
      "Epoch 150, loss: 2.287831\n",
      "Epoch 151, loss: 2.289544\n",
      "Epoch 152, loss: 2.290441\n",
      "Epoch 153, loss: 2.291425\n",
      "Epoch 154, loss: 2.288849\n",
      "Epoch 155, loss: 2.287935\n",
      "Epoch 156, loss: 2.286641\n",
      "Epoch 157, loss: 2.287135\n",
      "Epoch 158, loss: 2.282425\n",
      "Epoch 159, loss: 2.286918\n",
      "Epoch 160, loss: 2.284796\n",
      "Epoch 161, loss: 2.287570\n",
      "Epoch 162, loss: 2.286756\n",
      "Epoch 163, loss: 2.283230\n",
      "Epoch 164, loss: 2.288077\n",
      "Epoch 165, loss: 2.289031\n",
      "Epoch 166, loss: 2.287290\n",
      "Epoch 167, loss: 2.290941\n",
      "Epoch 168, loss: 2.275041\n",
      "Epoch 169, loss: 2.292502\n",
      "Epoch 170, loss: 2.288573\n",
      "Epoch 171, loss: 2.288512\n",
      "Epoch 172, loss: 2.286046\n",
      "Epoch 173, loss: 2.286341\n",
      "Epoch 174, loss: 2.287343\n",
      "Epoch 175, loss: 2.286231\n",
      "Epoch 176, loss: 2.290824\n",
      "Epoch 177, loss: 2.288464\n",
      "Epoch 178, loss: 2.285094\n",
      "Epoch 179, loss: 2.289301\n",
      "Epoch 180, loss: 2.284758\n",
      "Epoch 181, loss: 2.286389\n",
      "Epoch 182, loss: 2.281142\n",
      "Epoch 183, loss: 2.288191\n",
      "Epoch 184, loss: 2.287798\n",
      "Epoch 185, loss: 2.292332\n",
      "Epoch 186, loss: 2.286986\n",
      "Epoch 187, loss: 2.288255\n",
      "Epoch 188, loss: 2.288488\n",
      "Epoch 189, loss: 2.285969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190, loss: 2.280045\n",
      "Epoch 191, loss: 2.284540\n",
      "Epoch 192, loss: 2.292176\n",
      "Epoch 193, loss: 2.286534\n",
      "Epoch 194, loss: 2.286227\n",
      "Epoch 195, loss: 2.280396\n",
      "Epoch 196, loss: 2.286643\n",
      "Epoch 197, loss: 2.280135\n",
      "Epoch 198, loss: 2.286603\n",
      "Epoch 199, loss: 2.288792\n",
      "learning rate = 0.0001 reg = 1e-06 accuracy = 0.176\n",
      "Epoch 0, loss: 2.609948\n",
      "Epoch 1, loss: 2.605079\n",
      "Epoch 2, loss: 2.602374\n",
      "Epoch 3, loss: 2.599429\n",
      "Epoch 4, loss: 2.595535\n",
      "Epoch 5, loss: 2.592375\n",
      "Epoch 6, loss: 2.588975\n",
      "Epoch 7, loss: 2.584147\n",
      "Epoch 8, loss: 2.581975\n",
      "Epoch 9, loss: 2.577969\n",
      "Epoch 10, loss: 2.575053\n",
      "Epoch 11, loss: 2.571648\n",
      "Epoch 12, loss: 2.567860\n",
      "Epoch 13, loss: 2.565876\n",
      "Epoch 14, loss: 2.562403\n",
      "Epoch 15, loss: 2.559502\n",
      "Epoch 16, loss: 2.555753\n",
      "Epoch 17, loss: 2.552234\n",
      "Epoch 18, loss: 2.549575\n",
      "Epoch 19, loss: 2.546776\n",
      "Epoch 20, loss: 2.544583\n",
      "Epoch 21, loss: 2.541165\n",
      "Epoch 22, loss: 2.538875\n",
      "Epoch 23, loss: 2.535650\n",
      "Epoch 24, loss: 2.532693\n",
      "Epoch 25, loss: 2.529618\n",
      "Epoch 26, loss: 2.527985\n",
      "Epoch 27, loss: 2.524135\n",
      "Epoch 28, loss: 2.521381\n",
      "Epoch 29, loss: 2.519190\n",
      "Epoch 30, loss: 2.516666\n",
      "Epoch 31, loss: 2.512908\n",
      "Epoch 32, loss: 2.510714\n",
      "Epoch 33, loss: 2.509053\n",
      "Epoch 34, loss: 2.506125\n",
      "Epoch 35, loss: 2.503580\n",
      "Epoch 36, loss: 2.501083\n",
      "Epoch 37, loss: 2.499562\n",
      "Epoch 38, loss: 2.497011\n",
      "Epoch 39, loss: 2.494304\n",
      "Epoch 40, loss: 2.492739\n",
      "Epoch 41, loss: 2.490350\n",
      "Epoch 42, loss: 2.488450\n",
      "Epoch 43, loss: 2.485346\n",
      "Epoch 44, loss: 2.483352\n",
      "Epoch 45, loss: 2.481609\n",
      "Epoch 46, loss: 2.479343\n",
      "Epoch 47, loss: 2.477092\n",
      "Epoch 48, loss: 2.474845\n",
      "Epoch 49, loss: 2.472256\n",
      "Epoch 50, loss: 2.470287\n",
      "Epoch 51, loss: 2.468928\n",
      "Epoch 52, loss: 2.466158\n",
      "Epoch 53, loss: 2.464773\n",
      "Epoch 54, loss: 2.462389\n",
      "Epoch 55, loss: 2.461096\n",
      "Epoch 56, loss: 2.458898\n",
      "Epoch 57, loss: 2.456901\n",
      "Epoch 58, loss: 2.454064\n",
      "Epoch 59, loss: 2.453527\n",
      "Epoch 60, loss: 2.451567\n",
      "Epoch 61, loss: 2.450377\n",
      "Epoch 62, loss: 2.447219\n",
      "Epoch 63, loss: 2.446097\n",
      "Epoch 64, loss: 2.444547\n",
      "Epoch 65, loss: 2.442617\n",
      "Epoch 66, loss: 2.442050\n",
      "Epoch 67, loss: 2.438838\n",
      "Epoch 68, loss: 2.437453\n",
      "Epoch 69, loss: 2.436625\n",
      "Epoch 70, loss: 2.434750\n",
      "Epoch 71, loss: 2.433528\n",
      "Epoch 72, loss: 2.431498\n",
      "Epoch 73, loss: 2.429989\n",
      "Epoch 74, loss: 2.428524\n",
      "Epoch 75, loss: 2.427226\n",
      "Epoch 76, loss: 2.425885\n",
      "Epoch 77, loss: 2.424180\n",
      "Epoch 78, loss: 2.422743\n",
      "Epoch 79, loss: 2.421342\n",
      "Epoch 80, loss: 2.419163\n",
      "Epoch 81, loss: 2.418024\n",
      "Epoch 82, loss: 2.416657\n",
      "Epoch 83, loss: 2.416263\n",
      "Epoch 84, loss: 2.413909\n",
      "Epoch 85, loss: 2.412486\n",
      "Epoch 86, loss: 2.411914\n",
      "Epoch 87, loss: 2.410170\n",
      "Epoch 88, loss: 2.408754\n",
      "Epoch 89, loss: 2.407998\n",
      "Epoch 90, loss: 2.406612\n",
      "Epoch 91, loss: 2.404933\n",
      "Epoch 92, loss: 2.404115\n",
      "Epoch 93, loss: 2.402662\n",
      "Epoch 94, loss: 2.401582\n",
      "Epoch 95, loss: 2.400277\n",
      "Epoch 96, loss: 2.398969\n",
      "Epoch 97, loss: 2.398099\n",
      "Epoch 98, loss: 2.396216\n",
      "Epoch 99, loss: 2.395722\n",
      "Epoch 100, loss: 2.394458\n",
      "Epoch 101, loss: 2.392595\n",
      "Epoch 102, loss: 2.391465\n",
      "Epoch 103, loss: 2.390809\n",
      "Epoch 104, loss: 2.389536\n",
      "Epoch 105, loss: 2.389010\n",
      "Epoch 106, loss: 2.388431\n",
      "Epoch 107, loss: 2.387248\n",
      "Epoch 108, loss: 2.385748\n",
      "Epoch 109, loss: 2.385471\n",
      "Epoch 110, loss: 2.384300\n",
      "Epoch 111, loss: 2.383077\n",
      "Epoch 112, loss: 2.381676\n",
      "Epoch 113, loss: 2.380999\n",
      "Epoch 114, loss: 2.379544\n",
      "Epoch 115, loss: 2.378783\n",
      "Epoch 116, loss: 2.378467\n",
      "Epoch 117, loss: 2.377277\n",
      "Epoch 118, loss: 2.376370\n",
      "Epoch 119, loss: 2.375846\n",
      "Epoch 120, loss: 2.374915\n",
      "Epoch 121, loss: 2.374142\n",
      "Epoch 122, loss: 2.372773\n",
      "Epoch 123, loss: 2.372288\n",
      "Epoch 124, loss: 2.371560\n",
      "Epoch 125, loss: 2.370656\n",
      "Epoch 126, loss: 2.369303\n",
      "Epoch 127, loss: 2.368825\n",
      "Epoch 128, loss: 2.367769\n",
      "Epoch 129, loss: 2.367452\n",
      "Epoch 130, loss: 2.366592\n",
      "Epoch 131, loss: 2.366360\n",
      "Epoch 132, loss: 2.365414\n",
      "Epoch 133, loss: 2.364444\n",
      "Epoch 134, loss: 2.363389\n",
      "Epoch 135, loss: 2.362559\n",
      "Epoch 136, loss: 2.361851\n",
      "Epoch 137, loss: 2.361478\n",
      "Epoch 138, loss: 2.361150\n",
      "Epoch 139, loss: 2.360207\n",
      "Epoch 140, loss: 2.359412\n",
      "Epoch 141, loss: 2.358162\n",
      "Epoch 142, loss: 2.358230\n",
      "Epoch 143, loss: 2.357332\n",
      "Epoch 144, loss: 2.355966\n",
      "Epoch 145, loss: 2.356033\n",
      "Epoch 146, loss: 2.355258\n",
      "Epoch 147, loss: 2.354474\n",
      "Epoch 148, loss: 2.353848\n",
      "Epoch 149, loss: 2.353299\n",
      "Epoch 150, loss: 2.351765\n",
      "Epoch 151, loss: 2.352150\n",
      "Epoch 152, loss: 2.351237\n",
      "Epoch 153, loss: 2.350728\n",
      "Epoch 154, loss: 2.350059\n",
      "Epoch 155, loss: 2.349912\n",
      "Epoch 156, loss: 2.348957\n",
      "Epoch 157, loss: 2.348318\n",
      "Epoch 158, loss: 2.348070\n",
      "Epoch 159, loss: 2.346790\n",
      "Epoch 160, loss: 2.347332\n",
      "Epoch 161, loss: 2.346421\n",
      "Epoch 162, loss: 2.345628\n",
      "Epoch 163, loss: 2.345267\n",
      "Epoch 164, loss: 2.344056\n",
      "Epoch 165, loss: 2.343532\n",
      "Epoch 166, loss: 2.343273\n",
      "Epoch 167, loss: 2.342470\n",
      "Epoch 168, loss: 2.342463\n",
      "Epoch 169, loss: 2.342160\n",
      "Epoch 170, loss: 2.342714\n",
      "Epoch 171, loss: 2.341440\n",
      "Epoch 172, loss: 2.340894\n",
      "Epoch 173, loss: 2.341028\n",
      "Epoch 174, loss: 2.339492\n",
      "Epoch 175, loss: 2.339326\n",
      "Epoch 176, loss: 2.338702\n",
      "Epoch 177, loss: 2.338179\n",
      "Epoch 178, loss: 2.337700\n",
      "Epoch 179, loss: 2.337431\n",
      "Epoch 180, loss: 2.337805\n",
      "Epoch 181, loss: 2.336820\n",
      "Epoch 182, loss: 2.336264\n",
      "Epoch 183, loss: 2.335845\n",
      "Epoch 184, loss: 2.335685\n",
      "Epoch 185, loss: 2.335282\n",
      "Epoch 186, loss: 2.334553\n",
      "Epoch 187, loss: 2.334782\n",
      "Epoch 188, loss: 2.334089\n",
      "Epoch 189, loss: 2.333690\n",
      "Epoch 190, loss: 2.332911\n",
      "Epoch 191, loss: 2.333023\n",
      "Epoch 192, loss: 2.333321\n",
      "Epoch 193, loss: 2.332224\n",
      "Epoch 194, loss: 2.331491\n",
      "Epoch 195, loss: 2.330936\n",
      "Epoch 196, loss: 2.331051\n",
      "Epoch 197, loss: 2.330673\n",
      "Epoch 198, loss: 2.330240\n",
      "Epoch 199, loss: 2.329329\n",
      "learning rate = 1e-05 reg = 10 accuracy = 0.147\n",
      "Epoch 0, loss: 2.332628\n",
      "Epoch 1, loss: 2.332923\n",
      "Epoch 2, loss: 2.333059\n",
      "Epoch 3, loss: 2.332866\n",
      "Epoch 4, loss: 2.332926\n",
      "Epoch 5, loss: 2.332559\n",
      "Epoch 6, loss: 2.332429\n",
      "Epoch 7, loss: 2.332386\n",
      "Epoch 8, loss: 2.331953\n",
      "Epoch 9, loss: 2.332888\n",
      "Epoch 10, loss: 2.332079\n",
      "Epoch 11, loss: 2.332977\n",
      "Epoch 12, loss: 2.331973\n",
      "Epoch 13, loss: 2.332141\n",
      "Epoch 14, loss: 2.331986\n",
      "Epoch 15, loss: 2.332798\n",
      "Epoch 16, loss: 2.332332\n",
      "Epoch 17, loss: 2.331818\n",
      "Epoch 18, loss: 2.331535\n",
      "Epoch 19, loss: 2.332595\n",
      "Epoch 20, loss: 2.332242\n",
      "Epoch 21, loss: 2.331220\n",
      "Epoch 22, loss: 2.332399\n",
      "Epoch 23, loss: 2.331019\n",
      "Epoch 24, loss: 2.332068\n",
      "Epoch 25, loss: 2.331601\n",
      "Epoch 26, loss: 2.330971\n",
      "Epoch 27, loss: 2.332315\n",
      "Epoch 28, loss: 2.331337\n",
      "Epoch 29, loss: 2.332090\n",
      "Epoch 30, loss: 2.331505\n",
      "Epoch 31, loss: 2.330080\n",
      "Epoch 32, loss: 2.330235\n",
      "Epoch 33, loss: 2.331361\n",
      "Epoch 34, loss: 2.333027\n",
      "Epoch 35, loss: 2.331709\n",
      "Epoch 36, loss: 2.331249\n",
      "Epoch 37, loss: 2.330668\n",
      "Epoch 38, loss: 2.331769\n",
      "Epoch 39, loss: 2.331760\n",
      "Epoch 40, loss: 2.331567\n",
      "Epoch 41, loss: 2.330519\n",
      "Epoch 42, loss: 2.331245\n",
      "Epoch 43, loss: 2.331129\n",
      "Epoch 44, loss: 2.330807\n",
      "Epoch 45, loss: 2.331473\n",
      "Epoch 46, loss: 2.330574\n",
      "Epoch 47, loss: 2.331280\n",
      "Epoch 48, loss: 2.330464\n",
      "Epoch 49, loss: 2.330767\n",
      "Epoch 50, loss: 2.331777\n",
      "Epoch 51, loss: 2.330897\n",
      "Epoch 52, loss: 2.331501\n",
      "Epoch 53, loss: 2.330789\n",
      "Epoch 54, loss: 2.331066\n",
      "Epoch 55, loss: 2.330638\n",
      "Epoch 56, loss: 2.330105\n",
      "Epoch 57, loss: 2.331185\n",
      "Epoch 58, loss: 2.330246\n",
      "Epoch 59, loss: 2.330567\n",
      "Epoch 60, loss: 2.329986\n",
      "Epoch 61, loss: 2.330939\n",
      "Epoch 62, loss: 2.330165\n",
      "Epoch 63, loss: 2.330404\n",
      "Epoch 64, loss: 2.330040\n",
      "Epoch 65, loss: 2.329490\n",
      "Epoch 66, loss: 2.329759\n",
      "Epoch 67, loss: 2.329986\n",
      "Epoch 68, loss: 2.330564\n",
      "Epoch 69, loss: 2.329839\n",
      "Epoch 70, loss: 2.329143\n",
      "Epoch 71, loss: 2.329098\n",
      "Epoch 72, loss: 2.329839\n",
      "Epoch 73, loss: 2.330378\n",
      "Epoch 74, loss: 2.329057\n",
      "Epoch 75, loss: 2.330294\n",
      "Epoch 76, loss: 2.329164\n",
      "Epoch 77, loss: 2.330139\n",
      "Epoch 78, loss: 2.330254\n",
      "Epoch 79, loss: 2.329254\n",
      "Epoch 80, loss: 2.328729\n",
      "Epoch 81, loss: 2.328305\n",
      "Epoch 82, loss: 2.329890\n",
      "Epoch 83, loss: 2.330471\n",
      "Epoch 84, loss: 2.328387\n",
      "Epoch 85, loss: 2.328367\n",
      "Epoch 86, loss: 2.328797\n",
      "Epoch 87, loss: 2.328377\n",
      "Epoch 88, loss: 2.329429\n",
      "Epoch 89, loss: 2.329095\n",
      "Epoch 90, loss: 2.329451\n",
      "Epoch 91, loss: 2.328949\n",
      "Epoch 92, loss: 2.328573\n",
      "Epoch 93, loss: 2.328402\n",
      "Epoch 94, loss: 2.328588\n",
      "Epoch 95, loss: 2.329178\n",
      "Epoch 96, loss: 2.327823\n",
      "Epoch 97, loss: 2.329252\n",
      "Epoch 98, loss: 2.328887\n",
      "Epoch 99, loss: 2.329457\n",
      "Epoch 100, loss: 2.328300\n",
      "Epoch 101, loss: 2.328391\n",
      "Epoch 102, loss: 2.327930\n",
      "Epoch 103, loss: 2.328631\n",
      "Epoch 104, loss: 2.328797\n",
      "Epoch 105, loss: 2.328412\n",
      "Epoch 106, loss: 2.327911\n",
      "Epoch 107, loss: 2.327178\n",
      "Epoch 108, loss: 2.328168\n",
      "Epoch 109, loss: 2.327165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110, loss: 2.327752\n",
      "Epoch 111, loss: 2.327241\n",
      "Epoch 112, loss: 2.328034\n",
      "Epoch 113, loss: 2.326805\n",
      "Epoch 114, loss: 2.327933\n",
      "Epoch 115, loss: 2.327255\n",
      "Epoch 116, loss: 2.327170\n",
      "Epoch 117, loss: 2.328984\n",
      "Epoch 118, loss: 2.328072\n",
      "Epoch 119, loss: 2.328487\n",
      "Epoch 120, loss: 2.327821\n",
      "Epoch 121, loss: 2.327875\n",
      "Epoch 122, loss: 2.327596\n",
      "Epoch 123, loss: 2.327471\n",
      "Epoch 124, loss: 2.328870\n",
      "Epoch 125, loss: 2.328421\n",
      "Epoch 126, loss: 2.326702\n",
      "Epoch 127, loss: 2.327496\n",
      "Epoch 128, loss: 2.327262\n",
      "Epoch 129, loss: 2.326951\n",
      "Epoch 130, loss: 2.326709\n",
      "Epoch 131, loss: 2.327457\n",
      "Epoch 132, loss: 2.328125\n",
      "Epoch 133, loss: 2.327116\n",
      "Epoch 134, loss: 2.327107\n",
      "Epoch 135, loss: 2.327002\n",
      "Epoch 136, loss: 2.326833\n",
      "Epoch 137, loss: 2.327208\n",
      "Epoch 138, loss: 2.326674\n",
      "Epoch 139, loss: 2.327228\n",
      "Epoch 140, loss: 2.327779\n",
      "Epoch 141, loss: 2.327243\n",
      "Epoch 142, loss: 2.325735\n",
      "Epoch 143, loss: 2.326807\n",
      "Epoch 144, loss: 2.326746\n",
      "Epoch 145, loss: 2.326050\n",
      "Epoch 146, loss: 2.325734\n",
      "Epoch 147, loss: 2.326810\n",
      "Epoch 148, loss: 2.327233\n",
      "Epoch 149, loss: 2.326238\n",
      "Epoch 150, loss: 2.326225\n",
      "Epoch 151, loss: 2.326402\n",
      "Epoch 152, loss: 2.326894\n",
      "Epoch 153, loss: 2.326689\n",
      "Epoch 154, loss: 2.325798\n",
      "Epoch 155, loss: 2.325958\n",
      "Epoch 156, loss: 2.327431\n",
      "Epoch 157, loss: 2.326250\n",
      "Epoch 158, loss: 2.326722\n",
      "Epoch 159, loss: 2.326794\n",
      "Epoch 160, loss: 2.326166\n",
      "Epoch 161, loss: 2.325338\n",
      "Epoch 162, loss: 2.326378\n",
      "Epoch 163, loss: 2.326032\n",
      "Epoch 164, loss: 2.326287\n",
      "Epoch 165, loss: 2.326661\n",
      "Epoch 166, loss: 2.326821\n",
      "Epoch 167, loss: 2.325896\n",
      "Epoch 168, loss: 2.326981\n",
      "Epoch 169, loss: 2.326702\n",
      "Epoch 170, loss: 2.325670\n",
      "Epoch 171, loss: 2.324716\n",
      "Epoch 172, loss: 2.325537\n",
      "Epoch 173, loss: 2.325509\n",
      "Epoch 174, loss: 2.325197\n",
      "Epoch 175, loss: 2.325241\n",
      "Epoch 176, loss: 2.326495\n",
      "Epoch 177, loss: 2.324058\n",
      "Epoch 178, loss: 2.325006\n",
      "Epoch 179, loss: 2.324688\n",
      "Epoch 180, loss: 2.326050\n",
      "Epoch 181, loss: 2.324772\n",
      "Epoch 182, loss: 2.325243\n",
      "Epoch 183, loss: 2.324478\n",
      "Epoch 184, loss: 2.325576\n",
      "Epoch 185, loss: 2.324982\n",
      "Epoch 186, loss: 2.324471\n",
      "Epoch 187, loss: 2.325679\n",
      "Epoch 188, loss: 2.324935\n",
      "Epoch 189, loss: 2.324275\n",
      "Epoch 190, loss: 2.323882\n",
      "Epoch 191, loss: 2.325700\n",
      "Epoch 192, loss: 2.325008\n",
      "Epoch 193, loss: 2.323653\n",
      "Epoch 194, loss: 2.325416\n",
      "Epoch 195, loss: 2.324075\n",
      "Epoch 196, loss: 2.325451\n",
      "Epoch 197, loss: 2.325745\n",
      "Epoch 198, loss: 2.325247\n",
      "Epoch 199, loss: 2.325463\n",
      "learning rate = 1e-05 reg = 1 accuracy = 0.113\n",
      "Epoch 0, loss: 2.305814\n",
      "Epoch 1, loss: 2.305789\n",
      "Epoch 2, loss: 2.306639\n",
      "Epoch 3, loss: 2.305342\n",
      "Epoch 4, loss: 2.305385\n",
      "Epoch 5, loss: 2.305209\n",
      "Epoch 6, loss: 2.305916\n",
      "Epoch 7, loss: 2.305752\n",
      "Epoch 8, loss: 2.305483\n",
      "Epoch 9, loss: 2.305861\n",
      "Epoch 10, loss: 2.306658\n",
      "Epoch 11, loss: 2.306266\n",
      "Epoch 12, loss: 2.304914\n",
      "Epoch 13, loss: 2.305584\n",
      "Epoch 14, loss: 2.305177\n",
      "Epoch 15, loss: 2.305499\n",
      "Epoch 16, loss: 2.305272\n",
      "Epoch 17, loss: 2.305582\n",
      "Epoch 18, loss: 2.305618\n",
      "Epoch 19, loss: 2.305155\n",
      "Epoch 20, loss: 2.305777\n",
      "Epoch 21, loss: 2.305677\n",
      "Epoch 22, loss: 2.304467\n",
      "Epoch 23, loss: 2.306565\n",
      "Epoch 24, loss: 2.305759\n",
      "Epoch 25, loss: 2.306534\n",
      "Epoch 26, loss: 2.305653\n",
      "Epoch 27, loss: 2.305474\n",
      "Epoch 28, loss: 2.304050\n",
      "Epoch 29, loss: 2.305298\n",
      "Epoch 30, loss: 2.305395\n",
      "Epoch 31, loss: 2.304873\n",
      "Epoch 32, loss: 2.305051\n",
      "Epoch 33, loss: 2.304750\n",
      "Epoch 34, loss: 2.304850\n",
      "Epoch 35, loss: 2.305297\n",
      "Epoch 36, loss: 2.305506\n",
      "Epoch 37, loss: 2.305420\n",
      "Epoch 38, loss: 2.305819\n",
      "Epoch 39, loss: 2.304747\n",
      "Epoch 40, loss: 2.305612\n",
      "Epoch 41, loss: 2.304690\n",
      "Epoch 42, loss: 2.305236\n",
      "Epoch 43, loss: 2.304727\n",
      "Epoch 44, loss: 2.305008\n",
      "Epoch 45, loss: 2.305229\n",
      "Epoch 46, loss: 2.304444\n",
      "Epoch 47, loss: 2.305753\n",
      "Epoch 48, loss: 2.305485\n",
      "Epoch 49, loss: 2.305229\n",
      "Epoch 50, loss: 2.305460\n",
      "Epoch 51, loss: 2.305528\n",
      "Epoch 52, loss: 2.304390\n",
      "Epoch 53, loss: 2.304340\n",
      "Epoch 54, loss: 2.304752\n",
      "Epoch 55, loss: 2.304723\n",
      "Epoch 56, loss: 2.305623\n",
      "Epoch 57, loss: 2.305671\n",
      "Epoch 58, loss: 2.305207\n",
      "Epoch 59, loss: 2.304671\n",
      "Epoch 60, loss: 2.304501\n",
      "Epoch 61, loss: 2.304820\n",
      "Epoch 62, loss: 2.305178\n",
      "Epoch 63, loss: 2.304658\n",
      "Epoch 64, loss: 2.305200\n",
      "Epoch 65, loss: 2.305467\n",
      "Epoch 66, loss: 2.305084\n",
      "Epoch 67, loss: 2.305212\n",
      "Epoch 68, loss: 2.306547\n",
      "Epoch 69, loss: 2.305142\n",
      "Epoch 70, loss: 2.305185\n",
      "Epoch 71, loss: 2.303912\n",
      "Epoch 72, loss: 2.304416\n",
      "Epoch 73, loss: 2.306161\n",
      "Epoch 74, loss: 2.304616\n",
      "Epoch 75, loss: 2.304652\n",
      "Epoch 76, loss: 2.305016\n",
      "Epoch 77, loss: 2.303978\n",
      "Epoch 78, loss: 2.305136\n",
      "Epoch 79, loss: 2.304682\n",
      "Epoch 80, loss: 2.304011\n",
      "Epoch 81, loss: 2.305515\n",
      "Epoch 82, loss: 2.304849\n",
      "Epoch 83, loss: 2.304901\n",
      "Epoch 84, loss: 2.306530\n",
      "Epoch 85, loss: 2.305005\n",
      "Epoch 86, loss: 2.304605\n",
      "Epoch 87, loss: 2.304773\n",
      "Epoch 88, loss: 2.305096\n",
      "Epoch 89, loss: 2.304693\n",
      "Epoch 90, loss: 2.305673\n",
      "Epoch 91, loss: 2.305033\n",
      "Epoch 92, loss: 2.305075\n",
      "Epoch 93, loss: 2.305219\n",
      "Epoch 94, loss: 2.305107\n",
      "Epoch 95, loss: 2.305166\n",
      "Epoch 96, loss: 2.304645\n",
      "Epoch 97, loss: 2.304468\n",
      "Epoch 98, loss: 2.305103\n",
      "Epoch 99, loss: 2.303486\n",
      "Epoch 100, loss: 2.305569\n",
      "Epoch 101, loss: 2.305296\n",
      "Epoch 102, loss: 2.304103\n",
      "Epoch 103, loss: 2.304423\n",
      "Epoch 104, loss: 2.304043\n",
      "Epoch 105, loss: 2.305218\n",
      "Epoch 106, loss: 2.305039\n",
      "Epoch 107, loss: 2.304067\n",
      "Epoch 108, loss: 2.304488\n",
      "Epoch 109, loss: 2.305203\n",
      "Epoch 110, loss: 2.305308\n",
      "Epoch 111, loss: 2.305265\n",
      "Epoch 112, loss: 2.305544\n",
      "Epoch 113, loss: 2.305181\n",
      "Epoch 114, loss: 2.305795\n",
      "Epoch 115, loss: 2.304588\n",
      "Epoch 116, loss: 2.303885\n",
      "Epoch 117, loss: 2.305001\n",
      "Epoch 118, loss: 2.304638\n",
      "Epoch 119, loss: 2.304788\n",
      "Epoch 120, loss: 2.304956\n",
      "Epoch 121, loss: 2.304249\n",
      "Epoch 122, loss: 2.303654\n",
      "Epoch 123, loss: 2.304991\n",
      "Epoch 124, loss: 2.305155\n",
      "Epoch 125, loss: 2.305038\n",
      "Epoch 126, loss: 2.304595\n",
      "Epoch 127, loss: 2.303918\n",
      "Epoch 128, loss: 2.304383\n",
      "Epoch 129, loss: 2.304107\n",
      "Epoch 130, loss: 2.304209\n",
      "Epoch 131, loss: 2.304193\n",
      "Epoch 132, loss: 2.304335\n",
      "Epoch 133, loss: 2.305632\n",
      "Epoch 134, loss: 2.303940\n",
      "Epoch 135, loss: 2.304999\n",
      "Epoch 136, loss: 2.304170\n",
      "Epoch 137, loss: 2.304833\n",
      "Epoch 138, loss: 2.304808\n",
      "Epoch 139, loss: 2.304327\n",
      "Epoch 140, loss: 2.304865\n",
      "Epoch 141, loss: 2.304291\n",
      "Epoch 142, loss: 2.304721\n",
      "Epoch 143, loss: 2.304584\n",
      "Epoch 144, loss: 2.304315\n",
      "Epoch 145, loss: 2.304196\n",
      "Epoch 146, loss: 2.304823\n",
      "Epoch 147, loss: 2.304439\n",
      "Epoch 148, loss: 2.304968\n",
      "Epoch 149, loss: 2.303532\n",
      "Epoch 150, loss: 2.304202\n",
      "Epoch 151, loss: 2.304562\n",
      "Epoch 152, loss: 2.304002\n",
      "Epoch 153, loss: 2.304299\n",
      "Epoch 154, loss: 2.304696\n",
      "Epoch 155, loss: 2.303188\n",
      "Epoch 156, loss: 2.304054\n",
      "Epoch 157, loss: 2.303213\n",
      "Epoch 158, loss: 2.305285\n",
      "Epoch 159, loss: 2.304137\n",
      "Epoch 160, loss: 2.303410\n",
      "Epoch 161, loss: 2.303522\n",
      "Epoch 162, loss: 2.303985\n",
      "Epoch 163, loss: 2.304407\n",
      "Epoch 164, loss: 2.304284\n",
      "Epoch 165, loss: 2.303238\n",
      "Epoch 166, loss: 2.303411\n",
      "Epoch 167, loss: 2.303809\n",
      "Epoch 168, loss: 2.304460\n",
      "Epoch 169, loss: 2.305597\n",
      "Epoch 170, loss: 2.303077\n",
      "Epoch 171, loss: 2.304368\n",
      "Epoch 172, loss: 2.304902\n",
      "Epoch 173, loss: 2.303570\n",
      "Epoch 174, loss: 2.303072\n",
      "Epoch 175, loss: 2.304812\n",
      "Epoch 176, loss: 2.305054\n",
      "Epoch 177, loss: 2.303286\n",
      "Epoch 178, loss: 2.303733\n",
      "Epoch 179, loss: 2.303102\n",
      "Epoch 180, loss: 2.304512\n",
      "Epoch 181, loss: 2.303240\n",
      "Epoch 182, loss: 2.303419\n",
      "Epoch 183, loss: 2.302907\n",
      "Epoch 184, loss: 2.304598\n",
      "Epoch 185, loss: 2.305376\n",
      "Epoch 186, loss: 2.302445\n",
      "Epoch 187, loss: 2.304225\n",
      "Epoch 188, loss: 2.304170\n",
      "Epoch 189, loss: 2.302247\n",
      "Epoch 190, loss: 2.305034\n",
      "Epoch 191, loss: 2.304059\n",
      "Epoch 192, loss: 2.304284\n",
      "Epoch 193, loss: 2.304903\n",
      "Epoch 194, loss: 2.304034\n",
      "Epoch 195, loss: 2.303337\n",
      "Epoch 196, loss: 2.303098\n",
      "Epoch 197, loss: 2.303920\n",
      "Epoch 198, loss: 2.303669\n",
      "Epoch 199, loss: 2.303118\n",
      "learning rate = 1e-05 reg = 0.1 accuracy = 0.105\n",
      "Epoch 0, loss: 2.303609\n",
      "Epoch 1, loss: 2.303340\n",
      "Epoch 2, loss: 2.301985\n",
      "Epoch 3, loss: 2.303130\n",
      "Epoch 4, loss: 2.303351\n",
      "Epoch 5, loss: 2.302317\n",
      "Epoch 6, loss: 2.303714\n",
      "Epoch 7, loss: 2.302810\n",
      "Epoch 8, loss: 2.302349\n",
      "Epoch 9, loss: 2.303734\n",
      "Epoch 10, loss: 2.303322\n",
      "Epoch 11, loss: 2.301796\n",
      "Epoch 12, loss: 2.302621\n",
      "Epoch 13, loss: 2.302744\n",
      "Epoch 14, loss: 2.302895\n",
      "Epoch 15, loss: 2.302259\n",
      "Epoch 16, loss: 2.301740\n",
      "Epoch 17, loss: 2.302069\n",
      "Epoch 18, loss: 2.302831\n",
      "Epoch 19, loss: 2.303147\n",
      "Epoch 20, loss: 2.301598\n",
      "Epoch 21, loss: 2.302006\n",
      "Epoch 22, loss: 2.302742\n",
      "Epoch 23, loss: 2.302777\n",
      "Epoch 24, loss: 2.303229\n",
      "Epoch 25, loss: 2.301997\n",
      "Epoch 26, loss: 2.302886\n",
      "Epoch 27, loss: 2.302907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, loss: 2.302753\n",
      "Epoch 29, loss: 2.301904\n",
      "Epoch 30, loss: 2.301985\n",
      "Epoch 31, loss: 2.302234\n",
      "Epoch 32, loss: 2.302304\n",
      "Epoch 33, loss: 2.302741\n",
      "Epoch 34, loss: 2.302295\n",
      "Epoch 35, loss: 2.303741\n",
      "Epoch 36, loss: 2.301942\n",
      "Epoch 37, loss: 2.302705\n",
      "Epoch 38, loss: 2.303106\n",
      "Epoch 39, loss: 2.301423\n",
      "Epoch 40, loss: 2.304520\n",
      "Epoch 41, loss: 2.302496\n",
      "Epoch 42, loss: 2.303094\n",
      "Epoch 43, loss: 2.303399\n",
      "Epoch 44, loss: 2.302525\n",
      "Epoch 45, loss: 2.302144\n",
      "Epoch 46, loss: 2.302043\n",
      "Epoch 47, loss: 2.303162\n",
      "Epoch 48, loss: 2.301454\n",
      "Epoch 49, loss: 2.302059\n",
      "Epoch 50, loss: 2.302482\n",
      "Epoch 51, loss: 2.302145\n",
      "Epoch 52, loss: 2.301693\n",
      "Epoch 53, loss: 2.302939\n",
      "Epoch 54, loss: 2.301909\n",
      "Epoch 55, loss: 2.300927\n",
      "Epoch 56, loss: 2.302466\n",
      "Epoch 57, loss: 2.302588\n",
      "Epoch 58, loss: 2.301590\n",
      "Epoch 59, loss: 2.303524\n",
      "Epoch 60, loss: 2.302282\n",
      "Epoch 61, loss: 2.301915\n",
      "Epoch 62, loss: 2.302174\n",
      "Epoch 63, loss: 2.302850\n",
      "Epoch 64, loss: 2.301588\n",
      "Epoch 65, loss: 2.302060\n",
      "Epoch 66, loss: 2.302913\n",
      "Epoch 67, loss: 2.302920\n",
      "Epoch 68, loss: 2.302307\n",
      "Epoch 69, loss: 2.302542\n",
      "Epoch 70, loss: 2.301581\n",
      "Epoch 71, loss: 2.302142\n",
      "Epoch 72, loss: 2.303106\n",
      "Epoch 73, loss: 2.302228\n",
      "Epoch 74, loss: 2.301886\n",
      "Epoch 75, loss: 2.302020\n",
      "Epoch 76, loss: 2.302172\n",
      "Epoch 77, loss: 2.302174\n",
      "Epoch 78, loss: 2.302344\n",
      "Epoch 79, loss: 2.301404\n",
      "Epoch 80, loss: 2.301125\n",
      "Epoch 81, loss: 2.302050\n",
      "Epoch 82, loss: 2.302318\n",
      "Epoch 83, loss: 2.302481\n",
      "Epoch 84, loss: 2.302247\n",
      "Epoch 85, loss: 2.301798\n",
      "Epoch 86, loss: 2.302365\n",
      "Epoch 87, loss: 2.301291\n",
      "Epoch 88, loss: 2.301320\n",
      "Epoch 89, loss: 2.302549\n",
      "Epoch 90, loss: 2.302518\n",
      "Epoch 91, loss: 2.300580\n",
      "Epoch 92, loss: 2.301703\n",
      "Epoch 93, loss: 2.301560\n",
      "Epoch 94, loss: 2.301068\n",
      "Epoch 95, loss: 2.301490\n",
      "Epoch 96, loss: 2.301989\n",
      "Epoch 97, loss: 2.302356\n",
      "Epoch 98, loss: 2.302652\n",
      "Epoch 99, loss: 2.301219\n",
      "Epoch 100, loss: 2.303028\n",
      "Epoch 101, loss: 2.302825\n",
      "Epoch 102, loss: 2.301191\n",
      "Epoch 103, loss: 2.301450\n",
      "Epoch 104, loss: 2.301197\n",
      "Epoch 105, loss: 2.302007\n",
      "Epoch 106, loss: 2.303143\n",
      "Epoch 107, loss: 2.302387\n",
      "Epoch 108, loss: 2.301933\n",
      "Epoch 109, loss: 2.302136\n",
      "Epoch 110, loss: 2.302380\n",
      "Epoch 111, loss: 2.302445\n",
      "Epoch 112, loss: 2.301898\n",
      "Epoch 113, loss: 2.303416\n",
      "Epoch 114, loss: 2.302014\n",
      "Epoch 115, loss: 2.302645\n",
      "Epoch 116, loss: 2.302310\n",
      "Epoch 117, loss: 2.300815\n",
      "Epoch 118, loss: 2.301788\n",
      "Epoch 119, loss: 2.301467\n",
      "Epoch 120, loss: 2.300924\n",
      "Epoch 121, loss: 2.301749\n",
      "Epoch 122, loss: 2.301081\n",
      "Epoch 123, loss: 2.302293\n",
      "Epoch 124, loss: 2.300754\n",
      "Epoch 125, loss: 2.301521\n",
      "Epoch 126, loss: 2.300953\n",
      "Epoch 127, loss: 2.301196\n",
      "Epoch 128, loss: 2.300123\n",
      "Epoch 129, loss: 2.301814\n",
      "Epoch 130, loss: 2.302628\n",
      "Epoch 131, loss: 2.300781\n",
      "Epoch 132, loss: 2.302258\n",
      "Epoch 133, loss: 2.302718\n",
      "Epoch 134, loss: 2.301730\n",
      "Epoch 135, loss: 2.301498\n",
      "Epoch 136, loss: 2.301175\n",
      "Epoch 137, loss: 2.301078\n",
      "Epoch 138, loss: 2.301327\n",
      "Epoch 139, loss: 2.302584\n",
      "Epoch 140, loss: 2.301699\n",
      "Epoch 141, loss: 2.302605\n",
      "Epoch 142, loss: 2.302805\n",
      "Epoch 143, loss: 2.300949\n",
      "Epoch 144, loss: 2.302800\n",
      "Epoch 145, loss: 2.301511\n",
      "Epoch 146, loss: 2.300300\n",
      "Epoch 147, loss: 2.301386\n",
      "Epoch 148, loss: 2.300554\n",
      "Epoch 149, loss: 2.301510\n",
      "Epoch 150, loss: 2.301172\n",
      "Epoch 151, loss: 2.300875\n",
      "Epoch 152, loss: 2.302302\n",
      "Epoch 153, loss: 2.302288\n",
      "Epoch 154, loss: 2.301282\n",
      "Epoch 155, loss: 2.301891\n",
      "Epoch 156, loss: 2.299965\n",
      "Epoch 157, loss: 2.300997\n",
      "Epoch 158, loss: 2.301366\n",
      "Epoch 159, loss: 2.301726\n",
      "Epoch 160, loss: 2.301265\n",
      "Epoch 161, loss: 2.302154\n",
      "Epoch 162, loss: 2.300749\n",
      "Epoch 163, loss: 2.301803\n",
      "Epoch 164, loss: 2.300925\n",
      "Epoch 165, loss: 2.301230\n",
      "Epoch 166, loss: 2.300377\n",
      "Epoch 167, loss: 2.300638\n",
      "Epoch 168, loss: 2.301505\n",
      "Epoch 169, loss: 2.301500\n",
      "Epoch 170, loss: 2.301479\n",
      "Epoch 171, loss: 2.300321\n",
      "Epoch 172, loss: 2.300892\n",
      "Epoch 173, loss: 2.301490\n",
      "Epoch 174, loss: 2.301969\n",
      "Epoch 175, loss: 2.301159\n",
      "Epoch 176, loss: 2.300923\n",
      "Epoch 177, loss: 2.301675\n",
      "Epoch 178, loss: 2.301250\n",
      "Epoch 179, loss: 2.301289\n",
      "Epoch 180, loss: 2.301957\n",
      "Epoch 181, loss: 2.300928\n",
      "Epoch 182, loss: 2.301571\n",
      "Epoch 183, loss: 2.301532\n",
      "Epoch 184, loss: 2.301468\n",
      "Epoch 185, loss: 2.302963\n",
      "Epoch 186, loss: 2.300840\n",
      "Epoch 187, loss: 2.301669\n",
      "Epoch 188, loss: 2.299753\n",
      "Epoch 189, loss: 2.298693\n",
      "Epoch 190, loss: 2.301116\n",
      "Epoch 191, loss: 2.301166\n",
      "Epoch 192, loss: 2.300565\n",
      "Epoch 193, loss: 2.301771\n",
      "Epoch 194, loss: 2.300789\n",
      "Epoch 195, loss: 2.300929\n",
      "Epoch 196, loss: 2.300903\n",
      "Epoch 197, loss: 2.301065\n",
      "Epoch 198, loss: 2.301076\n",
      "Epoch 199, loss: 2.301968\n",
      "learning rate = 1e-05 reg = 0.01 accuracy = 0.125\n",
      "Epoch 0, loss: 2.302677\n",
      "Epoch 1, loss: 2.303107\n",
      "Epoch 2, loss: 2.302664\n",
      "Epoch 3, loss: 2.303296\n",
      "Epoch 4, loss: 2.302326\n",
      "Epoch 5, loss: 2.303061\n",
      "Epoch 6, loss: 2.303350\n",
      "Epoch 7, loss: 2.303423\n",
      "Epoch 8, loss: 2.302314\n",
      "Epoch 9, loss: 2.302617\n",
      "Epoch 10, loss: 2.302396\n",
      "Epoch 11, loss: 2.302813\n",
      "Epoch 12, loss: 2.301899\n",
      "Epoch 13, loss: 2.301632\n",
      "Epoch 14, loss: 2.303964\n",
      "Epoch 15, loss: 2.301856\n",
      "Epoch 16, loss: 2.302558\n",
      "Epoch 17, loss: 2.301844\n",
      "Epoch 18, loss: 2.302531\n",
      "Epoch 19, loss: 2.301985\n",
      "Epoch 20, loss: 2.301756\n",
      "Epoch 21, loss: 2.302597\n",
      "Epoch 22, loss: 2.302448\n",
      "Epoch 23, loss: 2.302040\n",
      "Epoch 24, loss: 2.302591\n",
      "Epoch 25, loss: 2.302665\n",
      "Epoch 26, loss: 2.301959\n",
      "Epoch 27, loss: 2.302273\n",
      "Epoch 28, loss: 2.302772\n",
      "Epoch 29, loss: 2.302742\n",
      "Epoch 30, loss: 2.301557\n",
      "Epoch 31, loss: 2.302515\n",
      "Epoch 32, loss: 2.302332\n",
      "Epoch 33, loss: 2.301556\n",
      "Epoch 34, loss: 2.302276\n",
      "Epoch 35, loss: 2.301830\n",
      "Epoch 36, loss: 2.302811\n",
      "Epoch 37, loss: 2.301559\n",
      "Epoch 38, loss: 2.302300\n",
      "Epoch 39, loss: 2.303829\n",
      "Epoch 40, loss: 2.302044\n",
      "Epoch 41, loss: 2.302121\n",
      "Epoch 42, loss: 2.302072\n",
      "Epoch 43, loss: 2.302468\n",
      "Epoch 44, loss: 2.301541\n",
      "Epoch 45, loss: 2.301594\n",
      "Epoch 46, loss: 2.301948\n",
      "Epoch 47, loss: 2.301968\n",
      "Epoch 48, loss: 2.302469\n",
      "Epoch 49, loss: 2.302313\n",
      "Epoch 50, loss: 2.302243\n",
      "Epoch 51, loss: 2.302361\n",
      "Epoch 52, loss: 2.303381\n",
      "Epoch 53, loss: 2.302453\n",
      "Epoch 54, loss: 2.301785\n",
      "Epoch 55, loss: 2.302658\n",
      "Epoch 56, loss: 2.302315\n",
      "Epoch 57, loss: 2.302585\n",
      "Epoch 58, loss: 2.303116\n",
      "Epoch 59, loss: 2.302281\n",
      "Epoch 60, loss: 2.301944\n",
      "Epoch 61, loss: 2.301854\n",
      "Epoch 62, loss: 2.302035\n",
      "Epoch 63, loss: 2.302465\n",
      "Epoch 64, loss: 2.302853\n",
      "Epoch 65, loss: 2.302256\n",
      "Epoch 66, loss: 2.302069\n",
      "Epoch 67, loss: 2.302206\n",
      "Epoch 68, loss: 2.302369\n",
      "Epoch 69, loss: 2.301090\n",
      "Epoch 70, loss: 2.302118\n",
      "Epoch 71, loss: 2.302744\n",
      "Epoch 72, loss: 2.302100\n",
      "Epoch 73, loss: 2.301591\n",
      "Epoch 74, loss: 2.302795\n",
      "Epoch 75, loss: 2.301178\n",
      "Epoch 76, loss: 2.302893\n",
      "Epoch 77, loss: 2.301468\n",
      "Epoch 78, loss: 2.301644\n",
      "Epoch 79, loss: 2.301370\n",
      "Epoch 80, loss: 2.301160\n",
      "Epoch 81, loss: 2.301336\n",
      "Epoch 82, loss: 2.300898\n",
      "Epoch 83, loss: 2.302068\n",
      "Epoch 84, loss: 2.301658\n",
      "Epoch 85, loss: 2.301943\n",
      "Epoch 86, loss: 2.301508\n",
      "Epoch 87, loss: 2.302260\n",
      "Epoch 88, loss: 2.301544\n",
      "Epoch 89, loss: 2.300281\n",
      "Epoch 90, loss: 2.301021\n",
      "Epoch 91, loss: 2.302097\n",
      "Epoch 92, loss: 2.303496\n",
      "Epoch 93, loss: 2.302004\n",
      "Epoch 94, loss: 2.300759\n",
      "Epoch 95, loss: 2.301673\n",
      "Epoch 96, loss: 2.302155\n",
      "Epoch 97, loss: 2.301015\n",
      "Epoch 98, loss: 2.300829\n",
      "Epoch 99, loss: 2.301062\n",
      "Epoch 100, loss: 2.302262\n",
      "Epoch 101, loss: 2.301056\n",
      "Epoch 102, loss: 2.301720\n",
      "Epoch 103, loss: 2.300210\n",
      "Epoch 104, loss: 2.301296\n",
      "Epoch 105, loss: 2.301116\n",
      "Epoch 106, loss: 2.301711\n",
      "Epoch 107, loss: 2.300499\n",
      "Epoch 108, loss: 2.299750\n",
      "Epoch 109, loss: 2.301734\n",
      "Epoch 110, loss: 2.301148\n",
      "Epoch 111, loss: 2.301809\n",
      "Epoch 112, loss: 2.301682\n",
      "Epoch 113, loss: 2.301458\n",
      "Epoch 114, loss: 2.300552\n",
      "Epoch 115, loss: 2.301427\n",
      "Epoch 116, loss: 2.301159\n",
      "Epoch 117, loss: 2.302267\n",
      "Epoch 118, loss: 2.300825\n",
      "Epoch 119, loss: 2.301842\n",
      "Epoch 120, loss: 2.301995\n",
      "Epoch 121, loss: 2.300417\n",
      "Epoch 122, loss: 2.300813\n",
      "Epoch 123, loss: 2.302366\n",
      "Epoch 124, loss: 2.301494\n",
      "Epoch 125, loss: 2.302646\n",
      "Epoch 126, loss: 2.301613\n",
      "Epoch 127, loss: 2.300429\n",
      "Epoch 128, loss: 2.302458\n",
      "Epoch 129, loss: 2.301099\n",
      "Epoch 130, loss: 2.301339\n",
      "Epoch 131, loss: 2.301207\n",
      "Epoch 132, loss: 2.302744\n",
      "Epoch 133, loss: 2.301986\n",
      "Epoch 134, loss: 2.300946\n",
      "Epoch 135, loss: 2.302359\n",
      "Epoch 136, loss: 2.301926\n",
      "Epoch 137, loss: 2.301161\n",
      "Epoch 138, loss: 2.300730\n",
      "Epoch 139, loss: 2.300729\n",
      "Epoch 140, loss: 2.300676\n",
      "Epoch 141, loss: 2.302649\n",
      "Epoch 142, loss: 2.301237\n",
      "Epoch 143, loss: 2.300983\n",
      "Epoch 144, loss: 2.301708\n",
      "Epoch 145, loss: 2.300344\n",
      "Epoch 146, loss: 2.301645\n",
      "Epoch 147, loss: 2.300526\n",
      "Epoch 148, loss: 2.301887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149, loss: 2.301965\n",
      "Epoch 150, loss: 2.300884\n",
      "Epoch 151, loss: 2.301767\n",
      "Epoch 152, loss: 2.301067\n",
      "Epoch 153, loss: 2.300149\n",
      "Epoch 154, loss: 2.302167\n",
      "Epoch 155, loss: 2.301048\n",
      "Epoch 156, loss: 2.301255\n",
      "Epoch 157, loss: 2.302339\n",
      "Epoch 158, loss: 2.301328\n",
      "Epoch 159, loss: 2.301862\n",
      "Epoch 160, loss: 2.300936\n",
      "Epoch 161, loss: 2.301052\n",
      "Epoch 162, loss: 2.302322\n",
      "Epoch 163, loss: 2.301759\n",
      "Epoch 164, loss: 2.300770\n",
      "Epoch 165, loss: 2.300058\n",
      "Epoch 166, loss: 2.300319\n",
      "Epoch 167, loss: 2.300955\n",
      "Epoch 168, loss: 2.301253\n",
      "Epoch 169, loss: 2.301165\n",
      "Epoch 170, loss: 2.301057\n",
      "Epoch 171, loss: 2.301253\n",
      "Epoch 172, loss: 2.300068\n",
      "Epoch 173, loss: 2.299963\n",
      "Epoch 174, loss: 2.300914\n",
      "Epoch 175, loss: 2.300945\n",
      "Epoch 176, loss: 2.300750\n",
      "Epoch 177, loss: 2.301121\n",
      "Epoch 178, loss: 2.301077\n",
      "Epoch 179, loss: 2.300961\n",
      "Epoch 180, loss: 2.299752\n",
      "Epoch 181, loss: 2.300920\n",
      "Epoch 182, loss: 2.299992\n",
      "Epoch 183, loss: 2.302212\n",
      "Epoch 184, loss: 2.300644\n",
      "Epoch 185, loss: 2.301059\n",
      "Epoch 186, loss: 2.301889\n",
      "Epoch 187, loss: 2.300312\n",
      "Epoch 188, loss: 2.299770\n",
      "Epoch 189, loss: 2.301402\n",
      "Epoch 190, loss: 2.301031\n",
      "Epoch 191, loss: 2.301447\n",
      "Epoch 192, loss: 2.301752\n",
      "Epoch 193, loss: 2.299847\n",
      "Epoch 194, loss: 2.300485\n",
      "Epoch 195, loss: 2.299522\n",
      "Epoch 196, loss: 2.300877\n",
      "Epoch 197, loss: 2.300865\n",
      "Epoch 198, loss: 2.302110\n",
      "Epoch 199, loss: 2.300887\n",
      "learning rate = 1e-05 reg = 0.001 accuracy = 0.095\n",
      "Epoch 0, loss: 2.302634\n",
      "Epoch 1, loss: 2.302583\n",
      "Epoch 2, loss: 2.301600\n",
      "Epoch 3, loss: 2.303225\n",
      "Epoch 4, loss: 2.302369\n",
      "Epoch 5, loss: 2.301696\n",
      "Epoch 6, loss: 2.302319\n",
      "Epoch 7, loss: 2.302206\n",
      "Epoch 8, loss: 2.302851\n",
      "Epoch 9, loss: 2.301467\n",
      "Epoch 10, loss: 2.302935\n",
      "Epoch 11, loss: 2.302574\n",
      "Epoch 12, loss: 2.301270\n",
      "Epoch 13, loss: 2.302831\n",
      "Epoch 14, loss: 2.302982\n",
      "Epoch 15, loss: 2.302307\n",
      "Epoch 16, loss: 2.303006\n",
      "Epoch 17, loss: 2.301723\n",
      "Epoch 18, loss: 2.301408\n",
      "Epoch 19, loss: 2.302100\n",
      "Epoch 20, loss: 2.302211\n",
      "Epoch 21, loss: 2.302314\n",
      "Epoch 22, loss: 2.301889\n",
      "Epoch 23, loss: 2.302645\n",
      "Epoch 24, loss: 2.301419\n",
      "Epoch 25, loss: 2.301301\n",
      "Epoch 26, loss: 2.301760\n",
      "Epoch 27, loss: 2.302655\n",
      "Epoch 28, loss: 2.302999\n",
      "Epoch 29, loss: 2.302941\n",
      "Epoch 30, loss: 2.301228\n",
      "Epoch 31, loss: 2.302670\n",
      "Epoch 32, loss: 2.301806\n",
      "Epoch 33, loss: 2.302339\n",
      "Epoch 34, loss: 2.302597\n",
      "Epoch 35, loss: 2.300946\n",
      "Epoch 36, loss: 2.302580\n",
      "Epoch 37, loss: 2.301756\n",
      "Epoch 38, loss: 2.303050\n",
      "Epoch 39, loss: 2.303257\n",
      "Epoch 40, loss: 2.302315\n",
      "Epoch 41, loss: 2.302707\n",
      "Epoch 42, loss: 2.302072\n",
      "Epoch 43, loss: 2.302044\n",
      "Epoch 44, loss: 2.301918\n",
      "Epoch 45, loss: 2.301733\n",
      "Epoch 46, loss: 2.302335\n",
      "Epoch 47, loss: 2.301450\n",
      "Epoch 48, loss: 2.302119\n",
      "Epoch 49, loss: 2.301483\n",
      "Epoch 50, loss: 2.301234\n",
      "Epoch 51, loss: 2.301341\n",
      "Epoch 52, loss: 2.302633\n",
      "Epoch 53, loss: 2.300695\n",
      "Epoch 54, loss: 2.302356\n",
      "Epoch 55, loss: 2.301794\n",
      "Epoch 56, loss: 2.302401\n",
      "Epoch 57, loss: 2.302436\n",
      "Epoch 58, loss: 2.300150\n",
      "Epoch 59, loss: 2.302649\n",
      "Epoch 60, loss: 2.302577\n",
      "Epoch 61, loss: 2.300759\n",
      "Epoch 62, loss: 2.302744\n",
      "Epoch 63, loss: 2.301465\n",
      "Epoch 64, loss: 2.302157\n",
      "Epoch 65, loss: 2.301731\n",
      "Epoch 66, loss: 2.301523\n",
      "Epoch 67, loss: 2.301759\n",
      "Epoch 68, loss: 2.302310\n",
      "Epoch 69, loss: 2.301888\n",
      "Epoch 70, loss: 2.301977\n",
      "Epoch 71, loss: 2.302423\n",
      "Epoch 72, loss: 2.301335\n",
      "Epoch 73, loss: 2.302101\n",
      "Epoch 74, loss: 2.301765\n",
      "Epoch 75, loss: 2.301801\n",
      "Epoch 76, loss: 2.302238\n",
      "Epoch 77, loss: 2.301785\n",
      "Epoch 78, loss: 2.303097\n",
      "Epoch 79, loss: 2.302493\n",
      "Epoch 80, loss: 2.302891\n",
      "Epoch 81, loss: 2.300936\n",
      "Epoch 82, loss: 2.301539\n",
      "Epoch 83, loss: 2.301627\n",
      "Epoch 84, loss: 2.302918\n",
      "Epoch 85, loss: 2.302356\n",
      "Epoch 86, loss: 2.301833\n",
      "Epoch 87, loss: 2.301830\n",
      "Epoch 88, loss: 2.302637\n",
      "Epoch 89, loss: 2.302092\n",
      "Epoch 90, loss: 2.302370\n",
      "Epoch 91, loss: 2.301134\n",
      "Epoch 92, loss: 2.300479\n",
      "Epoch 93, loss: 2.301296\n",
      "Epoch 94, loss: 2.303136\n",
      "Epoch 95, loss: 2.302163\n",
      "Epoch 96, loss: 2.300867\n",
      "Epoch 97, loss: 2.302664\n",
      "Epoch 98, loss: 2.302029\n",
      "Epoch 99, loss: 2.301446\n",
      "Epoch 100, loss: 2.302668\n",
      "Epoch 101, loss: 2.301483\n",
      "Epoch 102, loss: 2.301685\n",
      "Epoch 103, loss: 2.301757\n",
      "Epoch 104, loss: 2.301168\n",
      "Epoch 105, loss: 2.300435\n",
      "Epoch 106, loss: 2.301565\n",
      "Epoch 107, loss: 2.302055\n",
      "Epoch 108, loss: 2.302327\n",
      "Epoch 109, loss: 2.299454\n",
      "Epoch 110, loss: 2.300554\n",
      "Epoch 111, loss: 2.301887\n",
      "Epoch 112, loss: 2.300943\n",
      "Epoch 113, loss: 2.300557\n",
      "Epoch 114, loss: 2.301257\n",
      "Epoch 115, loss: 2.300415\n",
      "Epoch 116, loss: 2.301547\n",
      "Epoch 117, loss: 2.300922\n",
      "Epoch 118, loss: 2.302105\n",
      "Epoch 119, loss: 2.301759\n",
      "Epoch 120, loss: 2.301083\n",
      "Epoch 121, loss: 2.300466\n",
      "Epoch 122, loss: 2.301601\n",
      "Epoch 123, loss: 2.303023\n",
      "Epoch 124, loss: 2.301333\n",
      "Epoch 125, loss: 2.300924\n",
      "Epoch 126, loss: 2.300620\n",
      "Epoch 127, loss: 2.301610\n",
      "Epoch 128, loss: 2.300531\n",
      "Epoch 129, loss: 2.301976\n",
      "Epoch 130, loss: 2.301873\n",
      "Epoch 131, loss: 2.301129\n",
      "Epoch 132, loss: 2.301659\n",
      "Epoch 133, loss: 2.300942\n",
      "Epoch 134, loss: 2.301745\n",
      "Epoch 135, loss: 2.300353\n",
      "Epoch 136, loss: 2.300561\n",
      "Epoch 137, loss: 2.302109\n",
      "Epoch 138, loss: 2.299650\n",
      "Epoch 139, loss: 2.300301\n",
      "Epoch 140, loss: 2.300726\n",
      "Epoch 141, loss: 2.302204\n",
      "Epoch 142, loss: 2.301191\n",
      "Epoch 143, loss: 2.300726\n",
      "Epoch 144, loss: 2.301963\n",
      "Epoch 145, loss: 2.301774\n",
      "Epoch 146, loss: 2.301857\n",
      "Epoch 147, loss: 2.300350\n",
      "Epoch 148, loss: 2.300796\n",
      "Epoch 149, loss: 2.302336\n",
      "Epoch 150, loss: 2.301081\n",
      "Epoch 151, loss: 2.299866\n",
      "Epoch 152, loss: 2.301109\n",
      "Epoch 153, loss: 2.301489\n",
      "Epoch 154, loss: 2.300961\n",
      "Epoch 155, loss: 2.300506\n",
      "Epoch 156, loss: 2.300349\n",
      "Epoch 157, loss: 2.300608\n",
      "Epoch 158, loss: 2.299822\n",
      "Epoch 159, loss: 2.300521\n",
      "Epoch 160, loss: 2.300791\n",
      "Epoch 161, loss: 2.302094\n",
      "Epoch 162, loss: 2.300946\n",
      "Epoch 163, loss: 2.301965\n",
      "Epoch 164, loss: 2.300962\n",
      "Epoch 165, loss: 2.300154\n",
      "Epoch 166, loss: 2.301997\n",
      "Epoch 167, loss: 2.300947\n",
      "Epoch 168, loss: 2.299765\n",
      "Epoch 169, loss: 2.300727\n",
      "Epoch 170, loss: 2.300306\n",
      "Epoch 171, loss: 2.300991\n",
      "Epoch 172, loss: 2.300909\n",
      "Epoch 173, loss: 2.302225\n",
      "Epoch 174, loss: 2.301900\n",
      "Epoch 175, loss: 2.300137\n",
      "Epoch 176, loss: 2.302195\n",
      "Epoch 177, loss: 2.301347\n",
      "Epoch 178, loss: 2.300717\n",
      "Epoch 179, loss: 2.300646\n",
      "Epoch 180, loss: 2.302110\n",
      "Epoch 181, loss: 2.300479\n",
      "Epoch 182, loss: 2.301465\n",
      "Epoch 183, loss: 2.299414\n",
      "Epoch 184, loss: 2.299794\n",
      "Epoch 185, loss: 2.300080\n",
      "Epoch 186, loss: 2.299912\n",
      "Epoch 187, loss: 2.299862\n",
      "Epoch 188, loss: 2.301608\n",
      "Epoch 189, loss: 2.300759\n",
      "Epoch 190, loss: 2.300248\n",
      "Epoch 191, loss: 2.300434\n",
      "Epoch 192, loss: 2.300416\n",
      "Epoch 193, loss: 2.300507\n",
      "Epoch 194, loss: 2.300843\n",
      "Epoch 195, loss: 2.298937\n",
      "Epoch 196, loss: 2.300677\n",
      "Epoch 197, loss: 2.298893\n",
      "Epoch 198, loss: 2.300683\n",
      "Epoch 199, loss: 2.300174\n",
      "learning rate = 1e-05 reg = 0.0001 accuracy = 0.122\n",
      "Epoch 0, loss: 2.302496\n",
      "Epoch 1, loss: 2.302879\n",
      "Epoch 2, loss: 2.302919\n",
      "Epoch 3, loss: 2.303235\n",
      "Epoch 4, loss: 2.302496\n",
      "Epoch 5, loss: 2.302608\n",
      "Epoch 6, loss: 2.302464\n",
      "Epoch 7, loss: 2.302812\n",
      "Epoch 8, loss: 2.302836\n",
      "Epoch 9, loss: 2.302909\n",
      "Epoch 10, loss: 2.301782\n",
      "Epoch 11, loss: 2.302863\n",
      "Epoch 12, loss: 2.302053\n",
      "Epoch 13, loss: 2.302863\n",
      "Epoch 14, loss: 2.302188\n",
      "Epoch 15, loss: 2.301367\n",
      "Epoch 16, loss: 2.302187\n",
      "Epoch 17, loss: 2.302505\n",
      "Epoch 18, loss: 2.302935\n",
      "Epoch 19, loss: 2.301797\n",
      "Epoch 20, loss: 2.301661\n",
      "Epoch 21, loss: 2.302947\n",
      "Epoch 22, loss: 2.302805\n",
      "Epoch 23, loss: 2.302189\n",
      "Epoch 24, loss: 2.303714\n",
      "Epoch 25, loss: 2.302952\n",
      "Epoch 26, loss: 2.300936\n",
      "Epoch 27, loss: 2.301499\n",
      "Epoch 28, loss: 2.302161\n",
      "Epoch 29, loss: 2.302338\n",
      "Epoch 30, loss: 2.302533\n",
      "Epoch 31, loss: 2.301943\n",
      "Epoch 32, loss: 2.302436\n",
      "Epoch 33, loss: 2.301866\n",
      "Epoch 34, loss: 2.302224\n",
      "Epoch 35, loss: 2.301982\n",
      "Epoch 36, loss: 2.301596\n",
      "Epoch 37, loss: 2.302118\n",
      "Epoch 38, loss: 2.302501\n",
      "Epoch 39, loss: 2.302553\n",
      "Epoch 40, loss: 2.302471\n",
      "Epoch 41, loss: 2.301551\n",
      "Epoch 42, loss: 2.302494\n",
      "Epoch 43, loss: 2.302260\n",
      "Epoch 44, loss: 2.302832\n",
      "Epoch 45, loss: 2.303741\n",
      "Epoch 46, loss: 2.302129\n",
      "Epoch 47, loss: 2.301967\n",
      "Epoch 48, loss: 2.301318\n",
      "Epoch 49, loss: 2.302035\n",
      "Epoch 50, loss: 2.302991\n",
      "Epoch 51, loss: 2.301574\n",
      "Epoch 52, loss: 2.301543\n",
      "Epoch 53, loss: 2.301619\n",
      "Epoch 54, loss: 2.301789\n",
      "Epoch 55, loss: 2.301124\n",
      "Epoch 56, loss: 2.302432\n",
      "Epoch 57, loss: 2.301341\n",
      "Epoch 58, loss: 2.302271\n",
      "Epoch 59, loss: 2.302407\n",
      "Epoch 60, loss: 2.301451\n",
      "Epoch 61, loss: 2.302131\n",
      "Epoch 62, loss: 2.302200\n",
      "Epoch 63, loss: 2.301713\n",
      "Epoch 64, loss: 2.300105\n",
      "Epoch 65, loss: 2.301426\n",
      "Epoch 66, loss: 2.300213\n",
      "Epoch 67, loss: 2.301439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, loss: 2.302531\n",
      "Epoch 69, loss: 2.302328\n",
      "Epoch 70, loss: 2.302600\n",
      "Epoch 71, loss: 2.301869\n",
      "Epoch 72, loss: 2.302112\n",
      "Epoch 73, loss: 2.302511\n",
      "Epoch 74, loss: 2.301207\n",
      "Epoch 75, loss: 2.301613\n",
      "Epoch 76, loss: 2.302091\n",
      "Epoch 77, loss: 2.302555\n",
      "Epoch 78, loss: 2.301559\n",
      "Epoch 79, loss: 2.303422\n",
      "Epoch 80, loss: 2.302721\n",
      "Epoch 81, loss: 2.301588\n",
      "Epoch 82, loss: 2.301056\n",
      "Epoch 83, loss: 2.300650\n",
      "Epoch 84, loss: 2.301754\n",
      "Epoch 85, loss: 2.302447\n",
      "Epoch 86, loss: 2.301301\n",
      "Epoch 87, loss: 2.301980\n",
      "Epoch 88, loss: 2.302236\n",
      "Epoch 89, loss: 2.301545\n",
      "Epoch 90, loss: 2.302838\n",
      "Epoch 91, loss: 2.301304\n",
      "Epoch 92, loss: 2.301217\n",
      "Epoch 93, loss: 2.300923\n",
      "Epoch 94, loss: 2.300148\n",
      "Epoch 95, loss: 2.300849\n",
      "Epoch 96, loss: 2.300548\n",
      "Epoch 97, loss: 2.301513\n",
      "Epoch 98, loss: 2.299840\n",
      "Epoch 99, loss: 2.301484\n",
      "Epoch 100, loss: 2.301285\n",
      "Epoch 101, loss: 2.301598\n",
      "Epoch 102, loss: 2.302701\n",
      "Epoch 103, loss: 2.300794\n",
      "Epoch 104, loss: 2.302013\n",
      "Epoch 105, loss: 2.301083\n",
      "Epoch 106, loss: 2.299870\n",
      "Epoch 107, loss: 2.300927\n",
      "Epoch 108, loss: 2.301048\n",
      "Epoch 109, loss: 2.302659\n",
      "Epoch 110, loss: 2.301562\n",
      "Epoch 111, loss: 2.301347\n",
      "Epoch 112, loss: 2.302486\n",
      "Epoch 113, loss: 2.300220\n",
      "Epoch 114, loss: 2.302196\n",
      "Epoch 115, loss: 2.302021\n",
      "Epoch 116, loss: 2.301276\n",
      "Epoch 117, loss: 2.301496\n",
      "Epoch 118, loss: 2.300266\n",
      "Epoch 119, loss: 2.300153\n",
      "Epoch 120, loss: 2.302301\n",
      "Epoch 121, loss: 2.303527\n",
      "Epoch 122, loss: 2.300003\n",
      "Epoch 123, loss: 2.301086\n",
      "Epoch 124, loss: 2.300675\n",
      "Epoch 125, loss: 2.301156\n",
      "Epoch 126, loss: 2.300279\n",
      "Epoch 127, loss: 2.301655\n",
      "Epoch 128, loss: 2.302225\n",
      "Epoch 129, loss: 2.301734\n",
      "Epoch 130, loss: 2.300715\n",
      "Epoch 131, loss: 2.300202\n",
      "Epoch 132, loss: 2.300639\n",
      "Epoch 133, loss: 2.300475\n",
      "Epoch 134, loss: 2.302557\n",
      "Epoch 135, loss: 2.300845\n",
      "Epoch 136, loss: 2.300867\n",
      "Epoch 137, loss: 2.301935\n",
      "Epoch 138, loss: 2.302217\n",
      "Epoch 139, loss: 2.300017\n",
      "Epoch 140, loss: 2.300517\n",
      "Epoch 141, loss: 2.302000\n",
      "Epoch 142, loss: 2.301555\n",
      "Epoch 143, loss: 2.300540\n",
      "Epoch 144, loss: 2.300257\n",
      "Epoch 145, loss: 2.301846\n",
      "Epoch 146, loss: 2.300472\n",
      "Epoch 147, loss: 2.301042\n",
      "Epoch 148, loss: 2.300956\n",
      "Epoch 149, loss: 2.301825\n",
      "Epoch 150, loss: 2.302192\n",
      "Epoch 151, loss: 2.300267\n",
      "Epoch 152, loss: 2.300448\n",
      "Epoch 153, loss: 2.301905\n",
      "Epoch 154, loss: 2.301092\n",
      "Epoch 155, loss: 2.299249\n",
      "Epoch 156, loss: 2.300432\n",
      "Epoch 157, loss: 2.301849\n",
      "Epoch 158, loss: 2.300925\n",
      "Epoch 159, loss: 2.301200\n",
      "Epoch 160, loss: 2.300084\n",
      "Epoch 161, loss: 2.300093\n",
      "Epoch 162, loss: 2.302227\n",
      "Epoch 163, loss: 2.301110\n",
      "Epoch 164, loss: 2.300206\n",
      "Epoch 165, loss: 2.302094\n",
      "Epoch 166, loss: 2.301906\n",
      "Epoch 167, loss: 2.298940\n",
      "Epoch 168, loss: 2.299837\n",
      "Epoch 169, loss: 2.300543\n",
      "Epoch 170, loss: 2.298811\n",
      "Epoch 171, loss: 2.301028\n",
      "Epoch 172, loss: 2.300873\n",
      "Epoch 173, loss: 2.299349\n",
      "Epoch 174, loss: 2.303080\n",
      "Epoch 175, loss: 2.301333\n",
      "Epoch 176, loss: 2.303199\n",
      "Epoch 177, loss: 2.300946\n",
      "Epoch 178, loss: 2.300262\n",
      "Epoch 179, loss: 2.301276\n",
      "Epoch 180, loss: 2.300224\n",
      "Epoch 181, loss: 2.300719\n",
      "Epoch 182, loss: 2.299599\n",
      "Epoch 183, loss: 2.301277\n",
      "Epoch 184, loss: 2.301018\n",
      "Epoch 185, loss: 2.302007\n",
      "Epoch 186, loss: 2.301374\n",
      "Epoch 187, loss: 2.302145\n",
      "Epoch 188, loss: 2.300276\n",
      "Epoch 189, loss: 2.301392\n",
      "Epoch 190, loss: 2.298334\n",
      "Epoch 191, loss: 2.301532\n",
      "Epoch 192, loss: 2.300001\n",
      "Epoch 193, loss: 2.301095\n",
      "Epoch 194, loss: 2.300161\n",
      "Epoch 195, loss: 2.301152\n",
      "Epoch 196, loss: 2.301989\n",
      "Epoch 197, loss: 2.300909\n",
      "Epoch 198, loss: 2.300622\n",
      "Epoch 199, loss: 2.301643\n",
      "learning rate = 1e-05 reg = 1e-05 accuracy = 0.113\n",
      "Epoch 0, loss: 2.302950\n",
      "Epoch 1, loss: 2.302632\n",
      "Epoch 2, loss: 2.302603\n",
      "Epoch 3, loss: 2.303065\n",
      "Epoch 4, loss: 2.301963\n",
      "Epoch 5, loss: 2.301933\n",
      "Epoch 6, loss: 2.303269\n",
      "Epoch 7, loss: 2.302338\n",
      "Epoch 8, loss: 2.302521\n",
      "Epoch 9, loss: 2.301864\n",
      "Epoch 10, loss: 2.301865\n",
      "Epoch 11, loss: 2.302497\n",
      "Epoch 12, loss: 2.302950\n",
      "Epoch 13, loss: 2.302400\n",
      "Epoch 14, loss: 2.302205\n",
      "Epoch 15, loss: 2.303174\n",
      "Epoch 16, loss: 2.302375\n",
      "Epoch 17, loss: 2.302173\n",
      "Epoch 18, loss: 2.302448\n",
      "Epoch 19, loss: 2.302022\n",
      "Epoch 20, loss: 2.302625\n",
      "Epoch 21, loss: 2.301539\n",
      "Epoch 22, loss: 2.302353\n",
      "Epoch 23, loss: 2.302372\n",
      "Epoch 24, loss: 2.302277\n",
      "Epoch 25, loss: 2.302245\n",
      "Epoch 26, loss: 2.302101\n",
      "Epoch 27, loss: 2.301661\n",
      "Epoch 28, loss: 2.302623\n",
      "Epoch 29, loss: 2.302397\n",
      "Epoch 30, loss: 2.302716\n",
      "Epoch 31, loss: 2.301929\n",
      "Epoch 32, loss: 2.300942\n",
      "Epoch 33, loss: 2.300988\n",
      "Epoch 34, loss: 2.302122\n",
      "Epoch 35, loss: 2.302009\n",
      "Epoch 36, loss: 2.301952\n",
      "Epoch 37, loss: 2.302853\n",
      "Epoch 38, loss: 2.302506\n",
      "Epoch 39, loss: 2.302434\n",
      "Epoch 40, loss: 2.301828\n",
      "Epoch 41, loss: 2.302170\n",
      "Epoch 42, loss: 2.302319\n",
      "Epoch 43, loss: 2.302497\n",
      "Epoch 44, loss: 2.301064\n",
      "Epoch 45, loss: 2.301261\n",
      "Epoch 46, loss: 2.301888\n",
      "Epoch 47, loss: 2.301260\n",
      "Epoch 48, loss: 2.303228\n",
      "Epoch 49, loss: 2.302077\n",
      "Epoch 50, loss: 2.302337\n",
      "Epoch 51, loss: 2.301735\n",
      "Epoch 52, loss: 2.302880\n",
      "Epoch 53, loss: 2.301654\n",
      "Epoch 54, loss: 2.301128\n",
      "Epoch 55, loss: 2.302082\n",
      "Epoch 56, loss: 2.302679\n",
      "Epoch 57, loss: 2.302359\n",
      "Epoch 58, loss: 2.301365\n",
      "Epoch 59, loss: 2.301386\n",
      "Epoch 60, loss: 2.301530\n",
      "Epoch 61, loss: 2.302177\n",
      "Epoch 62, loss: 2.302914\n",
      "Epoch 63, loss: 2.300610\n",
      "Epoch 64, loss: 2.302075\n",
      "Epoch 65, loss: 2.302610\n",
      "Epoch 66, loss: 2.302762\n",
      "Epoch 67, loss: 2.301923\n",
      "Epoch 68, loss: 2.301176\n",
      "Epoch 69, loss: 2.302017\n",
      "Epoch 70, loss: 2.300900\n",
      "Epoch 71, loss: 2.301455\n",
      "Epoch 72, loss: 2.302161\n",
      "Epoch 73, loss: 2.300799\n",
      "Epoch 74, loss: 2.301610\n",
      "Epoch 75, loss: 2.300134\n",
      "Epoch 76, loss: 2.302300\n",
      "Epoch 77, loss: 2.301901\n",
      "Epoch 78, loss: 2.301019\n",
      "Epoch 79, loss: 2.301313\n",
      "Epoch 80, loss: 2.302016\n",
      "Epoch 81, loss: 2.301814\n",
      "Epoch 82, loss: 2.301883\n",
      "Epoch 83, loss: 2.302428\n",
      "Epoch 84, loss: 2.301496\n",
      "Epoch 85, loss: 2.301525\n",
      "Epoch 86, loss: 2.301106\n",
      "Epoch 87, loss: 2.302388\n",
      "Epoch 88, loss: 2.300886\n",
      "Epoch 89, loss: 2.302124\n",
      "Epoch 90, loss: 2.301367\n",
      "Epoch 91, loss: 2.300705\n",
      "Epoch 92, loss: 2.301330\n",
      "Epoch 93, loss: 2.301025\n",
      "Epoch 94, loss: 2.301909\n",
      "Epoch 95, loss: 2.300203\n",
      "Epoch 96, loss: 2.301657\n",
      "Epoch 97, loss: 2.302272\n",
      "Epoch 98, loss: 2.301543\n",
      "Epoch 99, loss: 2.302311\n",
      "Epoch 100, loss: 2.300009\n",
      "Epoch 101, loss: 2.300434\n",
      "Epoch 102, loss: 2.301037\n",
      "Epoch 103, loss: 2.301698\n",
      "Epoch 104, loss: 2.300669\n",
      "Epoch 105, loss: 2.300963\n",
      "Epoch 106, loss: 2.301269\n",
      "Epoch 107, loss: 2.301239\n",
      "Epoch 108, loss: 2.301390\n",
      "Epoch 109, loss: 2.300821\n",
      "Epoch 110, loss: 2.301336\n",
      "Epoch 111, loss: 2.301550\n",
      "Epoch 112, loss: 2.301551\n",
      "Epoch 113, loss: 2.301145\n",
      "Epoch 114, loss: 2.301223\n",
      "Epoch 115, loss: 2.301990\n",
      "Epoch 116, loss: 2.300861\n",
      "Epoch 117, loss: 2.300726\n",
      "Epoch 118, loss: 2.300305\n",
      "Epoch 119, loss: 2.300384\n",
      "Epoch 120, loss: 2.302547\n",
      "Epoch 121, loss: 2.300968\n",
      "Epoch 122, loss: 2.300863\n",
      "Epoch 123, loss: 2.300920\n",
      "Epoch 124, loss: 2.301399\n",
      "Epoch 125, loss: 2.300733\n",
      "Epoch 126, loss: 2.300966\n",
      "Epoch 127, loss: 2.301800\n",
      "Epoch 128, loss: 2.301082\n",
      "Epoch 129, loss: 2.301276\n",
      "Epoch 130, loss: 2.301805\n",
      "Epoch 131, loss: 2.301874\n",
      "Epoch 132, loss: 2.300835\n",
      "Epoch 133, loss: 2.299950\n",
      "Epoch 134, loss: 2.300570\n",
      "Epoch 135, loss: 2.299369\n",
      "Epoch 136, loss: 2.301455\n",
      "Epoch 137, loss: 2.300845\n",
      "Epoch 138, loss: 2.301048\n",
      "Epoch 139, loss: 2.300356\n",
      "Epoch 140, loss: 2.301370\n",
      "Epoch 141, loss: 2.300268\n",
      "Epoch 142, loss: 2.301645\n",
      "Epoch 143, loss: 2.301755\n",
      "Epoch 144, loss: 2.301100\n",
      "Epoch 145, loss: 2.301414\n",
      "Epoch 146, loss: 2.300861\n",
      "Epoch 147, loss: 2.300970\n",
      "Epoch 148, loss: 2.300966\n",
      "Epoch 149, loss: 2.300931\n",
      "Epoch 150, loss: 2.300467\n",
      "Epoch 151, loss: 2.301015\n",
      "Epoch 152, loss: 2.301495\n",
      "Epoch 153, loss: 2.301663\n",
      "Epoch 154, loss: 2.300055\n",
      "Epoch 155, loss: 2.299329\n",
      "Epoch 156, loss: 2.301103\n",
      "Epoch 157, loss: 2.301335\n",
      "Epoch 158, loss: 2.301836\n",
      "Epoch 159, loss: 2.300445\n",
      "Epoch 160, loss: 2.300819\n",
      "Epoch 161, loss: 2.300533\n",
      "Epoch 162, loss: 2.299960\n",
      "Epoch 163, loss: 2.300776\n",
      "Epoch 164, loss: 2.299980\n",
      "Epoch 165, loss: 2.301555\n",
      "Epoch 166, loss: 2.300975\n",
      "Epoch 167, loss: 2.301742\n",
      "Epoch 168, loss: 2.299427\n",
      "Epoch 169, loss: 2.300510\n",
      "Epoch 170, loss: 2.300310\n",
      "Epoch 171, loss: 2.300546\n",
      "Epoch 172, loss: 2.301500\n",
      "Epoch 173, loss: 2.301679\n",
      "Epoch 174, loss: 2.300604\n",
      "Epoch 175, loss: 2.302129\n",
      "Epoch 176, loss: 2.300206\n",
      "Epoch 177, loss: 2.300832\n",
      "Epoch 178, loss: 2.299498\n",
      "Epoch 179, loss: 2.300548\n",
      "Epoch 180, loss: 2.300273\n",
      "Epoch 181, loss: 2.299954\n",
      "Epoch 182, loss: 2.301132\n",
      "Epoch 183, loss: 2.301853\n",
      "Epoch 184, loss: 2.301355\n",
      "Epoch 185, loss: 2.301479\n",
      "Epoch 186, loss: 2.301751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187, loss: 2.299677\n",
      "Epoch 188, loss: 2.301568\n",
      "Epoch 189, loss: 2.301940\n",
      "Epoch 190, loss: 2.301392\n",
      "Epoch 191, loss: 2.300312\n",
      "Epoch 192, loss: 2.301318\n",
      "Epoch 193, loss: 2.299099\n",
      "Epoch 194, loss: 2.301202\n",
      "Epoch 195, loss: 2.300468\n",
      "Epoch 196, loss: 2.299004\n",
      "Epoch 197, loss: 2.301836\n",
      "Epoch 198, loss: 2.300311\n",
      "Epoch 199, loss: 2.301091\n",
      "learning rate = 1e-05 reg = 1e-06 accuracy = 0.146\n",
      "best validation accuracy achieved: 0.246000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        print('learning rate =', lr, 'reg =', rs, 'accuracy =', accuracy)\n",
    "        if best_val_accuracy is None:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "        if accuracy>best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T16:37:15.050614Z",
     "start_time": "2019-05-03T16:37:14.710633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.203000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
